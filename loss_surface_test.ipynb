{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b447e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pprint\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Set GPU Index to use\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformer import BertForSequenceClassification,WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformer.modeling_quant import BertForSequenceClassification as QuantBertForSequenceClassification\n",
    "from transformer import BertTokenizer\n",
    "from transformer import BertAdam\n",
    "from transformer import BertConfig\n",
    "from transformer import QuantizeLinear, QuantizeAct, BertSelfAttention, FP_BertSelfAttention, ClipLinear, BertAttention, FP_BertAttention\n",
    "from utils_glue import *\n",
    "from bertviz import model_view\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0 \n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def do_eval(model, task_name, eval_dataloader,\n",
    "            device, output_mode, eval_labels, num_labels, teacher_model=None):\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "\n",
    "    for batch_ in tqdm(eval_dataloader, desc=\"Inference\"):\n",
    "        batch_ = tuple(t.to(device) for t in batch_)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch_\n",
    "\n",
    "            # teacher attnmap test\n",
    "            if teacher_model is not None:\n",
    "                \n",
    "                # logits, _, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids, segment_ids, input_mask)\n",
    "                \n",
    "                # # logits, _, _, _, _ = model(input_ids, segment_ids, input_mask, teacher_probs=teacher_probs)\n",
    "                # logits, _, _, _, _ = model(input_ids, segment_ids, input_mask, teacher_probs=(teacher_probs, teacher_values, teacher_reps))\n",
    "                teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids, segment_ids, input_mask)\n",
    "                logits, student_atts, student_reps, student_probs, student_values  = model(input_ids, segment_ids, input_mask, teacher_outputs=(teacher_probs, teacher_values, teacher_reps, teacher_logits, teacher_atts))\n",
    "            else:\n",
    "                logits, _, _, _, _ = model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "        # create eval loss and other metric required by the task\n",
    "        if output_mode == \"classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        elif output_mode == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = preds[0]\n",
    "    if output_mode == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif output_mode == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result = compute_metrics(task_name, preds, eval_labels.numpy())\n",
    "    result['eval_loss'] = eval_loss\n",
    "    return result\n",
    "\n",
    "processors = {\n",
    "    \"cola\": ColaProcessor,\n",
    "    \"mnli\": MnliProcessor,\n",
    "    \"mnli-mm\": MnliMismatchedProcessor,\n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    \"sst-2\": Sst2Processor,\n",
    "    \"sts-b\": StsbProcessor,\n",
    "    \"qqp\": QqpProcessor,\n",
    "    \"qnli\": QnliProcessor,\n",
    "    \"rte\": RteProcessor   \n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "        \"cola\": \"classification\",\n",
    "        \"mnli\": \"classification\",\n",
    "        \"mrpc\": \"classification\",\n",
    "        \"sst-2\": \"classification\",\n",
    "        \"sts-b\": \"regression\",\n",
    "        \"qqp\": \"classification\",\n",
    "        \"qnli\": \"classification\",\n",
    "        \"rte\": \"classification\"\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "        \"cola\": {\"max_seq_length\": 64,\"batch_size\":16,\"eval_step\": 400}, # No Aug : 50 Aug : 400\n",
    "        \"mnli\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":8000},\n",
    "        \"mrpc\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":20},\n",
    "        \"sst-2\": {\"max_seq_length\": 64,\"batch_size\":32,\"eval_step\":100},\n",
    "        \"sts-b\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":100},\n",
    "        \"qqp\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":1000},\n",
    "        \"qnli\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":1000},\n",
    "        \"rte\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":100}\n",
    "    }\n",
    "\n",
    "def get_tensor_data(output_mode, features):\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "\n",
    "    all_seq_lengths = torch.tensor([f.seq_length for f in features], dtype=torch.long)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    tensor_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_label_ids, all_seq_lengths)\n",
    "    return tensor_data, all_label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6460b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"cola\"\n",
    "bert_size = \"base\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else: \n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "    \n",
    "teacher_model = None\n",
    "# torch.cuda.empty_cache()\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084fe35",
   "metadata": {},
   "source": [
    "# DEVICE / DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5214fdf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/13 01:06:01 AM Writing example 0 of 8551\n",
      "07/13 01:06:01 AM *** Example ***\n",
      "07/13 01:06:01 AM guid: train-0\n",
      "07/13 01:06:01 AM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]\n",
      "07/13 01:06:01 AM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:01 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:01 AM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:01 AM label: 1\n",
      "07/13 01:06:01 AM label_id: 1\n",
      "07/13 01:06:02 AM Writing example 0 of 1043\n",
      "07/13 01:06:02 AM *** Example ***\n",
      "07/13 01:06:02 AM guid: dev-0\n",
      "07/13 01:06:02 AM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]\n",
      "07/13 01:06:02 AM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:02 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:02 AM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 01:06:02 AM label: 1\n",
      "07/13 01:06:02 AM label_id: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "teacher_model_dir = os.path.join(model_dir,task_name)\n",
    "\n",
    "# Processor & Task Info\n",
    "processor = processors[task_name]()\n",
    "output_mode = output_modes[task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "if task_name in default_params:\n",
    "    batch_size = default_params[task_name][\"batch_size\"]\n",
    "    max_seq_length = default_params[task_name][\"max_seq_length\"]\n",
    "    eval_step = default_params[task_name][\"eval_step\"]\n",
    "    \n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(teacher_model_dir, do_lower_case=True)\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "data_dir = os.path.join(\"data\",task_name)\n",
    "processed_data_dir = os.path.join(data_dir,'preprocessed')\n",
    "\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "train_features = convert_examples_to_features(train_examples, label_list,\n",
    "                                max_seq_length, tokenizer, output_mode)\n",
    "\n",
    "len_train_data = int(len(train_features) * 1)\n",
    "train_features = train_features[:len_train_data]\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, output_mode)\n",
    "# dev_file = train_file = os.path.join(processed_data_dir,'dev.pkl') \n",
    "# eval_features = pickle.load(open(dev_file,'rb'))\n",
    "\n",
    "train_data, train_labels = get_tensor_data(output_mode, train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "eval_data, eval_labels = get_tensor_data(\"classification\", eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "eval_data, eval_labels = get_tensor_data(output_mode, eval_features)\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)\n",
    "\n",
    "# Sampling Sentence \n",
    "i = 0 \n",
    "# num = 3\n",
    "num = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054111a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/13 01:06:02 AM loading configuration file output/cola/exploration/1SB_M_100/config.json\n",
      "07/13 01:06:04 AM Loading model output/cola/exploration/1SB_M_100/pytorch_model.bin\n",
      "07/13 01:06:05 AM loading model...\n",
      "07/13 01:06:05 AM done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_dir, num_labels=num_labels)\n",
    "# teacher_model.to(device)\n",
    "# teacher_model.eval()\n",
    "\n",
    "st_model_name = \"1SB_M_100\"\n",
    "student_model_dir = os.path.join(output_dir, task_name, \"exploration\", st_model_name)   \n",
    "student_config = BertConfig.from_pretrained(student_model_dir)   \n",
    "student_model = QuantBertForSequenceClassification.from_pretrained(student_model_dir, config = student_config, num_labels=num_labels)\n",
    "student_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9f72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid:  [-1. -1.], Grid:  [-0.9 -1. ], Grid:  [-0.8 -1. ], Grid:  [-0.7 -1. ], Grid:  [-0.6 -1. ], Grid:  [-0.5 -1. ], Grid:  [-0.4 -1. ], Grid:  [-0.3 -1. ], Grid:  [-0.2 -1. ], Grid:  [-0.1 -1. ], Grid:  [ 0. -1.], Grid:  [ 0.1 -1. ], Grid:  [ 0.2 -1. ], Grid:  [ 0.3 -1. ], Grid:  [ 0.4 -1. ], Grid:  [ 0.5 -1. ], Grid:  [ 0.6 -1. ], Grid:  [ 0.7 -1. ], Grid:  [ 0.8 -1. ], Grid:  [ 0.9 -1. ], Grid:  [ 1. -1.], Grid:  [-1.  -0.9], Grid:  [-0.9 -0.9], Grid:  [-0.8 -0.9], Grid:  [-0.7 -0.9], Grid:  [-0.6 -0.9], Grid:  [-0.5 -0.9], Grid:  [-0.4 -0.9], Grid:  [-0.3 -0.9], Grid:  [-0.2 -0.9], Grid:  [-0.1 -0.9], Grid:  [ 0.  -0.9], Grid:  [ 0.1 -0.9], Grid:  [ 0.2 -0.9], Grid:  [ 0.3 -0.9], Grid:  [ 0.4 -0.9], Grid:  [ 0.5 -0.9], Grid:  [ 0.6 -0.9], Grid:  [ 0.7 -0.9], Grid:  [ 0.8 -0.9], Grid:  [ 0.9 -0.9], Grid:  [ 1.  -0.9], Grid:  [-1.  -0.8], Grid:  [-0.9 -0.8], Grid:  [-0.8 -0.8], Grid:  [-0.7 -0.8], Grid:  [-0.6 -0.8], Grid:  [-0.5 -0.8], Grid:  [-0.4 -0.8], Grid:  [-0.3 -0.8], Grid:  [-0.2 -0.8], Grid:  [-0.1 -0.8], Grid:  [ 0.  -0.8], Grid:  [ 0.1 -0.8], Grid:  [ 0.2 -0.8], Grid:  [ 0.3 -0.8], Grid:  [ 0.4 -0.8], Grid:  [ 0.5 -0.8], Grid:  [ 0.6 -0.8], Grid:  [ 0.7 -0.8], Grid:  [ 0.8 -0.8], Grid:  [ 0.9 -0.8], Grid:  [ 1.  -0.8], Grid:  [-1.  -0.7], Grid:  [-0.9 -0.7], Grid:  [-0.8 -0.7], Grid:  [-0.7 -0.7], Grid:  [-0.6 -0.7], Grid:  [-0.5 -0.7], Grid:  [-0.4 -0.7], Grid:  [-0.3 -0.7], Grid:  [-0.2 -0.7], Grid:  [-0.1 -0.7], Grid:  [ 0.  -0.7], Grid:  [ 0.1 -0.7], Grid:  [ 0.2 -0.7], Grid:  [ 0.3 -0.7], Grid:  [ 0.4 -0.7], Grid:  [ 0.5 -0.7], Grid:  [ 0.6 -0.7], Grid:  [ 0.7 -0.7], Grid:  [ 0.8 -0.7], Grid:  [ 0.9 -0.7], Grid:  [ 1.  -0.7], Grid:  [-1.  -0.6], Grid:  [-0.9 -0.6], Grid:  [-0.8 -0.6], Grid:  [-0.7 -0.6], Grid:  [-0.6 -0.6], Grid:  [-0.5 -0.6], Grid:  [-0.4 -0.6], Grid:  [-0.3 -0.6], Grid:  [-0.2 -0.6], Grid:  [-0.1 -0.6], Grid:  [ 0.  -0.6], Grid:  [ 0.1 -0.6], Grid:  [ 0.2 -0.6], Grid:  [ 0.3 -0.6], Grid:  [ 0.4 -0.6], Grid:  [ 0.5 -0.6], Grid:  [ 0.6 -0.6], Grid:  [ 0.7 -0.6], Grid:  [ 0.8 -0.6], Grid:  [ 0.9 -0.6], Grid:  [ 1.  -0.6], Grid:  [-1.  -0.5], Grid:  [-0.9 -0.5], Grid:  [-0.8 -0.5], Grid:  [-0.7 -0.5], Grid:  [-0.6 -0.5], Grid:  [-0.5 -0.5], Grid:  [-0.4 -0.5], Grid:  [-0.3 -0.5], Grid:  [-0.2 -0.5], Grid:  [-0.1 -0.5], Grid:  [ 0.  -0.5], Grid:  [ 0.1 -0.5], Grid:  [ 0.2 -0.5], Grid:  [ 0.3 -0.5], Grid:  [ 0.4 -0.5], Grid:  [ 0.5 -0.5], Grid:  [ 0.6 -0.5], Grid:  [ 0.7 -0.5], Grid:  [ 0.8 -0.5], Grid:  [ 0.9 -0.5], Grid:  [ 1.  -0.5], Grid:  [-1.  -0.4], Grid:  [-0.9 -0.4], Grid:  [-0.8 -0.4], Grid:  [-0.7 -0.4], Grid:  [-0.6 -0.4], Grid:  [-0.5 -0.4], Grid:  [-0.4 -0.4], Grid:  [-0.3 -0.4], Grid:  [-0.2 -0.4], Grid:  [-0.1 -0.4], Grid:  [ 0.  -0.4], Grid:  [ 0.1 -0.4], Grid:  [ 0.2 -0.4], Grid:  [ 0.3 -0.4], Grid:  [ 0.4 -0.4], Grid:  [ 0.5 -0.4], Grid:  [ 0.6 -0.4], Grid:  [ 0.7 -0.4], Grid:  [ 0.8 -0.4], Grid:  [ 0.9 -0.4], Grid:  [ 1.  -0.4], Grid:  [-1.  -0.3], Grid:  [-0.9 -0.3], Grid:  [-0.8 -0.3], Grid:  [-0.7 -0.3], Grid:  [-0.6 -0.3], Grid:  [-0.5 -0.3], Grid:  [-0.4 -0.3], Grid:  [-0.3 -0.3], Grid:  [-0.2 -0.3], Grid:  [-0.1 -0.3], Grid:  [ 0.  -0.3], Grid:  [ 0.1 -0.3], Grid:  [ 0.2 -0.3], Grid:  [ 0.3 -0.3], Grid:  [ 0.4 -0.3], Grid:  [ 0.5 -0.3], Grid:  [ 0.6 -0.3], Grid:  [ 0.7 -0.3], Grid:  [ 0.8 -0.3], Grid:  [ 0.9 -0.3], Grid:  [ 1.  -0.3], Grid:  [-1.  -0.2], Grid:  [-0.9 -0.2], Grid:  [-0.8 -0.2], Grid:  [-0.7 -0.2], Grid:  [-0.6 -0.2], Grid:  [-0.5 -0.2], Grid:  [-0.4 -0.2], Grid:  [-0.3 -0.2], Grid:  [-0.2 -0.2], Grid:  [-0.1 -0.2], Grid:  [ 0.  -0.2], Grid:  [ 0.1 -0.2], Grid:  [ 0.2 -0.2], Grid:  [ 0.3 -0.2], Grid:  [ 0.4 -0.2], Grid:  [ 0.5 -0.2], Grid:  [ 0.6 -0.2], Grid:  [ 0.7 -0.2], Grid:  [ 0.8 -0.2], Grid:  [ 0.9 -0.2], Grid:  [ 1.  -0.2], Grid:  [-1.  -0.1], Grid:  [-0.9 -0.1], Grid:  [-0.8 -0.1], Grid:  [-0.7 -0.1], Grid:  [-0.6 -0.1], Grid:  [-0.5 -0.1], Grid:  [-0.4 -0.1], Grid:  [-0.3 -0.1], Grid:  [-0.2 -0.1], Grid:  [-0.1 -0.1], Grid:  [ 0.  -0.1], Grid:  [ 0.1 -0.1], Grid:  [ 0.2 -0.1], Grid:  [ 0.3 -0.1], Grid:  [ 0.4 -0.1], Grid:  [ 0.5 -0.1], Grid:  [ 0.6 -0.1], Grid:  [ 0.7 -0.1], Grid:  [ 0.8 -0.1], Grid:  [ 0.9 -0.1], Grid:  [ 1.  -0.1], Grid:  [-1.  0.], Grid:  [-0.9  0. ], Grid:  [-0.8  0. ], Grid:  [-0.7  0. ], Grid:  [-0.6  0. ], Grid:  [-0.5  0. ], Grid:  [-0.4  0. ], Grid:  [-0.3  0. ], Grid:  [-0.2  0. ], Grid:  [-0.1  0. ], Grid:  [0. 0.], Grid:  [0.1 0. ], Grid:  [0.2 0. ], Grid:  [0.3 0. ], Grid:  [0.4 0. ], Grid:  [0.5 0. ], Grid:  [0.6 0. ], Grid:  [0.7 0. ], Grid:  [0.8 0. ], Grid:  [0.9 0. ], Grid:  [1. 0.], Grid:  [-1.   0.1], Grid:  [-0.9  0.1], Grid:  [-0.8  0.1], Grid:  [-0.7  0.1], Grid:  [-0.6  0.1], Grid:  [-0.5  0.1], Grid:  [-0.4  0.1], Grid:  [-0.3  0.1], Grid:  [-0.2  0.1], Grid:  [-0.1  0.1], Grid:  [0.  0.1], Grid:  [0.1 0.1], Grid:  [0.2 0.1], Grid:  [0.3 0.1], Grid:  [0.4 0.1], Grid:  [0.5 0.1], Grid:  [0.6 0.1], Grid:  [0.7 0.1], Grid:  [0.8 0.1], Grid:  [0.9 0.1], Grid:  [1.  0.1], Grid:  [-1.   0.2], Grid:  [-0.9  0.2], Grid:  [-0.8  0.2], Grid:  [-0.7  0.2], Grid:  [-0.6  0.2], Grid:  [-0.5  0.2], Grid:  [-0.4  0.2], Grid:  [-0.3  0.2], Grid:  [-0.2  0.2], Grid:  [-0.1  0.2], Grid:  [0.  0.2], Grid:  [0.1 0.2], Grid:  [0.2 0.2], Grid:  [0.3 0.2], Grid:  [0.4 0.2], Grid:  [0.5 0.2], Grid:  [0.6 0.2], Grid:  [0.7 0.2], Grid:  [0.8 0.2], Grid:  [0.9 0.2], Grid:  [1.  0.2], Grid:  [-1.   0.3], Grid:  [-0.9  0.3], Grid:  [-0.8  0.3], Grid:  [-0.7  0.3], Grid:  [-0.6  0.3], Grid:  [-0.5  0.3], Grid:  [-0.4  0.3], Grid:  [-0.3  0.3], Grid:  [-0.2  0.3], Grid:  [-0.1  0.3], Grid:  [0.  0.3], Grid:  [0.1 0.3], Grid:  [0.2 0.3], Grid:  [0.3 0.3], Grid:  [0.4 0.3], Grid:  [0.5 0.3], Grid:  [0.6 0.3], Grid:  [0.7 0.3], Grid:  [0.8 0.3], Grid:  [0.9 0.3], Grid:  [1.  0.3], Grid:  [-1.   0.4], Grid:  [-0.9  0.4], Grid:  [-0.8  0.4], Grid:  [-0.7  0.4], Grid:  [-0.6  0.4], Grid:  [-0.5  0.4], Grid:  [-0.4  0.4], Grid:  [-0.3  0.4], Grid:  [-0.2  0.4], Grid:  [-0.1  0.4], Grid:  [0.  0.4], Grid:  [0.1 0.4], Grid:  [0.2 0.4], Grid:  [0.3 0.4], Grid:  [0.4 0.4], Grid:  [0.5 0.4], Grid:  [0.6 0.4], Grid:  [0.7 0.4], Grid:  [0.8 0.4], Grid:  [0.9 0.4], Grid:  [1.  0.4], Grid:  [-1.   0.5], Grid:  [-0.9  0.5], Grid:  [-0.8  0.5], Grid:  [-0.7  0.5], Grid:  [-0.6  0.5], Grid:  [-0.5  0.5], Grid:  [-0.4  0.5], Grid:  [-0.3  0.5], Grid:  [-0.2  0.5], Grid:  [-0.1  0.5], Grid:  [0.  0.5], Grid:  [0.1 0.5], Grid:  [0.2 0.5], Grid:  [0.3 0.5], Grid:  [0.4 0.5], Grid:  [0.5 0.5], Grid:  [0.6 0.5], Grid:  [0.7 0.5], Grid:  [0.8 0.5], Grid:  [0.9 0.5], Grid:  [1.  0.5], Grid:  [-1.   0.6], Grid:  [-0.9  0.6], Grid:  [-0.8  0.6], Grid:  [-0.7  0.6], Grid:  [-0.6  0.6], Grid:  [-0.5  0.6], Grid:  [-0.4  0.6], Grid:  [-0.3  0.6], Grid:  [-0.2  0.6], Grid:  [-0.1  0.6], Grid:  [0.  0.6], Grid:  [0.1 0.6], Grid:  [0.2 0.6], Grid:  [0.3 0.6], Grid:  [0.4 0.6], Grid:  [0.5 0.6], Grid:  [0.6 0.6], Grid:  [0.7 0.6], Grid:  [0.8 0.6], Grid:  [0.9 0.6], Grid:  [1.  0.6], Grid:  [-1.   0.7], "
     ]
    }
   ],
   "source": [
    "import ops.tests as tests\n",
    "import ops.datasets as datasets\n",
    "import ops.loss_landscapes as lls\n",
    "\n",
    "scale = 1e-0\n",
    "n = 21\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "metrics_grid = lls.get_loss_landscape(\n",
    "    student_model, 1, train_dataloader, transform=None,\n",
    "    kws=[\"pos_embed\", \"relative_position\"],\n",
    "    x_min=-1.0 * scale, x_max=1.0 * scale, n_x=n, y_min=-1.0 * scale, y_max=1.0 * scale, n_y=n, gpu=gpu\n",
    ")\n",
    "\n",
    "metrics_dir = os.path.join(\"lls_logs\", \"%s_long_losslandscape.csv\" % (task_name))\n",
    "metrics_list = [[*grid, metrics] for grid, metrics in metrics_grid.items()]\n",
    "\n",
    "with open(metrics_dir, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for metrics in metrics_list:\n",
    "            writer.writerow(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# load losslandscape raw data of ResNet-50 or ViT-Ti\n",
    "# names = [\"x\", \"y\", \"l1\", \"l2\", \"NLL\", \"Cutoff1\", \"Cutoff2\", \"Acc\", \"Acc-90\", \"Unc\", \"Unc-90\", \"IoU\", \"IoU-90\", \"Freq\", \"Freq-90\", \"Top-5\", \"Brier\", \"ECE\", \"ECSE\"]\n",
    "# path = \"%s/resources/results/cifar100_vit_ti_losslandscape.csv\" % root  # for ViT-Ti\n",
    "\n",
    "names = [\"x\", \"y\", \"NLL\"]\n",
    "data = pd.read_csv(metrics_dir, names=names)\n",
    "data[\"loss\"] = data[\"NLL\"] # + optim_args[\"weight_decay\"] * data[\"l2\"]  # NLL + l2\n",
    "\n",
    "# prepare data\n",
    "p = int(math.sqrt(len(data)))\n",
    "shape = [p, p]\n",
    "xs = data[\"x\"].to_numpy().reshape(shape) \n",
    "ys = data[\"y\"].to_numpy().reshape(shape)\n",
    "zs = data[\"loss\"].to_numpy().reshape(shape)\n",
    "\n",
    "zs = zs - zs[np.isfinite(zs)].min()\n",
    "zs[zs > 42] = np.nan\n",
    "\n",
    "norm = plt.Normalize(zs[np.isfinite(zs)].min(), zs[np.isfinite(zs)].max())  # normalize to [0,1]\n",
    "colors = cm.plasma(norm(zs))\n",
    "rcount, ccount, _ = colors.shape\n",
    "\n",
    "fig = plt.figure(figsize=(4.2, 4), dpi=120)\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "ax.view_init(elev=10, azim=30)  # angle\n",
    "\n",
    "# make the panes transparent\n",
    "ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "# make the grid lines transparent\n",
    "ax.xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "ax.yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "ax.zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "\n",
    "surf = ax.plot_surface(\n",
    "    xs, ys, zs, \n",
    "    rcount=rcount, ccount=ccount,\n",
    "    cmap=plt.cm.coolwarm, shade=False,\n",
    ")\n",
    "surf.set_facecolor((0,0,0,0))\n",
    "\n",
    "# remove white spaces\n",
    "adjust_lim = 1\n",
    "ax.set_xlim(-1 * adjust_lim, 1 * adjust_lim)\n",
    "ax.set_ylim(-1 * adjust_lim, 1 * adjust_lim)\n",
    "ax.set_zlim(0, 1)\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "# ax.axis('off')\n",
    "\n",
    "ls = 9\n",
    "ax.tick_params(axis=\"x\", labelsize=ls)\n",
    "ax.tick_params(axis=\"y\", labelsize=ls)\n",
    "ax.tick_params(axis=\"z\", labelsize=ls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b70aec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0,9]\n",
    "ys[0,9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
