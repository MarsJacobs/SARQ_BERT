{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f99d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pprint\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Set GPU Index to use\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from tqdm import tqdm\n",
    "from transformer import BertForSequenceClassification,WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformer.modeling_quant import BertForSequenceClassification as QuantBertForSequenceClassification\n",
    "from transformer import BertTokenizer\n",
    "from transformer import BertAdam\n",
    "from transformer import BertConfig\n",
    "from transformer import QuantizeLinear, QuantizeAct, BertSelfAttention, FP_BertSelfAttention, ClipLinear\n",
    "from utils_glue import *\n",
    "from bertviz import model_view\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0 \n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def cv_initialize(model, loader, ratio, device):\n",
    "    \n",
    "    def initialize_hook(module, input, output):\n",
    "        if isinstance(module, (QuantizeLinear, QuantizeAct, ClipLinear)):\n",
    "            \"\"\"KDLSQ-BERT ACT Quant init Method\n",
    "            Ref: https://arxiv.org/abs/2101.05938\n",
    "            \"\"\"\n",
    "            if not isinstance(input, torch.Tensor):\n",
    "                input = input[0]\n",
    "        \n",
    "            n = torch.numel(input)\n",
    "            input_sorted, index = torch.sort(input.reshape(-1), descending=False)\n",
    "            \n",
    "            index_min = torch.round(ratio * n / 2)\n",
    "            index_max = n - index_min\n",
    "            \n",
    "            s_init = (input_sorted[int(index_min)].to(device), input_sorted[int(index_max)].to(device))\n",
    "            \n",
    "            # MATPLOT\n",
    "            \n",
    "            fig, [ax1, ax2, ax3] = plt.subplots(1,3, figsize=(16, 4))            \n",
    "            \n",
    "            sns.histplot(data=input.reshape(-1).detach().cpu().numpy(), kde = True, bins=100, ax=ax1)\n",
    "            sns.rugplot(data=input.reshape(-1).detach().cpu().numpy(), ax=ax1)\n",
    "            sns.histplot(data=module.weight.reshape(-1).detach().cpu().numpy(), kde = True, bins=100, ax=ax2)\n",
    "            sns.rugplot(data=module.weight.reshape(-1).detach().cpu().numpy(), ax=ax2)\n",
    "            sns.histplot(data=output.reshape(-1).detach().cpu().numpy(), kde = True, bins=100, ax=ax3)\n",
    "            sns.rugplot(data=output.reshape(-1).detach().cpu().numpy(), ax=ax3)\n",
    "            # fig, [ax1, ax2] = plt.subplots(1,2, figsize=(12, 4))            \n",
    "            \n",
    "            # sns.distplot(input.reshape(-1).detach().cpu().numpy() , hist = True, rug = True, kde = True, bins=100, norm_hist=False, kde_kws=dict(linewidth=0.5), rug_kws=dict(linewidth=0.5), ax=ax1)\n",
    "            # sns.distplot(output.reshape(-1).detach().cpu().numpy() , hist = True, rug = True, kde = True, bins=100, norm_hist=False, kde_kws=dict(linewidth=0.5), rug_kws=dict(linewidth=0.5), ax=ax2)\n",
    "            # # plt.axvline(x=s_init[0].detach().cpu().numpy(), color='r', linestyle='--')\n",
    "            # # plt.axvline(x=s_init[1].detach().cpu().numpy(), color='r', linestyle='--')\n",
    "\n",
    "            ax1.set_xlabel(\"Input Activation\")\n",
    "            # ax2.set_xlabel(\"Output Activation\")\n",
    "            ax2.set_xlabel(\"Module Weight\")\n",
    "            ax3.set_xlabel(\"Output Activation\")\n",
    "            \n",
    "            ax1.set_ylabel(\"Density\")\n",
    "            ax2.set_ylabel(\"Density\")\n",
    "            ax3.set_ylabel(\"Density\")\n",
    "\n",
    "            ax1.set_title(f\"{module.name} Input ACT histogram\")\n",
    "            # ax2.set_title(f\"{module.name} Output ACT histogram\")\n",
    "            ax2.set_title(f\"{module.name} Weight histogram\")\n",
    "            ax3.set_title(f\"{module.name} Output ACT histogram\")\n",
    "            # plt.savefig(f\"plt_storage/hook_inputs/sst-2-fp/{module.name}.png\")\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "            # module.clip_initialize(s_init)\n",
    "            # logger.info(f\"{module} : min {s_init[0].item()} max {s_init[1].item()}\") \n",
    "\n",
    "    \n",
    "    hooks = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        hook = module.register_forward_hook(initialize_hook)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch        \n",
    "        with torch.no_grad():\n",
    "            student_logits, student_atts, student_reps, student_probs, student_values = model(input_ids, segment_ids, input_mask, teacher_probs=None)\n",
    "        break\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            #vocab[token] = index\n",
    "            vocab[index] = token\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def attention_pattern(model, loader, device):\n",
    "    \n",
    "    def initialize_hook(module, input, output):\n",
    "        if isinstance(module, BertSelfAttention):\n",
    "            \n",
    "            attn_mask = input[1]\n",
    "            attention_output = output[-2]\n",
    "            \n",
    "            seq_length = (attn_mask == 0).sum()\n",
    "            \n",
    "            print(attention_output[0,:,:seq_length,seq_length-1].mean().item())\n",
    "            \n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        hook = module.register_forward_hook(initialize_hook)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch        \n",
    "        with torch.no_grad():\n",
    "            student_logits, student_atts, student_reps, student_probs, student_values = model(input_ids, segment_ids, input_mask)\n",
    "        break\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "        \n",
    "def get_tensor_data(output_mode, features):\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "\n",
    "    all_seq_lengths = torch.tensor([f.seq_length for f in features], dtype=torch.long)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    tensor_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_label_ids, all_seq_lengths)\n",
    "    return tensor_data, all_label_ids\n",
    "\n",
    "def do_logging(run, student_model, teacher_model, test_dataloader, device, global_step, args, vocab):\n",
    "    \n",
    "    if args.bert == \"large\":\n",
    "        layer_num = 24\n",
    "        head_num = 16\n",
    "    else:\n",
    "        layer_num = 12\n",
    "        head_num = 12\n",
    "        \n",
    "    nb_steps = 0\n",
    "    \n",
    "    kl_div_sum = [0 for i in range(layer_num)]\n",
    "    st_sep_avg_sum = [0 for i in range(layer_num)]; st_cls_avg_sum = [0 for i in range(layer_num)]; tc_sep_avg_sum = [0 for i in range(layer_num)]; tc_cls_avg_sum = [0 for i in range(layer_num)]\n",
    "    cover_sum = [0 for i in range(layer_num)]\n",
    "    cover_teacher_sum = [0 for i in range(layer_num)]\n",
    "    \n",
    "    batch_num = 0\n",
    "    \n",
    "    for batch_ in tqdm(test_dataloader, desc=\"Logging Test\", mininterval=0.01, ascii=True, leave=False):\n",
    "        batch_ = tuple(t.to(device) for t in batch_)\n",
    "        \n",
    "        if batch_num >= 1: # Visualize Attention Map only First Batch \n",
    "            args.log_map = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, segment_ids, label_id, seq_length = batch_\n",
    "\n",
    "            teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids, segment_ids, input_mask)\n",
    "            student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids, segment_ids, input_mask, teacher_probs=teacher_probs)\n",
    "            \n",
    "            # Layer\n",
    "            for i, (student_prob, teacher_prob) in enumerate(zip(student_probs, teacher_probs)): \n",
    "\n",
    "                # Head\n",
    "                for head in range(head_num):\n",
    "                    \n",
    "                    if args.log_map:\n",
    "                        \n",
    "                        word_list = []\n",
    "                        \n",
    "                        for word in range(seq_length):\n",
    "                            word_list.append(vocab[input_ids[0][word].item()])\n",
    "                        \n",
    "                        student_prob_map = student_prob[0][head][:seq_length,:seq_length].clone().detach().cpu().numpy()\n",
    "                        teacher_prob_map = teacher_prob[0][head][:seq_length,:seq_length].clone().detach().cpu().numpy()\n",
    "                        \n",
    "                        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16,8))\n",
    "                        ax1.set_title(f\"{i}th Layer {head}th Head Teacher\")\n",
    "                        heatmap = ax1.pcolor(teacher_prob_map, cmap=plt.cm.Blues)\n",
    "    \n",
    "                        ax1.set_xticks(numpy.arange(teacher_prob_map.shape[1]) + 0.5, minor=False)\n",
    "                        ax1.set_yticks(numpy.arange(teacher_prob_map.shape[0]) + 0.5, minor=False)\n",
    "                        \n",
    "                        ax1.set_xlim(0, int(teacher_prob_map.shape[1]))\n",
    "                        ax1.set_ylim(0, int(teacher_prob_map.shape[0]))\n",
    "\n",
    "                        ax1.invert_yaxis()\n",
    "                        ax1.xaxis.tick_top()\n",
    "\n",
    "                        ax1.set_xticklabels(word_list, minor=False)\n",
    "                        ax1.set_yticklabels(word_list, minor=False)\n",
    "\n",
    "                        plt.xticks(rotation=45)\n",
    "                        \n",
    "                        ax2.set_title(f\"{i}th Layer {head}th Head Student\")\n",
    "                        heatmap = ax2.pcolor(student_prob_map, cmap=plt.cm.Blues)\n",
    "\n",
    "                        ax2.set_xticks(numpy.arange(student_prob_map.shape[1]) + 0.5, minor=False)\n",
    "                        ax2.set_yticks(numpy.arange(student_prob_map.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "                        ax2.set_xlim(0, int(student_prob_map.shape[1]))\n",
    "                        ax2.set_ylim(0, int(student_prob_map.shape[0]))\n",
    "\n",
    "                        ax2.invert_yaxis()\n",
    "                        ax2.xaxis.tick_top()\n",
    "\n",
    "                        ax2.set_xticklabels(word_list, minor=False)\n",
    "                        ax2.set_yticklabels(word_list, minor=False)\n",
    "\n",
    "                        plt.xticks(rotation=45)\n",
    "                        \n",
    "                        plt_folder_name = os.path.join(\"plt_storage\" + \"/\" + args.exp_name)\n",
    "                        if not os.path.exists(plt_folder_name):\n",
    "                            os.mkdir(plt_folder_name)          \n",
    "                        plt_folder_name = os.path.join(plt_folder_name, f\"step_{global_step}\")\n",
    "                        if not os.path.exists(plt_folder_name):\n",
    "                            os.mkdir(plt_folder_name)                        \n",
    "                        plt.savefig(plt_folder_name + \"/\" + f\"L{i}_H{head}.png\")\n",
    "                        plt.close()\n",
    "                        \n",
    "\n",
    "                    if args.log_metric:\n",
    "                        \n",
    "                        student_prob = student_prob\n",
    "                        teacher_prob = teacher_prob\n",
    "\n",
    "                        # Attention Map\n",
    "                        student_attn_map = student_prob[0][head][:seq_length,:seq_length].clone().detach()\n",
    "                        teacher_attn_map = teacher_prob[0][head][:seq_length,:seq_length].clone().detach()\n",
    "\n",
    "                        # KL Divergence\n",
    "                        kl_div = F.kl_div(student_attn_map.log(), teacher_attn_map, reduction='batchmean')\n",
    "                        kl_div_sum[i] += kl_div\n",
    "\n",
    "                        # Special Token Prob Mean\n",
    "                        st_sep_avg = student_attn_map[:,-1].mean()\n",
    "                        st_cls_avg = student_attn_map[:,0].mean()\n",
    "                        st_sep_avg_sum[i] += st_sep_avg\n",
    "                        st_cls_avg_sum[i] += st_cls_avg\n",
    "                        \n",
    "                        # Ground Truth\n",
    "                        tc_sep_avg = teacher_attn_map[:,-1].mean()\n",
    "                        tc_cls_avg = teacher_attn_map[:,0].mean()\n",
    "                        tc_sep_avg_sum[i] += tc_sep_avg\n",
    "                        tc_cls_avg_sum[i] += tc_cls_avg\n",
    "\n",
    "                        # Coverage Test\n",
    "                        coverage_head_sum = 0\n",
    "                        coverage_teacher_head_sum = 0\n",
    "                        for k in range(student_attn_map.shape[0]):\n",
    "                            st_argsort = student_attn_map[k].sort(descending=True)[1]\n",
    "                            tc_argsort = teacher_attn_map[k].sort(descending=True)[1][:args.tc_top_k] # Top-5\n",
    "                            \n",
    "                            max_idx = 0\n",
    "                            for idx in tc_argsort: # Teacher Top-5                             \n",
    "                                tmp = torch.where(st_argsort == idx)\n",
    "                                max_idx = max(tmp[0].item(), max_idx)\n",
    "                            \n",
    "                            coverage_ratio = max_idx / student_attn_map.shape[0]\n",
    "                            coverage_teacher_ratio = (args.tc_top_k - 1) / student_attn_map.shape[0]\n",
    "                            coverage_head_sum += coverage_ratio\n",
    "                            coverage_teacher_head_sum += coverage_teacher_ratio\n",
    "                        \n",
    "                        coverage_head = coverage_head_sum / student_attn_map.shape[0]\n",
    "                        coverage_teacher_head = coverage_teacher_head_sum / student_attn_map.shape[0]\n",
    "                        \n",
    "                        cover_sum[i] += coverage_head\n",
    "                        cover_teacher_sum[i] += coverage_teacher_head\n",
    "                        \n",
    "                        nb_steps += 1\n",
    "        \n",
    "        batch_num = batch_num + 1\n",
    "    \n",
    "    if args.log_metric:\n",
    "        nb_steps = nb_steps / 12\n",
    "        \n",
    "        for l in range(12):\n",
    "            run[f\"attn/L{l}_KLdiv_mean\"].log(value=kl_div_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_st_SepProb_mean\"].log(value=st_sep_avg_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_st_ClsProb_mean\"].log(value=st_cls_avg_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_tc_SepProb_mean\"].log(value=tc_sep_avg_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_tc_ClsProb_mean\"].log(value=tc_cls_avg_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_st_cover_mean\"].log(value=cover_sum[l] / nb_steps, step=global_step)\n",
    "            run[f\"attn/L{l}_tc_cover_mean\"].log(value=cover_teacher_sum[l] / nb_steps, step=global_step)\n",
    "\n",
    "    args.log_map = True                    \n",
    "\n",
    "\n",
    "def do_eval(model, task_name, eval_dataloader,\n",
    "            device, output_mode, eval_labels, num_labels, teacher_model=None):\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "\n",
    "    for batch_ in tqdm(eval_dataloader, desc=\"Inference\"):\n",
    "        batch_ = tuple(t.to(device) for t in batch_)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch_\n",
    "\n",
    "            # teacher attnmap test\n",
    "            if teacher_model is not None:\n",
    "                logits, teacher_atts, _, teacher_probs, _ = teacher_model(input_ids, segment_ids, input_mask)\n",
    "                # teacher_probs = 0\n",
    "                logits, _, _, _, _ = model(input_ids, segment_ids, input_mask, teacher_outputs=None)\n",
    "            else:\n",
    "                logits, _, _, _, _ = model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "        # create eval loss and other metric required by the task\n",
    "        if output_mode == \"classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        elif output_mode == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = preds[0]\n",
    "    if output_mode == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif output_mode == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result = compute_metrics(task_name, preds, eval_labels.numpy())\n",
    "    result['eval_loss'] = eval_loss\n",
    "    return result\n",
    "\n",
    "def soft_cross_entropy(predicts, targets):\n",
    "    student_likelihood = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "    targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "    return torch.sum((- targets_prob * student_likelihood), dim=-1).mean()\n",
    "\n",
    "processors = {\n",
    "    \"cola\": ColaProcessor,\n",
    "    \"mnli\": MnliProcessor,\n",
    "    \"mnli-mm\": MnliMismatchedProcessor,\n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    \"sst-2\": Sst2Processor,\n",
    "    \"sts-b\": StsbProcessor,\n",
    "    \"qqp\": QqpProcessor,\n",
    "    \"qnli\": QnliProcessor,\n",
    "    \"rte\": RteProcessor   \n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "        \"cola\": \"classification\",\n",
    "        \"mnli\": \"classification\",\n",
    "        \"mrpc\": \"classification\",\n",
    "        \"sst-2\": \"classification\",\n",
    "        \"sts-b\": \"regression\",\n",
    "        \"qqp\": \"classification\",\n",
    "        \"qnli\": \"classification\",\n",
    "        \"rte\": \"classification\"\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "        \"cola\": {\"max_seq_length\": 64,\"batch_size\":1,\"eval_step\": 50}, # No Aug : 50 Aug : 400\n",
    "        \"mnli\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\":8000},\n",
    "        \"mrpc\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\":100},\n",
    "        \"sst-2\": {\"max_seq_length\": 64,\"batch_size\":1,\"eval_step\":100},\n",
    "        \"sts-b\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\":100},\n",
    "        \"qqp\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\":1000},\n",
    "        \"qnli\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\":1000},\n",
    "        \"rte\": {\"max_seq_length\": 128,\"batch_size\":1,\"eval_step\": 20}\n",
    "    }\n",
    "\n",
    "from bertviz import head_view, model_view\n",
    "# from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "import bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9bf6b",
   "metadata": {},
   "source": [
    "# GLUE Task Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f78270",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"qnli\"\n",
    "bert_size = \"base\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else:\n",
    "    layer_num = 12\n",
    "    head_num = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b94ac6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Dir, Device & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fec849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "student_model_dir = os.path.join(model_dir,task_name)\n",
    "# student_model_dir = os.path.join(output_dir, task_name, \"quant\", \"ternary_save\") # DA-A4W2 51.2\n",
    "student_model_dir = os.path.join(output_dir, task_name, \"exploration\", \"1SB_O\")\n",
    "teacher_model_dir = os.path.join(model_dir,task_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb545e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/13 07:53:51 PM Writing example 0 of 5463\n",
      "07/13 07:53:51 PM *** Example ***\n",
      "07/13 07:53:51 PM guid: dev_matched-0\n",
      "07/13 07:53:51 PM tokens: [CLS] what came into force after the new constitution was herald ? [SEP] as of that day , the new constitution herald ##ing the second republic came into force . [SEP]\n",
      "07/13 07:53:51 PM input_ids: 101 2054 2234 2046 2486 2044 1996 2047 4552 2001 9536 1029 102 2004 1997 2008 2154 1010 1996 2047 4552 9536 2075 1996 2117 3072 2234 2046 2486 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 07:53:51 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 07:53:51 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/13 07:53:51 PM label: entailment\n",
      "07/13 07:53:51 PM label_id: 0\n"
     ]
    }
   ],
   "source": [
    "# Processor & Task Info\n",
    "processor = processors[task_name]()\n",
    "output_mode = output_modes[task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "if task_name in default_params:\n",
    "    batch_size = default_params[task_name][\"batch_size\"]\n",
    "    max_seq_length = default_params[task_name][\"max_seq_length\"]\n",
    "    eval_step = default_params[task_name][\"eval_step\"]\n",
    "    \n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(teacher_model_dir, do_lower_case=True)\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "data_dir = os.path.join(\"data\",task_name)\n",
    "processed_data_dir = os.path.join(data_dir,'preprocessed')\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, output_mode)\n",
    "# dev_file = train_file = os.path.join(processed_data_dir,'dev.pkl') \n",
    "# eval_features = pickle.load(open(dev_file,'rb'))\n",
    "\n",
    "eval_data, eval_labels = get_tensor_data(\"classification\", eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "eval_data, eval_labels = get_tensor_data(output_mode, eval_features)\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b63b5",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a8929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/13 07:53:56 PM Loading model models/qnli/pytorch_model.bin\n",
      "07/13 07:53:56 PM loading model...\n",
      "07/13 07:53:56 PM done!\n",
      "07/13 07:53:56 PM Weights from pretrained model not used in BertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "07/13 07:53:56 PM loading configuration file output/qnli/exploration/1SB_O/config.json\n",
      "07/13 07:53:58 PM Loading model output/qnli/exploration/1SB_O/pytorch_model.bin\n",
      "07/13 07:54:00 PM loading model...\n",
      "07/13 07:54:00 PM done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "build_tc = 1\n",
    "build_st = 1\n",
    "\n",
    "if build_tc:\n",
    "    # Teacher Model Build\n",
    "    teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_dir, num_labels=num_labels)\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    model = teacher_model\n",
    "\n",
    "if build_st:\n",
    "    # Student Model Build\n",
    "    student_config = BertConfig.from_pretrained(student_model_dir,\n",
    "                                                    quantize_act=True,\n",
    "                                                    quantize_weight=True,\n",
    "                                                    weight_bits = 2, # Always Ternary when \"quantize_weight = True\"\n",
    "                                                    input_bits = 8,\n",
    "                                                    clip_val = 2.5,\n",
    "                                                    quantize = True,\n",
    "                                                    ffn_q_1 = True,\n",
    "                                                    ffn_q_2 = True,\n",
    "                                                    qkv_q = True,\n",
    "                                                    emb_q = True,\n",
    "                                                    cls_q = True,\n",
    "                                                    clipping = False,\n",
    "                                                    layer_num = -1,\n",
    "                                                    mean_scale = 0.7,\n",
    "                                                    quantizer = \"ternary\",\n",
    "                                                    act_quantizer = \"ternary\",\n",
    "                                                    init_scaling = 1,\n",
    "                                                    clip_ratio = 1,\n",
    "                                                    gradient_scaling = False,\n",
    "                                                    clip_method = \"minmax\",\n",
    "                                                    teacher_attnmap = False,\n",
    "                                                    parks = False,\n",
    "                                                    stop_grad = False,\n",
    "                                                    qk_FP = False,\n",
    "                                                    map=False,\n",
    "                                                    act_method = \"clipping\"\n",
    "                                                    )\n",
    "\n",
    "    student_model = QuantBertForSequenceClassification.from_pretrained(student_model_dir, config = student_config, num_labels=num_labels)\n",
    "    student_model.to(device)\n",
    "    model = student_model\n",
    "    print()\n",
    "\n",
    "    # Quantization Option ACT/WEIGHT\n",
    "    for name, module in student_model.named_modules():\n",
    "        if isinstance(module, (QuantizeLinear, QuantizeAct, ClipLinear)):    \n",
    "            module.act_flag = True\n",
    "            module.weight_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0dfd7",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c515c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_st = 1\n",
    "eval_tc = 1\n",
    "\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n",
    "\n",
    "if eval_st:\n",
    "    print(\"Student Model Inferece\")\n",
    "    student_model.eval()\n",
    "    student_result = do_eval(student_model, task_name, eval_dataloader, device, output_mode, eval_labels, num_labels, teacher_model=teacher_model)\n",
    "    print(f\"Student Result : {student_result}\")\n",
    "\n",
    "if eval_tc:\n",
    "    print(\"Teacher Model Inferece\")\n",
    "    teacher_result = do_eval(teacher_model, task_name, eval_dataloader, device, output_mode, eval_labels, num_labels)\n",
    "    print(f\"Teacher Result : {teacher_result}\")\n",
    "    \n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Sentence \n",
    "i = 0 \n",
    "# num = \n",
    "num = 0\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    model.train()\n",
    "            \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch\n",
    "    i = i + 1\n",
    "    if i == num:\n",
    "        break\n",
    "\n",
    "seq_length = seq_lengths.item()\n",
    "\n",
    "input_ids_sliced = input_ids[:,:seq_length]\n",
    "input_id = []\n",
    "for i in input_ids_sliced[0]:\n",
    "    input_id.append(i.item())\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "\n",
    "\n",
    "\n",
    "sample_sentence_a = str()\n",
    "sample_sentence_b = str()\n",
    "index = 0\n",
    "\n",
    "for i, word in enumerate(tokens[1:-1]):\n",
    "    if word == \"[SEP]\":\n",
    "        break\n",
    "    sample_sentence_a += word\n",
    "    sample_sentence_a += \" \"\n",
    "index = i\n",
    "\n",
    "for i, word in enumerate(tokens[index+2:-1]):\n",
    "    if word == \"[SEP]\":\n",
    "        break\n",
    "    sample_sentence_b += word\n",
    "    sample_sentence_b += \" \"\n",
    "\n",
    "sep_index = torch.where(input_ids[0] == 102)[0]\n",
    "\n",
    "if len(sample_sentence_b) > 1:\n",
    "    sample_sentence_b_start = segment_ids[0].tolist().index(1)\n",
    "else:\n",
    "    sample_sentence_b_start = None\n",
    "\n",
    "print(f\"input_ids : {input_ids_sliced}\")\n",
    "print(f\"tokens : {tokens}\")\n",
    "print(f\"A : {sample_sentence_a}\")\n",
    "print(f\"B : {sample_sentence_b}\")\n",
    "print(sep_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265abf87",
   "metadata": {},
   "source": [
    "## BERTViz Model View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d1da2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "bertviz_neuron_tc = 1\n",
    "bertviz_neuron_st = 0\n",
    "bertviz_model_tc = 0\n",
    "bertviz_model_st = 0\n",
    "\n",
    "# Quantization Setting\n",
    "if bertviz_neuron_st or bertviz_model_st:\n",
    "    for name, module in student_model.named_modules():\n",
    "            if isinstance(module, (QuantizeLinear, ClipLinear)):    \n",
    "                module.act_flag = True\n",
    "                module.weight_flag = True\n",
    "            if isinstance(module, QuantizeAct):    \n",
    "                module.act_flag = True\n",
    "                module.weight_flag = True\n",
    "\n",
    "if bertviz_neuron_tc or bertviz_neuron_st:\n",
    "    if bertviz_neuron_st:\n",
    "        for name, module in student_model.named_modules():\n",
    "                if isinstance(module, BertSelfAttention):    \n",
    "                    module.output_bertviz = True\n",
    "    if bertviz_neuron_tc:\n",
    "        for name, module in teacher_model.named_modules():\n",
    "                if isinstance(module, FP_BertSelfAttention):    \n",
    "                    module.output_bertviz = True\n",
    "\n",
    "    model_type = 'bert'\n",
    "    model_version = 'bert-base-uncased'\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "    if bertviz_neuron_tc:\n",
    "        if len(sample_sentence_b) > 1:\n",
    "            show(teacher_model.cpu(), model_type, tokenizer, sample_sentence_a, sample_sentence_b, display_mode=\"light\")\n",
    "        else:\n",
    "            show(teacher_model.cpu(), model_type, tokenizer, sample_sentence_a,display_mode=\"light\")\n",
    "    if bertviz_neuron_st:\n",
    "        if len(sample_sentence_b) > 1:\n",
    "            show(student_model.cpu(), model_type, tokenizer, sample_sentence_a, sample_sentence_b, display_mode=\"light\")\n",
    "        else:\n",
    "            show(student_model.cpu(), model_type, tokenizer, sample_sentence_a,display_mode=\"light\")\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else:\n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "    \n",
    "all_layers = list(range(layer_num))\n",
    "layers_to_show = all_layers[20:]\n",
    "\n",
    "if bertviz_model_tc or bertviz_model_st:\n",
    "    \n",
    "    if bertviz_model_tc:\n",
    "        print(\"teacher_map\")\n",
    "        for name, module in teacher_model.named_modules():\n",
    "                    if isinstance(module, FP_BertSelfAttention):    \n",
    "                        module.output_bertviz = False\n",
    "        teacher_model.eval()\n",
    "        teacher_model.to(device)\n",
    "        teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "        model_view(teacher_probs, tokens, include_layers=layers_to_show,  display_mode=\"light\")\n",
    "        \n",
    "    if bertviz_model_st:\n",
    "        print(\"student_map\")\n",
    "        for name, module in student_model.named_modules():\n",
    "                    if isinstance(module, BertSelfAttention):    \n",
    "                        module.output_bertviz = False\n",
    "        student_model.eval()\n",
    "        student_model.to(device)\n",
    "        student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_outputs=None)\n",
    "        model_view(student_probs, tokens, sample_sentence_b_start,include_layers=layers_to_show, display_mode=\"light\")# , include_layers=[0, 1])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf6575",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "mse_func = MSELoss()\n",
    "loss_cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "norm_func = torch.linalg.norm\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else:\n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "\n",
    "attention_mean_check = 0\n",
    "cover_mean_check = 0\n",
    "kl_div_check = 1\n",
    "mse_check = 0\n",
    "attnmap_mse_check = 0\n",
    "norm_check = 0\n",
    "\n",
    "exclude_sep = 0\n",
    "\n",
    "for name, module in student_model.named_modules():\n",
    "            if isinstance(module, BertSelfAttention):    \n",
    "                module.output_bertviz = False\n",
    "for name, module in teacher_model.named_modules():\n",
    "            if isinstance(module, FP_BertSelfAttention):    \n",
    "                module.output_bertviz = False\n",
    "                \n",
    "for name, module in student_model.named_modules():\n",
    "            if isinstance(module, (QuantizeLinear, ClipLinear, QuantizeAct)):    \n",
    "                module.act_flag = True\n",
    "                module.weight_flag = True\n",
    "\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "\n",
    "if cover_mean_check:\n",
    "    print(\"COVER MEAN CHECK\")\n",
    "    top_k = 5\n",
    "\n",
    "    for i in range(layer_num):\n",
    "        teacher = teacher_probs[i][0]\n",
    "        student = student_probs[i][0]\n",
    "\n",
    "        head_sum = 0\n",
    "        for h in range(head_num):\n",
    "            coverage_head_sum = 0\n",
    "            for row in range(seq_length-1):\n",
    "                if exclude_sep:\n",
    "                    tc_argsort = teacher[h][:seq_length-1,:seq_length-1].sort(descending=True)[1][row][:top_k] # top-k\n",
    "                    st_argsort = student[h][:seq_length-1,:seq_length-1].sort(descending=True)[1][row]\n",
    "                tc_argsort = teacher[h].sort(descending=True)[1][row][:top_k] # top-k\n",
    "                st_argsort = student[h].sort(descending=True)[1][row]\n",
    "\n",
    "                max_idx = 0\n",
    "                for idx in tc_argsort:\n",
    "                    tmp = torch.where(st_argsort == idx)\n",
    "                    max_idx = max(tmp[0].item(), max_idx)\n",
    "\n",
    "                coverage_ratio = max_idx / student.shape[1]\n",
    "                coverage_head_sum += coverage_ratio\n",
    "\n",
    "                # print(f\"H{h} : {coverage_head_sum/seq_length}\")\n",
    "\n",
    "            head_sum += coverage_head_sum / seq_length\n",
    "        print(head_sum / head_num)\n",
    "\n",
    "if kl_div_check:\n",
    "    print(\"KL DIV CHECK\")\n",
    "    for i in range(layer_num):\n",
    "        if exclude_sep:\n",
    "            if len(sep_index) == 2:\n",
    "                teacher_atts[i][:,:,:,sep_index[0]] = -100000; teacher_atts[i][:,:,:,sep_index[1]] = -100000\n",
    "                student_atts[i][:,:,:,sep_index[0]] = -100000; student_atts[i][:,:,:,sep_index[1]] = -100000\n",
    "            else:\n",
    "                teacher_atts[i][:,:,:,sep_index[0]] = -100000\n",
    "                student_atts[i][:,:,:,sep_index[0]] = -100000\n",
    "                \n",
    "            teacher = torch.nn.Softmax(dim=-1)(teacher_atts[i])\n",
    "            student = torch.nn.Softmax(dim=-1)(student_atts[i])\n",
    "            \n",
    "            student = torch.clamp_min(student, 1e-8)\n",
    "            teacher = torch.clamp_min(teacher, 1e-8)\n",
    "        else:    \n",
    "            teacher = teacher_probs[i]\n",
    "            student = student_probs[i]\n",
    "        \n",
    "        neg_cross_entropy = teacher * torch.log(student) \n",
    "        neg_cross_entropy = torch.sum(neg_cross_entropy, dim=-1)  # (b, h, s, s) -> (b, h, s)\n",
    "        neg_cross_entropy = torch.sum(neg_cross_entropy, dim=-1) / seq_lengths.view(-1, 1)  # (b, h, s) -> (b, h)\n",
    "\n",
    "        # p(t) log p(t) = negative entropy\n",
    "        neg_entropy = teacher * torch.log(teacher) \n",
    "        neg_entropy = torch.sum(neg_entropy, dim=-1)  # (b, h, s, s) -> (b, h, s)\n",
    "        neg_entropy = torch.sum(neg_entropy, dim=-1) / seq_lengths.view(-1, 1)  # (b, h, s) -> (b, h)\n",
    "\n",
    "        kld_loss = neg_entropy - neg_cross_entropy\n",
    "\n",
    "        kld_loss_sum = torch.sum(kld_loss)\n",
    "        print(kld_loss_sum.item())\n",
    "\n",
    "if mse_check:\n",
    "    for i in range(layer_num):\n",
    "        print(mse_func(teacher_atts[i], student_atts[i]).item())\n",
    "        \n",
    "if attnmap_mse_check:\n",
    "    for i in range(layer_num):\n",
    "        if exclude_sep:\n",
    "            if len(sep_index) == 2:\n",
    "                teacher_atts[i][:,:,:,sep_index[0]] = -100000; teacher_atts[i][:,:,:,sep_index[1]] = -100000\n",
    "                student_atts[i][:,:,:,sep_index[0]] = -100000; student_atts[i][:,:,:,sep_index[1]] = -100000\n",
    "            else:\n",
    "                teacher_atts[i][:,:,:,sep_index[0]] = -100000\n",
    "                student_atts[i][:,:,:,sep_index[0]] = -100000\n",
    "                \n",
    "            teacher = torch.nn.Softmax(dim=-1)(teacher_atts[i])\n",
    "            student = torch.nn.Softmax(dim=-1)(student_atts[i])\n",
    "            print(mse_func(teacher, student).item())\n",
    "        else:    \n",
    "            print(mse_func(teacher_probs[i], student_probs[i]).item())\n",
    "\n",
    "if norm_check:\n",
    "    for i in range(layer_num):\n",
    "        print(mse_func(torch.linalg.norm(teacher_values[i], dim=-1), torch.linalg.norm(student_values[i], dim=-1)).mean().item())\n",
    "        # print(loss_cos(teacher_values[i], student_values[i]).mean().item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0039fe",
   "metadata": {},
   "source": [
    "## Norm-Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ec106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import MSELoss\n",
    "mse_func = MSELoss()\n",
    "loss_cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "norm_func = torch.linalg.norm\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else:\n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "    \n",
    "sep_w_list = [ AverageMeter() for i in range(layer_num) ]; cls_w_list = [ AverageMeter() for i in range(layer_num) ] ;punc_w_list = [ AverageMeter() for i in range(layer_num) ] ;other_w_list = [ AverageMeter() for i in range(layer_num) ]\n",
    "sep_n_list = [ AverageMeter() for i in range(layer_num) ]; cls_n_list = [ AverageMeter() for i in range(layer_num) ] ;punc_n_list = [ AverageMeter() for i in range(layer_num) ] ;other_n_list = [ AverageMeter() for i in range(layer_num) ]\n",
    "sep_v_list = [ AverageMeter() for i in range(layer_num) ]; cls_v_list = [ AverageMeter() for i in range(layer_num) ] ;punc_v_list = [ AverageMeter() for i in range(layer_num) ] ;other_v_list = [ AverageMeter() for i in range(layer_num) ]\n",
    "\n",
    "\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Inference\"):\n",
    "    model.train()\n",
    "            \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch\n",
    "    seq_length = seq_lengths.item()\n",
    "    input_ids_sliced = input_ids[:,:seq_length]\n",
    "    sep_index = torch.where(input_ids[0] == 102)[0]\n",
    "    \n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    \n",
    "    teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "    student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "    probs = student_probs\n",
    "    values = student_values\n",
    "    \n",
    "#     probs = teacher_probs\n",
    "#     values = teacher_values\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    for i in range(layer_num):\n",
    "        # Attention Weight Analysis\n",
    "        if len(sep_index) == 2:\n",
    "            sep_w = (probs[i][0,:,:,sep_index[0]].mean() + probs[i][0,:,:,sep_index[1]].mean() / 2).item()\n",
    "            punc_w = (probs[i][0,:,:,sep_index[0] - 1].mean() + probs[i][0,:,:,sep_index[1] - 1].mean() / 2).item()\n",
    "            sep_w_list[i].update(sep_w)\n",
    "            punc_w_list[i].update(punc_w)\n",
    "        else:\n",
    "            sep_w = probs[i][0,:,:,sep_index[0]].mean().item()\n",
    "            punc_w = probs[i][0,:,:,sep_index[0]-1].mean().item()\n",
    "            sep_w_list[i].update(sep_w)\n",
    "            punc_w_list[i].update(punc_w)\n",
    "        cls_w = probs[i][0,:,:,0].mean().item()\n",
    "        cls_w_list[i].update(cls_w)\n",
    "        other_w_list[i].update(1 - (sep_w + punc_w + cls_w))\n",
    "\n",
    "        # Attention Norm based Analysis (|| alpha f(x) ||)\n",
    "        if len(sep_index) == 2:\n",
    "            sep_n = (norm_func(values[i][0][0,:,sep_index[0],:], dim=-1).mean() + norm_func(values[i][0][0,:,sep_index[1],:], dim=-1).mean() / 2).item()\n",
    "            punc_n = (norm_func(values[i][0][0,:,sep_index[0] - 1,:], dim=-1).mean() + norm_func(values[i][0][0,:,sep_index[1] - 1,:], dim=-1).mean() / 2).item()\n",
    "            sep_n_list[i].update(sep_n)\n",
    "            punc_n_list[i].update(punc_n)\n",
    "        else:\n",
    "            sep_n = norm_func(values[i][0][0,:,sep_index[0],:], dim=-1).mean().item()\n",
    "            punc_n = norm_func(values[i][0][0,:,sep_index[0]-1,:], dim=-1).mean().item()\n",
    "            sep_n_list[i].update(sep_n)\n",
    "            punc_n_list[i].update(punc_n)\n",
    "\n",
    "        cls_n = norm_func(values[i][0][0,:,0,:], dim=-1).mean().item()\n",
    "        cls_n_list[i].update(cls_n)\n",
    "        \n",
    "        values[i][0][0,:,sep_index[0],:] = 0\n",
    "        if len(sep_index) == 2:\n",
    "            values[i][0][0,:,sep_index[1],:] = 0\n",
    "            \n",
    "        values[i][0][0,:,sep_index[0] - 1,:] = 0\n",
    "        if len(sep_index) == 2:\n",
    "            values[i][0][0,:,sep_index[1] - 1,:] = 0\n",
    "        \n",
    "        values[i][0][0,:,0,:] = 0\n",
    "\n",
    "        shape = values[i][0][0,:,:,:].shape\n",
    "        if len(sep_index) == 2:\n",
    "            num = shape[0] * (shape[1] - 5)\n",
    "        else:\n",
    "            num = shape[0] * (shape[1] - 3)\n",
    "\n",
    "        other_n = (norm_func(values[i][1][0,:,:,:], dim=-1).sum() / num).item()\n",
    "        other_n_list[i].update(other_n)\n",
    "\n",
    "        # Attention Norm based Analysis (|| f(x) ||)\n",
    "        if len(sep_index) == 2:\n",
    "            sep_v_list[i].update((norm_func(values[i][1][0,:,sep_index[0],:], dim=-1).mean() + norm_func(values[i][1][0,:,sep_index[1],:], dim=-1).mean() / 2).item())\n",
    "            punc_v_list[i].update((norm_func(values[i][1][0,:,sep_index[0] - 1,:], dim=-1).mean() + norm_func(values[i][1][0,:,sep_index[1] - 1,:], dim=-1).mean() / 2).item())\n",
    "        else:\n",
    "            sep_v_list[i].update(norm_func(values[i][1][0,:,sep_index[0],:], dim=-1).mean().item())\n",
    "            punc_v_list[i].update(norm_func(values[i][1][0,:,sep_index[0]-1,:], dim=-1).mean().item())\n",
    "        cls_v_list[i].update(norm_func(values[i][1][0,:,0,:], dim=-1).mean().item())\n",
    "\n",
    "        values[i][1][0,:,sep_index[0],:] = 0\n",
    "        if len(sep_index) == 2:\n",
    "            values[i][1][0,:,sep_index[1],:] = 0\n",
    "        \n",
    "        values[i][1][0,:,sep_index[0] - 1,:] = 0\n",
    "        if len(sep_index) == 2:\n",
    "            values[i][1][0,:,sep_index[1] - 1,:] = 0\n",
    "        values[i][1][0,:,0,:] = 0\n",
    "\n",
    "        shape = values[i][1][0,:,:,:].shape\n",
    "        if len(sep_index) == 2:\n",
    "            num = shape[0] * (shape[1] - 5)\n",
    "        else:\n",
    "            num = shape[0] * (shape[1] - 3)\n",
    "\n",
    "        other_v_list[i].update((norm_func(values[i][1][0,:,:,:], dim=-1).sum() / num).item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb671dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sep_w_list = list(); l_cls_w_list = list() ;l_punc_w_list = list() ;l_other_w_list = list()\n",
    "l_sep_n_list = list(); l_cls_n_list = list() ;l_punc_n_list = list() ;l_other_n_list = list()\n",
    "l_sep_v_list = list(); l_cls_v_list = list() ;l_punc_v_list = list() ;l_other_v_list = list()\n",
    "\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(3,1, figsize=(12,16))            \n",
    "x_axis = list(range(layer_num))\n",
    "\n",
    "for layer in range(layer_num):\n",
    "    l_sep_w_list.append(sep_w_list[layer].avg); l_sep_n_list.append(sep_n_list[layer].avg); l_sep_v_list.append(sep_v_list[layer].avg)\n",
    "    l_cls_w_list.append(cls_w_list[layer].avg); l_cls_n_list.append(cls_n_list[layer].avg); l_cls_v_list.append(cls_v_list[layer].avg)\n",
    "    l_punc_w_list.append(punc_w_list[layer].avg); l_punc_n_list.append(punc_n_list[layer].avg); l_punc_v_list.append(punc_v_list[layer].avg)\n",
    "    l_other_w_list.append(other_w_list[layer].avg); l_other_n_list.append(other_n_list[layer].avg); l_other_v_list.append(other_v_list[layer].avg) \n",
    "\n",
    "ax1.set_title(\"Attention Weight-based Analysis\")\n",
    "ax1.plot(x_axis, l_sep_w_list, label=\"SEP\", linewidth=3)\n",
    "ax1.plot(x_axis, l_cls_w_list, label=\"CLS\", linewidth=3)\n",
    "ax1.plot(x_axis, l_punc_w_list, label=\". or ,\", linewidth=3)\n",
    "ax1.plot(x_axis, l_other_w_list, label=\"Other\", linewidth=3)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title(\"Attention Norm-based Analysis\")\n",
    "ax2.plot(x_axis, l_sep_n_list, label=\"SEP\", linewidth=3)\n",
    "ax2.plot(x_axis, l_cls_n_list, label=\"CLS\", linewidth=3)\n",
    "ax2.plot(x_axis, l_punc_n_list, label=\". or ,\", linewidth=3)\n",
    "ax2.plot(x_axis, l_other_n_list, label=\"Other\", linewidth=3)\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_title(\"Attention Value-Norm-based Analysis\")\n",
    "ax3.plot(x_axis, l_sep_v_list, label=\"SEP\", linewidth=3)\n",
    "ax3.plot(x_axis, l_cls_v_list, label=\"CLS\", linewidth=3)\n",
    "ax3.plot(x_axis, l_punc_v_list, label=\". or ,\", linewidth=3)\n",
    "ax3.plot(x_axis, l_other_v_list, label=\"Other\", linewidth=3)\n",
    "ax3.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ef071",
   "metadata": {},
   "source": [
    "### Weight - Norm Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee6ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "\n",
    "probs = student_probs\n",
    "values = student_values\n",
    "# probs = teacher_probs\n",
    "# values = teacher_values\n",
    "\n",
    "table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "table_other = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "for l in range(layer_num):\n",
    "    for h in range(head_num):        \n",
    "        prob_sep = ((probs[l][:,h,:,sep_index[0]].mean() + probs[l][:,h,:,sep_index[1]].mean())/2).item()\n",
    "        table_sep[l][h] = prob_sep\n",
    "        prob_cls = probs[l][:,h,:,0].mean().item()\n",
    "        table_cls[l][h] = prob_cls\n",
    "        prob_punc = ((probs[l][:,h,:,sep_index[0]-1].mean() + probs[l][:,h,:,sep_index[1]-1].mean())/2).item()\n",
    "        table_punc[l][h] = prob_punc\n",
    "#         prob_other = 1 - (prob_sep + prob_sep + prob_punc)\n",
    "#         table_other[l][h] = prob_other\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(32,8))\n",
    "ax1.set_xlabel(\"head\", fontsize=30); ax1.set_ylabel(\"layer\", fontsize=30)\n",
    "# ax1.set_title(\"SEP Probability AVG\")\n",
    "heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax1)\n",
    "\n",
    "ax2.set_xlabel(\"head\", fontsize=36); ax2.set_ylabel(\"layer\", fontsize=36)\n",
    "# ax2.set_title(\"CLS Probability AVG\")\n",
    "heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax2)\n",
    "\n",
    "ax3.set_xlabel(\"head\", fontsize=36); ax3.set_ylabel(\"layer\", fontsize=36)\n",
    "# ax3.set_title(\"PUNC Probability AVG\")\n",
    "heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax3)\n",
    "plt.show()\n",
    "# ax4.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# ax4.set_title(\"Other Probability AVG\")\n",
    "# heatmap = ax4.pcolor(table_other, cmap=plt.cm.Blues)\n",
    "\n",
    "\n",
    "norm_type = 0 # 0 : f(x) 1 : p*f(x)\n",
    "table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "# table_other = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "for l in range(layer_num):\n",
    "    for h in range(head_num):        \n",
    "        norm_sep = (norm_func(values[l][norm_type][0,h,sep_index[0],:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1],:], dim=-1).mean() / 2).item()\n",
    "        table_sep[l][h] = norm_sep\n",
    "        norm_cls = norm_func(values[l][norm_type][0,h,0,:], dim=-1).mean().item()\n",
    "        table_cls[l][h] = norm_cls\n",
    "        norm_punc = (norm_func(values[l][norm_type][0,h,sep_index[0] - 1,:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1] - 1,:], dim=-1).mean() / 2).item()\n",
    "        table_punc[l][h] = norm_punc\n",
    "#         prob_other = 1 - (prob_sep + prob_sep + prob_punc)\n",
    "#         table_other[l][h] = prob_other\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(32,8))\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "ax1.set_xlabel(\"head\", fontsize = 28); ax.set_ylabel(\"layer\", fontsize = 28)\n",
    "ax1.set_title(\"SEP ||f(x)|| Norm AVG\")\n",
    "heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "# fig.savefig(\"2SB.png\")\n",
    "fig.colorbar(heatmap, ax=ax1)\n",
    "\n",
    "ax2.set_xlabel(\"head\", fontsize = 28); ax2.set_ylabel(\"layer\", fontsize = 28)\n",
    "ax2.set_title(\"CLS ||f(x)|| Norm AVG\")\n",
    "heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax2)\n",
    "\n",
    "ax3.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax3.set_title(\"PUNC ||f(x)|| Norm AVG\")\n",
    "heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "norm_type = 1 # 0 : f(x) 1 : p*f(x)\n",
    "table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "# table_other = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "for l in range(layer_num):\n",
    "    for h in range(head_num):        \n",
    "        norm_sep = (norm_func(values[l][norm_type][0,h,sep_index[0],:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1],:], dim=-1).mean() / 2).item()\n",
    "        table_sep[l][h] = norm_sep\n",
    "        norm_cls = norm_func(values[l][norm_type][0,h,0,:], dim=-1).mean().item()\n",
    "        table_cls[l][h] = norm_cls\n",
    "        norm_punc = (norm_func(values[l][norm_type][0,h,sep_index[0] - 1,:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1] - 1,:], dim=-1).mean() / 2).item()\n",
    "        table_punc[l][h] = norm_punc\n",
    "#         prob_other = 1 - (prob_sep + prob_sep + prob_punc)\n",
    "#         table_other[l][h] = prob_other\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(32,8))\n",
    "ax1.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax1.set_title(\"SEP ||p*f(x)|| Norm AVG\")\n",
    "heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax1)\n",
    "\n",
    "ax2.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax2.set_title(\"CLS ||p*f(x)|| Norm AVG\")\n",
    "heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax2)\n",
    "\n",
    "ax3.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax3.set_title(\"PUNC ||p*f(x)|| Norm AVG\")\n",
    "heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax3)\n",
    "plt.show()\n",
    "# ax4.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# ax4.set_title(\"Other Probability AVG\")\n",
    "# heatmap = ax4.pcolor(table_other, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4722d72",
   "metadata": {},
   "source": [
    "### Norm Based Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57688e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "st_values = student_values\n",
    "tc_values = teacher_values\n",
    "\n",
    "loss_cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "\n",
    "norm_type = 0 # 0 : f(x) 1 : p*f(x)\n",
    "\n",
    "table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "for l in range(layer_num):\n",
    "    for h in range(head_num): \n",
    "        # cos_sep_1 = loss_cos(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "        # cos_sep_2 = loss_cos(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:]) \n",
    "        cos_sep_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "        cos_sep_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:]) \n",
    "        # table_sep[l][h] = (1-((cos_sep_1 + cos_sep_2) / 2)).item()\n",
    "        table_sep[l][h] = ((cos_sep_1 + cos_sep_2) / 2).item()\n",
    "        \n",
    "        # cos_cls = loss_cos(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "        cos_cls = mse_func(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "        # table_cls[l][h] = (1-cos_cls).item()\n",
    "        table_cls[l][h] = (cos_cls).item()\n",
    "        \n",
    "        # cos_punc_1 = loss_cos(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:]) \n",
    "        # cos_punc_2 = loss_cos(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:]) \n",
    "        cos_punc_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:]) \n",
    "        cos_punc_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:]) \n",
    "        # table_punc[l][h] = (1-((cos_sep_1 + cos_sep_2) / 2)).item()\n",
    "        table_punc[l][h] = ((cos_sep_1 + cos_sep_2) / 2).item()\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(32,8))\n",
    "ax1.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax1.set_title(\"SEP Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax1)\n",
    "\n",
    "ax2.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax2.set_title(\"CLS Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax2)\n",
    "\n",
    "ax3.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax3.set_title(\"PUNC Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax3)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "st_values = student_values\n",
    "tc_values = teacher_values\n",
    "\n",
    "loss_cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "\n",
    "norm_type = 1 # 0 : f(x) 1 : p*f(x)\n",
    "\n",
    "table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "for l in range(layer_num):\n",
    "    for h in range(head_num): \n",
    "        cos_sep_1 = loss_cos(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "        cos_sep_2 = loss_cos(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:])         \n",
    "        table_sep[l][h] = (1-((cos_sep_1 + cos_sep_2) / 2)).item()\n",
    "        \n",
    "        cos_cls = loss_cos(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "        table_cls[l][h] = (1-cos_cls).item()\n",
    "        \n",
    "        cos_punc_1 = loss_cos(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:]) \n",
    "        cos_punc_2 = loss_cos(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:])         \n",
    "        table_punc[l][h] = (1-((cos_sep_1 + cos_sep_2) / 2)).item()\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(32,8))\n",
    "ax1.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax1.set_title(\"SEP Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax1)\n",
    "\n",
    "ax2.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax2.set_title(\"CLS Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax2)\n",
    "\n",
    "ax3.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "ax3.set_title(\"PUNC Cosine Similarity w/ Teacher\")\n",
    "heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "fig.colorbar(heatmap, ax=ax3)\n",
    "plt.show()\n",
    "\n",
    "# # ax4.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# # ax4.set_title(\"Other Probability AVG\")\n",
    "# # heatmap = ax4.pcolor(table_other, cmap=plt.cm.Blues)\n",
    "\n",
    "# norm_type = 1 # 0 : f(x) 1 : p*f(x)\n",
    "# table_sep = [[0] * head_num for i in range(layer_num)]\n",
    "# table_cls = [[0] * head_num for i in range(layer_num)]\n",
    "# table_punc = [[0] * head_num for i in range(layer_num)]\n",
    "# # table_other = [[0] * head_num for i in range(layer_num)]\n",
    "\n",
    "# for l in range(layer_num):\n",
    "#     for h in range(head_num):        \n",
    "#         norm_sep = (norm_func(values[l][norm_type][0,h,sep_index[0],:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1],:], dim=-1).mean() / 2).item()\n",
    "#         table_sep[l][h] = norm_sep\n",
    "#         norm_cls = norm_func(values[l][norm_type][0,h,0,:], dim=-1).mean().item()\n",
    "#         table_cls[l][h] = norm_cls\n",
    "#         norm_punc = (norm_func(values[l][norm_type][0,h,sep_index[0] - 1,:], dim=-1).mean() + norm_func(values[l][norm_type][0,h,sep_index[1] - 1,:], dim=-1).mean() / 2).item()\n",
    "#         table_punc[l][h] = norm_punc\n",
    "# #         prob_other = 1 - (prob_sep + prob_sep + prob_punc)\n",
    "# #         table_other[l][h] = prob_other\n",
    "\n",
    "# fig, [ax1,ax2,ax3] = plt.subplots(1, 3, figsize=(24,8))\n",
    "# ax1.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# ax1.set_title(\"SEP ||p*f(x)|| Norm AVG\")\n",
    "# heatmap = ax1.pcolor(table_sep, cmap=plt.cm.Blues)\n",
    "\n",
    "# ax2.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# ax2.set_title(\"CLS ||p*f(x)|| Norm AVG\")\n",
    "# heatmap = ax2.pcolor(table_cls, cmap=plt.cm.Blues)\n",
    "\n",
    "# ax3.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# ax3.set_title(\"PUNC ||p*f(x)|| Norm AVG\")\n",
    "# heatmap = ax3.pcolor(table_punc, cmap=plt.cm.Blues)\n",
    "# plt.show()\n",
    "# # ax4.set_xlabel(\"head\"); ax1.set_ylabel(\"layer\")\n",
    "# # ax4.set_title(\"Other Probability AVG\")\n",
    "# # heatmap = ax4.pcolor(table_other, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd90fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "norm_type = 0 # 1 : f(x) 0 : p*f(x)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "\n",
    "        \n",
    "#         diff_sep_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "#         diff_sep_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:]) \n",
    "#         sep_list.append(((diff_sep_1 + diff_sep_2) / 2).item())\n",
    "        \n",
    "#         diff_cls = mse_func(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "#         cls_list.append((diff_cls).item())\n",
    "        \n",
    "#         diff_punc_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:])\n",
    "#         diff_punc_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:]) \n",
    "#         punc_list.append(((diff_punc_1 + diff_punc_2) / 2).item())\n",
    "        \n",
    "#         st_values[l][norm_type][0,h,sep_index[0],:] = 0\n",
    "#         tc_values[l][norm_type][0,h,sep_index[0],:] = 0        \n",
    "#         st_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "#         tc_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "        \n",
    "#         st_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "#         tc_values[l][norm_type][0,h,sep_index[0]-1,:] = 0        \n",
    "#         st_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "#         tc_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "        \n",
    "#         st_values[l][norm_type][0,h,0,:] = 0\n",
    "#         tc_values[l][norm_type][0,h,0,:] = 0\n",
    "        \n",
    "        # diff_other = mse_func(st_values[l][norm_type][0,h,:,:], tc_values[l][norm_type][0,h,:,:])\n",
    "        # other_list.append(diff_other.item())\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_prob_kl = kld_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarq_c_prob_kl = kld_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e66cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarq_prob_kl = kld_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_st = 1\n",
    "eval_tc = 0\n",
    "\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n",
    "\n",
    "if eval_st:\n",
    "    print(\"Student Model Inferece\")\n",
    "    student_model.eval()\n",
    "    student_result = do_eval(student_model, task_name, eval_dataloader, device, output_mode, eval_labels, num_labels, teacher_model=teacher_model)\n",
    "    print(f\"Student Result : {student_result}\")\n",
    "\n",
    "if eval_tc:\n",
    "    print(\"Teacher Model Inferece\")\n",
    "    teacher_result = do_eval(teacher_model, task_name, eval_dataloader, device, output_mode, eval_labels, num_labels)\n",
    "    print(f\"Teacher Result : {teacher_result}\")\n",
    "    \n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "mse_func = MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "student_model_dir = os.path.join(model_dir,task_name)\n",
    "\n",
    "# st_model_name = \"ternary_save\"\n",
    "# st_model_name = \"step_2_context\"\n",
    "# st_model_name = \"step_2_output\"\n",
    "# st_model_name = \"step_2\"\n",
    "# st_model_name = \"sarq_step1\"\n",
    "\n",
    "\n",
    "\n",
    "build_tc = 1\n",
    "build_st = 1\n",
    "\n",
    "if build_tc:\n",
    "    # Teacher Model Build\n",
    "    teacher_model_dir = os.path.join(model_dir,task_name)\n",
    "    teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_dir, num_labels=num_labels)\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    model = teacher_model\n",
    "    \n",
    "for st_model_name in [\"1SB_S\", \"1SB_S_M\", \"step_2_S_M\"]:\n",
    "    student_model_dir = os.path.join(output_dir, task_name, \"exploration\", st_model_name)   \n",
    "\n",
    "    if build_st:\n",
    "        # Student Model Build\n",
    "        student_config = BertConfig.from_pretrained(student_model_dir\n",
    "#                                                         quantize_act=True,\n",
    "#                                                         quantize_weight=True,\n",
    "#                                                         weight_bits = 2, # Always Ternary when \"quantize_weight = True\"\n",
    "#                                                         input_bits = 8,\n",
    "#                                                         clip_val = 2.5,\n",
    "#                                                         quantize = True,\n",
    "#                                                         ffn_q_1 = True,\n",
    "#                                                         ffn_q_2 = True,\n",
    "#                                                         qkv_q = True,\n",
    "#                                                         emb_q = True,\n",
    "#                                                         cls_q = True,\n",
    "#                                                         clipping = False,\n",
    "#                                                         layer_num = -1,\n",
    "#                                                         mean_scale = 0.7,\n",
    "#                                                         quantizer = \"ternary\",\n",
    "#                                                         act_quantizer = \"ternary\",\n",
    "#                                                         init_scaling = 1,\n",
    "#                                                         clip_ratio = 1,\n",
    "#                                                         gradient_scaling = False,\n",
    "#                                                         clip_method = \"minmax\",\n",
    "#                                                         teacher_attnmap = False,\n",
    "#                                                         parks = False,\n",
    "#                                                         stop_grad = False,\n",
    "#                                                         qk_FP = True,\n",
    "#                                                         map=False,\n",
    "#                                                         act_method = \"clipping\"\n",
    "                                                        )\n",
    "\n",
    "        student_model = QuantBertForSequenceClassification.from_pretrained(student_model_dir, config = student_config, num_labels=num_labels)\n",
    "        student_model.to(device)\n",
    "        model = student_model\n",
    "        print()\n",
    "\n",
    "        # Quantization Option ACT/WEIGHT\n",
    "        for name, module in student_model.named_modules():\n",
    "            if isinstance(module, (QuantizeLinear, QuantizeAct, ClipLinear)):    \n",
    "                module.act_flag = True\n",
    "                module.weight_flag = True\n",
    "\n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_zip = teacher_model(input_ids_sliced.to(device))\n",
    "    logits, loss, cls_loss, rep_loss, output_loss, attmap_loss, attscore_loss, coeff_list, student_zip = student_model(input_ids_sliced.to(device), teacher_outputs=None)\n",
    "    \n",
    "    \n",
    "    kld_list = []\n",
    "\n",
    "    student_probs = student_zip[1]\n",
    "    kl_loss = torch.nn.KLDivLoss(reduction=\"sum\")\n",
    "\n",
    "    for l in range(layer_num):\n",
    "        for h in range(head_num): \n",
    "            student = student_probs[l][0,h,:,:]\n",
    "            teacher = teacher_probs[l][0,h,:,:]\n",
    "            neg_cross_entropy = teacher * torch.log(student) \n",
    "            neg_cross_entropy = torch.sum(neg_cross_entropy, dim=-1)  # (b, h, s, s) -> (b, h, s)\n",
    "\n",
    "\n",
    "            # p(t) log p(t) = negative entropy\n",
    "            neg_entropy = teacher * torch.log(teacher) \n",
    "            neg_entropy = torch.sum(neg_entropy, dim=-1)  # (b, h, s, s) -> (b, h, s)\n",
    "\n",
    "            kl_div = neg_entropy - neg_cross_entropy\n",
    "            # print(kl_div.mean().item())\n",
    "            kld_list.append(kl_div.mean().item())\n",
    "     \n",
    "    if st_model_name == \"1SB_S\":\n",
    "        print(st_model_name)\n",
    "        t_kld_list = kld_list\n",
    "    elif st_model_name == \"1SB_S_M\":\n",
    "        print(st_model_name)    \n",
    "        s_kld_list = kld_list\n",
    "    elif st_model_name == \"step_2_S_M\":\n",
    "        print(st_model_name)\n",
    "        sc_kld_list = kld_list \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # (layer_context, attention_output, value_layer, self_output_hs)\n",
    "    st_values = student_zip[0]\n",
    "    tc_values = teacher_zip\n",
    "\n",
    "    norm_type = 2 \n",
    "\n",
    "    h_num = 1\n",
    "    sep_list = []\n",
    "    cls_list = []\n",
    "    punc_list = []\n",
    "    other_list = []\n",
    "\n",
    "    for l in range(layer_num):\n",
    "        for h in range(head_num): \n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                diff_sep_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "                diff_sep_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:]) \n",
    "                sep_list.append(((diff_sep_1 + diff_sep_2) / 2).item())\n",
    "            else:\n",
    "                diff_sep = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "                sep_list.append(diff_sep.item())\n",
    "\n",
    "            diff_cls = mse_func(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "            cls_list.append((diff_cls).item())\n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                diff_punc_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:])\n",
    "                diff_punc_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:]) \n",
    "                punc_list.append(((diff_punc_1 + diff_punc_2) / 2).item())\n",
    "            else:\n",
    "                diff_punc = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:])\n",
    "                punc_list.append((diff_punc).item())\n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                st_values[l][norm_type][0,h,sep_index[0],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0],:] = 0        \n",
    "                st_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "            else:\n",
    "                st_values[l][norm_type][0,h,sep_index[0],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0],:] = 0        \n",
    "\n",
    "            if len(sep_index) == 2:        \n",
    "                st_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0]-1,:] = 0        \n",
    "                st_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "            else:\n",
    "                st_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "\n",
    "            st_values[l][norm_type][0,h,0,:] = 0\n",
    "            tc_values[l][norm_type][0,h,0,:] = 0\n",
    "\n",
    "            diff_other = mse_func(st_values[l][norm_type][0,h,:,:], tc_values[l][norm_type][0,h,:,:])\n",
    "            other_list.append(diff_other.item())\n",
    "\n",
    "\n",
    "    if st_model_name == \"1SB_S\":\n",
    "        print(st_model_name)\n",
    "        v_t_sep = sep_list \n",
    "        v_t_cls = cls_list \n",
    "        v_t_punc = punc_list \n",
    "        v_t_other = other_list \n",
    "    elif st_model_name == \"1SB_S_M\":\n",
    "        print(st_model_name)    \n",
    "        v_s_sep = sep_list \n",
    "        v_s_cls = cls_list \n",
    "        v_s_punc = punc_list \n",
    "        v_s_other = other_list \n",
    "    elif st_model_name == \"step_2_S_M\":\n",
    "        print(st_model_name)\n",
    "        v_sc_sep = sep_list \n",
    "        v_sc_cls = cls_list \n",
    "        v_sc_punc = punc_list \n",
    "        v_sc_other = other_list         \n",
    "    elif st_model_name == \"step_2_output\" or st_model_name == \"sarq_step1\": \n",
    "        print(st_model_name)\n",
    "        v_so_sep = sep_list \n",
    "        v_so_cls = cls_list \n",
    "        v_so_punc = punc_list \n",
    "        v_so_other = other_list   \n",
    "\n",
    "    norm_type = 0 # 1 : Attention Output 0 : Layer Context\n",
    "\n",
    "    h_num = 1\n",
    "    sep_list = []\n",
    "    cls_list = []\n",
    "    punc_list = []\n",
    "    other_list = []\n",
    "\n",
    "    tokens\n",
    "#     for l in range(layer_num):\n",
    "#         if len(sep_index) == 2:\n",
    "#             diff_sep_1 = mse_func(st_values[l][norm_type][0,sep_index[0],:], tc_values[l][norm_type][0,sep_index[0],:]) \n",
    "#             diff_sep_2 = mse_func(st_values[l][norm_type][0,sep_index[1],:], tc_values[l][norm_type][0,sep_index[1],:]) \n",
    "#             sep_list.append(((diff_sep_1 + diff_sep_2) / 2).item())\n",
    "#         else:\n",
    "#             diff_sep = mse_func(st_values[l][norm_type][0,sep_index[0],:], tc_values[l][norm_type][0,sep_index[0],:]) \n",
    "#             sep_list.append(diff_sep.item())\n",
    "\n",
    "#         diff_cls = mse_func(st_values[l][norm_type][0,0,:], tc_values[l][norm_type][0,0,:])\n",
    "#         cls_list.append((diff_cls).item())\n",
    "\n",
    "#         if len(sep_index) == 2:\n",
    "#             diff_punc_1 = mse_func(st_values[l][norm_type][0,sep_index[0]-1,:], tc_values[l][norm_type][0,sep_index[0]-1,:])\n",
    "#             diff_punc_2 = mse_func(st_values[l][norm_type][0,sep_index[1]-1,:], tc_values[l][norm_type][0,sep_index[1]-1,:]) \n",
    "#             punc_list.append(((diff_punc_1 + diff_punc_2) / 2).item())\n",
    "#         else:\n",
    "#             diff_punc = mse_func(st_values[l][norm_type][0,sep_index[0]-1,:], tc_values[l][norm_type][0,sep_index[0]-1,:])\n",
    "#             punc_list.append((diff_punc).item())\n",
    "\n",
    "#         if len(sep_index) == 2:\n",
    "#             st_values[l][norm_type][0,sep_index[0],:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[0],:] = 0        \n",
    "#             st_values[l][norm_type][0,sep_index[1],:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[1],:] = 0\n",
    "#         else:\n",
    "#             st_values[l][norm_type][0,sep_index[0],:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[0],:] = 0        \n",
    "\n",
    "#         if len(sep_index) == 2:        \n",
    "#             st_values[l][norm_type][0,sep_index[0]-1,:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[0]-1,:] = 0        \n",
    "#             st_values[l][norm_type][0,sep_index[1]-1,:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[1]-1,:] = 0\n",
    "#         else:\n",
    "#             st_values[l][norm_type][0,sep_index[0]-1,:] = 0\n",
    "#             tc_values[l][norm_type][0,sep_index[0]-1,:] = 0\n",
    "\n",
    "#         st_values[l][norm_type][0,0,:] = 0\n",
    "#         tc_values[l][norm_type][0,0,:] = 0\n",
    "\n",
    "#         diff_other = mse_func(st_values[l][norm_type][0,:,:], tc_values[l][norm_type][0,:,:])\n",
    "#         other_list.append(diff_other.item())\n",
    "\n",
    "    for l in range(layer_num):\n",
    "        for h in range(head_num): \n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                diff_sep_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "                diff_sep_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1],:], tc_values[l][norm_type][0,h,sep_index[1],:]) \n",
    "                sep_list.append(((diff_sep_1 + diff_sep_2) / 2).item())\n",
    "            else:\n",
    "                diff_sep = mse_func(st_values[l][norm_type][0,h,sep_index[0],:], tc_values[l][norm_type][0,h,sep_index[0],:]) \n",
    "                sep_list.append(diff_sep.item())\n",
    "\n",
    "            diff_cls = mse_func(st_values[l][norm_type][0,h,0,:], tc_values[l][norm_type][0,h,0,:])\n",
    "            cls_list.append((diff_cls).item())\n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                diff_punc_1 = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:])\n",
    "                diff_punc_2 = mse_func(st_values[l][norm_type][0,h,sep_index[1]-1,:], tc_values[l][norm_type][0,h,sep_index[1]-1,:]) \n",
    "                punc_list.append(((diff_punc_1 + diff_punc_2) / 2).item())\n",
    "            else:\n",
    "                diff_punc = mse_func(st_values[l][norm_type][0,h,sep_index[0]-1,:], tc_values[l][norm_type][0,h,sep_index[0]-1,:])\n",
    "                punc_list.append((diff_punc).item())\n",
    "\n",
    "            if len(sep_index) == 2:\n",
    "                st_values[l][norm_type][0,h,sep_index[0],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0],:] = 0        \n",
    "                st_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[1],:] = 0\n",
    "            else:\n",
    "                st_values[l][norm_type][0,h,sep_index[0],:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0],:] = 0        \n",
    "\n",
    "            if len(sep_index) == 2:        \n",
    "                st_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0]-1,:] = 0        \n",
    "                st_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[1]-1,:] = 0\n",
    "            else:\n",
    "                st_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "                tc_values[l][norm_type][0,h,sep_index[0]-1,:] = 0\n",
    "\n",
    "            st_values[l][norm_type][0,h,0,:] = 0\n",
    "            tc_values[l][norm_type][0,h,0,:] = 0\n",
    "\n",
    "            diff_other = mse_func(st_values[l][norm_type][0,h,:,:], tc_values[l][norm_type][0,h,:,:])\n",
    "            other_list.append(diff_other.item())\n",
    "\n",
    "    if st_model_name == \"1SB_S\":\n",
    "        print(st_model_name)\n",
    "        vw_t_sep = sep_list \n",
    "        vw_t_cls = cls_list \n",
    "        vw_t_punc = punc_list \n",
    "        vw_t_other = other_list \n",
    "    elif st_model_name == \"1SB_S_M\":\n",
    "        print(st_model_name)\n",
    "        vw_s_sep = sep_list \n",
    "        vw_s_cls = cls_list \n",
    "        vw_s_punc = punc_list \n",
    "        vw_s_other = other_list \n",
    "    elif st_model_name == \"step_2_S_M\":\n",
    "        print(st_model_name)\n",
    "        vw_sc_sep = sep_list \n",
    "        vw_sc_cls = cls_list \n",
    "        vw_sc_punc = punc_list \n",
    "        vw_sc_other = other_list   \n",
    "    elif st_model_name == \"step_2_output\" or st_model_name == \"sarq_step1\": \n",
    "        print(st_model_name)\n",
    "        vw_so_sep = sep_list \n",
    "        vw_so_cls = cls_list \n",
    "        vw_so_punc = punc_list \n",
    "        vw_so_other = other_list   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee053f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7999f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(8, 5.5), dpi=70)\n",
    "plt.rcParams['axes.linewidth'] = 2.2\n",
    "plt.rcParams['patch.linewidth'] = 2.2\n",
    "\n",
    "font_size = 23\n",
    "line_w =1.5\n",
    "\n",
    "x_axis_num = layer_num * head_num\n",
    "\n",
    "# ax1.plot(list(range(x_axis_num)),v_t_cls, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),v_sc_cls, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),v_s_cls, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# # ax1.plot(list(range(x_axis_num)),v_so_cls, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "# ax1.tick_params(axis=\"x\", labelsize=font_size)\n",
    "# ax1.tick_params(axis=\"y\", labelsize=font_size)\n",
    "# ax1.legend(fontsize=font_size, loc=1)\n",
    "# ax1.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax1.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "# # ax1.set_title(\"CLS Value Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "\n",
    "ax2.plot(list(range(x_axis_num)),v_t_sep, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),v_s_sep, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),v_sc_sep, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax2.plot(list(range(x_axis_num)),v_so_sep, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "ax2.legend(fontsize=font_size, loc=1)\n",
    "ax2.set_xlabel(\"Head\", fontsize=font_size)\n",
    "ax2.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "# ax2.set_title(\"SEP Value Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "ax2.tick_params(axis=\"x\", labelsize=font_size)\n",
    "ax2.tick_params(axis=\"y\", labelsize=font_size)\n",
    "\n",
    "# ax3.plot(list(range(x_axis_num)),v_t_punc, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),v_s_punc, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),v_sc_punc, label=\"SARQ_C\", color='b', linewidth=line_w)\n",
    "# # ax3.plot(list(range(x_axis_num)),v_so_punc, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax3.legend(fontsize=font_size, loc=1)\n",
    "# ax3.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax3.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "# # ax3.set_title(\"PUNC Value Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "# ax4.plot(list(range(x_axis_num)),v_t_other, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),v_s_other, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),v_sc_other, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# # ax4.plot(list(range(x_axis_num)),v_so_other, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax4.legend(fontsize=font_size, loc=1)\n",
    "# ax4.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax4.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f'))\n",
    "# # ax4.set_title(\"Other Value Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "plt.show()\n",
    "\n",
    "print(\"=====================================================================================================================\")\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(8, 5.5), dpi=70)\n",
    "\n",
    "x_axis_num = layer_num * head_num\n",
    "\n",
    "# ax1.plot(list(range(x_axis_num)),vw_t_cls, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_s_cls, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_sc_cls, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# # ax1.plot(list(range(x_axis_num)),vw_so_cls, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax1.legend(fontsize=font_size, loc=1)\n",
    "# ax1.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax1.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# # ax1.set_title(\"CLS LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "\n",
    "ax2.plot(list(range(x_axis_num)),vw_t_sep, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),vw_s_sep, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),vw_sc_sep, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax2.plot(list(range(x_axis_num)),vw_so_sep, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "ax2.legend(fontsize=font_size, loc=1)\n",
    "ax2.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "ax2.set_xlabel(\"Head\", fontsize=font_size)\n",
    "ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax2.set_title(\"SEP LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "ax2.tick_params(axis=\"x\", labelsize=font_size)\n",
    "ax2.tick_params(axis=\"y\", labelsize=font_size)\n",
    "\n",
    "# ax3.plot(list(range(x_axis_num)),vw_t_punc, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),vw_s_punc, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),vw_sc_punc, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# # ax3.plot(list(range(x_axis_num)),vw_so_punc, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax3.legend(fontsize=font_size, loc=1)\n",
    "# ax3.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax3.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# #x3.set_title(\"PUNC LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "# ax4.plot(list(range(x_axis_num)),vw_t_other, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),vw_s_other, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),vw_sc_other, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "#  #ax4.plot(list(range(x_axis_num)),vw_so_other, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax4.legend(fontsize=font_size, loc=1)\n",
    "# ax4.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax4.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# # ax4.set_title(\"Other LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(8, 5.5), dpi=70)\n",
    "\n",
    "x_axis_num = layer_num * head_num\n",
    "\n",
    "# ax1.plot(list(range(x_axis_num)),vw_t_cls, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_s_cls, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_sc_cls, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# # ax1.plot(list(range(x_axis_num)),vw_so_cls, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "# ax1.legend(fontsize=font_size, loc=1)\n",
    "# ax1.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax1.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# # ax1.set_title(\"CLS LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "\n",
    "ax2.plot(list(range(x_axis_num)),t_kld_list, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),s_kld_list, label=\"SARQ-1step\", color='c', linewidth=line_w)\n",
    "ax2.plot(list(range(x_axis_num)),sc_kld_list, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax2.plot(list(range(x_axis_num)),vw_so_sep, label=\"SARQ_O\", color='tab:orange', linewidth=line_w)\n",
    "\n",
    "ax2.legend(fontsize=font_size, loc=1)\n",
    "ax2.set_ylabel(\"KL_Divergence\", fontsize=font_size)\n",
    "ax2.set_xlabel(\"Head\", fontsize=font_size)\n",
    "ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax2.set_title(\"SEP LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "ax2.tick_params(axis=\"x\", labelsize=font_size)\n",
    "ax2.tick_params(axis=\"y\", labelsize=font_size)\n",
    "# fig, [ax1, ax2, ax3, ax4] = plt.subplots(4,1, figsize=(16, 24), dpi=70)\n",
    "\n",
    "# x_axis_num = layer_num * head_num\n",
    "\n",
    "# ax1.plot(list(range(x_axis_num)),vw_t_cls, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_s_cls, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax1.plot(list(range(x_axis_num)),vw_sc_cls, label=\"SARQ_C\", color='c', linewidth=line_w)\n",
    "\n",
    "# ax1.legend(fontsize=font_size, loc=1)\n",
    "# ax1.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax1.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax1.set_title(\"CLS LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "\n",
    "# ax2.plot(list(range(x_axis_num)),vw_t_sep, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax2.plot(list(range(x_axis_num)),vw_s_sep, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax2.plot(list(range(x_axis_num)),vw_sc_sep, label=\"SARQ_C\", color='c', linewidth=line_w)\n",
    "\n",
    "# ax2.legend(fontsize=font_size, loc=1)\n",
    "# ax2.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax2.set_title(\"SEP LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "# ax3.plot(list(range(x_axis_num)),vw_t_punc, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),vw_s_punc, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax3.plot(list(range(x_axis_num)),vw_sc_punc, label=\"SARQ_C\", color='c', linewidth=line_w)\n",
    "\n",
    "# ax3.legend(fontsize=font_size, loc=1)\n",
    "# ax3.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax3.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax3.set_title(\"PUNC LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "\n",
    "# ax4.plot(list(range(x_axis_num)),vw_t_other, label=\"Ternary\", color='r', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),vw_s_other, label=\"SARQ\", color='b', linewidth=line_w)\n",
    "# ax4.plot(list(range(x_axis_num)),vw_sc_other, label=\"SARQ_C\", color='c', linewidth=line_w)\n",
    "\n",
    "# ax4.legend(fontsize=font_size, loc=1)\n",
    "# ax4.set_ylabel(\"MSE\", fontsize=font_size)\n",
    "# ax4.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "# ax4.set_title(\"Other LN Output Vector \", fontsize=font_size, fontweight=\"light\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2, ax3, ax4] = plt.subplots(4, 1, figsize=(12,16), dpi=80)\n",
    "\n",
    "x_axis_num = layer_num * head_num\n",
    "\n",
    "ax1.set_title(\"Period Token\")\n",
    "ax1.plot(list(range(x_axis_num)),t_punc_list, label=\"Ternary_punc\", color='r', linewidth=1)\n",
    "ax1.plot(list(range(x_axis_num)),SB_punc_list, label=\"SARQ_punc\", color='c', linewidth=1)\n",
    "ax1.legend(fontsize=\"12\")\n",
    "\n",
    "ax2.set_title(\"CLS Token\")\n",
    "ax2.plot(list(range(x_axis_num)),t_cls_list, label=\"Ternary_cls\", color='r', linewidth=1)\n",
    "ax2.plot(list(range(x_axis_num)),SB_cls_list, label=\"SARQ_cls\", color='c', linewidth=1)\n",
    "ax2.legend(fontsize=\"12\")\n",
    "\n",
    "ax3.set_title(\"SEP Token\")\n",
    "ax3.plot(list(range(x_axis_num)),t_sep_list, label=\"Ternary_sep\", color='r', linewidth=1)\n",
    "ax3.plot(list(range(x_axis_num)),SB_sep_list, label=\"SARQ_sep\", color='c', linewidth=1)\n",
    "ax3.legend(fontsize=\"12\")\n",
    "\n",
    "ax4.set_title(\"Other Token\")\n",
    "ax4.plot(list(range(x_axis_num)),t_other_list, label=\"Ternary_other\", color='r', linewidth=1)\n",
    "ax4.plot(list(range(x_axis_num)),SB_other_list, label=\"SARQ_other\", color='c', linewidth=1)\n",
    "ax4.legend(fontsize=\"12\")\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16,8))\n",
    "# ax.plot(list(range(144)),t_kld_list, label=\"Ternary_other\", color='r', linewidth=1.5)\n",
    "# ax.plot(list(range(144)),s_kld_list, label=\"SARQ_other\", color='c', linewidth=1.5)\n",
    "# ax.legend(fontsize=\"20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288feea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids_sliced.to(device))\n",
    "student_logits, student_atts, student_reps, student_probs, student_values = student_model(input_ids_sliced.to(device), teacher_probs=teacher_probs)\n",
    "\n",
    "st_values = student_values\n",
    "tc_values = teacher_values\n",
    "\n",
    "loss_cos(st_values[l][norm_type][0,2,sep_index[0],:], tc_values[l][norm_type][0,2,sep_index[0],:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = st_values[l][norm_type][0,2,sep_index[0],:]\n",
    "b = tc_values[l][norm_type][0,2,sep_index[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aaef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_func(st_values[0][0], tc_values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b) / (norm_func(a) * norm_func(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c481d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
