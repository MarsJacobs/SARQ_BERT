{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f780d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pprint\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformer import BertForSequenceClassification,WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformer.modeling_quant import BertForSequenceClassification as QuantBertForSequenceClassification\n",
    "from transformer import BertTokenizer\n",
    "from transformer import BertAdam\n",
    "from transformer import BertConfig\n",
    "from transformer import QuantizeLinear, QuantizeAct, BertSelfAttention, FP_BertSelfAttention, ClipLinear\n",
    "from utils_glue import *\n",
    "from bertviz import model_view\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0 \n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def get_tensor_data(output_mode, features):\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "\n",
    "    all_seq_lengths = torch.tensor([f.seq_length for f in features], dtype=torch.long)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    tensor_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_label_ids, all_seq_lengths)\n",
    "    return tensor_data, all_label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a445140",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"squadv2.0\" # squadv1.1 squadv2.0\n",
    "# task_name = \"squadv1.1\" \n",
    "bert_size = \"large\"\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8561e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else: \n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "    \n",
    "def read_squad_examples(input_file, is_training, version_2_with_negative):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                is_impossible = False\n",
    "                if is_training:\n",
    "                    if version_2_with_negative:\n",
    "                        if 'is_impossible' not in qa:\n",
    "                            qa['is_impossible'] = True\n",
    "                        is_impossible = qa[\"is_impossible\"]\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                        answer_length = len(orig_answer_text)\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                        # Only add answers where the text can be exactly recovered from the\n",
    "                        # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                        # stuff so we will just skip the example.\n",
    "                        #\n",
    "                        # Note that this means for training mode, every example is NOT\n",
    "                        # guaranteed to be preserved.\n",
    "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                        cleaned_answer_text = \" \".join(\n",
    "                            whitespace_tokenize(orig_answer_text))\n",
    "                        if actual_text.find(cleaned_answer_text) == -1:\n",
    "                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                           actual_text, cleaned_answer_text)\n",
    "                            continue\n",
    "                    else:\n",
    "                        start_position = -1\n",
    "                        end_position = -1\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=is_impossible)\n",
    "                examples.append(example)\n",
    "\n",
    "    logger.info('load {} examples!'.format(len(examples)))\n",
    "    return input_data,examples\n",
    "\n",
    "class SquadExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for the Squad dataset.\n",
    "    For examples without an answer, the start and end position are -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \", question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.end_position:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        if self.is_impossible:\n",
    "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "        return s\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training and example.is_impossible:\n",
    "            tok_start_position = -1\n",
    "            tok_end_position = -1\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training and not example.is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (tok_start_position >= doc_start and\n",
    "                        tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "                if out_of_span:\n",
    "                    start_position = 0\n",
    "                    end_position = 0\n",
    "                else:\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "            if is_training and example.is_impossible:\n",
    "                start_position = 0\n",
    "                end_position = 0\n",
    "            if example_index < 1:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training and example.is_impossible:\n",
    "                    logger.info(\"impossible example\")\n",
    "                if is_training and not example.is_impossible:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=example.is_impossible))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bad43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer Setting\n",
    "data_dir = os.path.join(\"data\",task_name)\n",
    "teacher_model_dir = os.path.join(model_dir,task_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(teacher_model_dir, do_lower_case=True)\n",
    "\n",
    "input_file = 'dev-v2.0.json' if task_name == \"squadv2.0\" else 'dev-v1.1.json'\n",
    "version_2_with_negative = True if task_name == \"squadv2.0\" else False\n",
    "dev_file = os.path.join(data_dir,input_file)\n",
    "\n",
    "data_dir = os.path.join(\"data\",task_name)\n",
    "# processed_data_dir = os.path.join(data_dir,'preprocessed')\n",
    "\n",
    "dev_file = os.path.join(data_dir,input_file)\n",
    "dev_dataset, eval_examples = read_squad_examples(\n",
    "                    input_file=dev_file, is_training=False,\n",
    "                    version_2_with_negative=version_2_with_negative)\n",
    "\n",
    "eval_features = convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=False)\n",
    "\n",
    "batch_size = 1\n",
    "logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be79fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Sentence \n",
    "i = 0 \n",
    "# num = 3\n",
    "num = 1234\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    # model.train()\n",
    "            \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "    i = i + 1\n",
    "    if i == num:\n",
    "        break\n",
    "        \n",
    "seq_length = input_mask[0].sum()\n",
    "input_ids_sliced = input_ids[:,:seq_length]\n",
    "input_id = []\n",
    "for i in input_ids_sliced[0]:\n",
    "    input_id.append(i.item())\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "mse_func = MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "student_model_dir = os.path.join(model_dir,task_name)\n",
    "\n",
    "# Teacher Model Build\n",
    "teacher_model_dir = os.path.join(model_dir,task_name)\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_dir)\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83716096",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_zip = teacher_model(input_ids_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_dict = dict()\n",
    "for l in range(layer_num):        \n",
    "    mag_dict[f\"attn_min_{l}\"] = []; mag_dict[f\"attn_max_{l}\"] = []; \n",
    "    mag_dict[f\"ffn_min_{l}\"] = []; mag_dict[f\"ffn_max_{l}\"] = []; \n",
    "    mag_dict[f\"sa_min_{l}\"] = []; mag_dict[f\"sa_max_{l}\"] = []; \n",
    "    mag_dict[f\"sa_ffn_min_{l}\"] = []; mag_dict[f\"sa_ffn_max_{l}\"] = []; \n",
    "\n",
    "for l in range(layer_num):\n",
    "        tc_attn_context, tc_attn_output, tc_value_vector, tc_sa_output = teacher_zip[l]\n",
    "        # st_attn_context, st_attn_output, st_value_vector, st_sa_output = student_zip[l]        \n",
    "        # st_ffn_output = student_zip[2][1+1]\n",
    "        tc_ffn_output = teacher_reps[l]\n",
    "        \n",
    "        tc_output = tc_attn_output\n",
    "        # st_output = st_attn_output\n",
    "        \n",
    "        for token in range(len(tokens)):\n",
    "          \n",
    "            mag_dict[f\"attn_min_{l}\"].append(tc_attn_output[0,token,:].min().item())\n",
    "            mag_dict[f\"attn_max_{l}\"].append(tc_attn_output[0,token,:].max().item())\n",
    "            \n",
    "            mag_dict[f\"ffn_min_{l}\"].append(tc_ffn_output[0,token,:].min().item())\n",
    "            mag_dict[f\"ffn_max_{l}\"].append(tc_ffn_output[0,token,:].max().item())\n",
    "            \n",
    "            mag_dict[f\"sa_min_{l}\"].append(tc_sa_output[0,token,:].min().item())\n",
    "            mag_dict[f\"sa_max_{l}\"].append(tc_sa_output[0,token,:].max().item())\n",
    "            \n",
    "            mag_dict[f\"sa_ffn_min_{l}\"].append((tc_ffn_output+tc_sa_output)[0,token,:].min().item())\n",
    "            mag_dict[f\"sa_ffn_max_{l}\"].append((tc_ffn_output+tc_sa_output)[0,token,:].max().item())\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "f2_dict = copy.deepcopy(mag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3248c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(tokens):\n",
    "    tokens[i] = token + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657e321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_len=214\n",
    "for l in range(layer_num):\n",
    "    fig, [ax1,ax2,ax3,ax4] = plt.subplots(4, 1, figsize=(15, 10), dpi=70)\n",
    "    ax1.plot(list(range(token_len)), mag_dict[f\"ffn_min_{l}\"], label=\"v2_ffn_min\", color=\"tab:red\", linewidth=2.5)\n",
    "    ax1.plot(list(range(token_len)), mag_dict[f\"ffn_max_{l}\"], label=\"v2_ffn_max\", color=\"tab:red\", linewidth=2.5)\n",
    "    \n",
    "    ax2.plot(list(range(token_len)), mag_dict[f\"sa_min_{l}\"], label=\"v2_sa_min\", color=\"orangered\", linewidth=2.5)\n",
    "    ax2.plot(list(range(token_len)), mag_dict[f\"sa_max_{l}\"], label=\"v2_sa_max\", color=\"orangered\", linewidth=2.5)\n",
    "    \n",
    "    ax3.plot(list(range(token_len)), mag_dict[f\"sa_ffn_min_{l}\"], label=\"v2_sa+ffn_min\", color=\"c\", linewidth=2.5)\n",
    "    ax3.plot(list(range(token_len)), mag_dict[f\"sa_ffn_max_{l}\"], label=\"v2_sa+ffn_max\", color=\"c\", linewidth=2.5)\n",
    "    \n",
    "    ax4.plot(list(range(token_len)), mag_dict[f\"attn_min_{l}\"], label=\"v2_attn_min\", color=\"dodgerblue\", linewidth=2.5)\n",
    "    ax4.plot(list(range(token_len)), mag_dict[f\"attn_max_{l}\"], label=\"v2_attn_max\", color=\"dodgerblue\", linewidth=2.5)\n",
    "    \n",
    "    \n",
    "    ax1.set_title(f\"Layer {l}\", fontsize=15)\n",
    "    ax1.legend(loc=2, fontsize=15)\n",
    "    ax2.legend(loc=2, fontsize=15)\n",
    "    ax3.legend(loc=2, fontsize=15)\n",
    "    ax4.legend(loc=2, fontsize=15)\n",
    "    \n",
    "    plt.xticks(rotation=60, fontsize=10)\n",
    "    \n",
    "#     ax2.plot(list(range(152)), mag_dict[f\"tc_min_{l}\"], label=\"v1_FP_min\", color=\"tab:blue\", linewidth=2.5)\n",
    "#     ax2.plot(list(range(152)), mag_dict[f\"tc_max_{l}\"], label=\"v1_FP_max\", color=\"tab:blue\", linewidth=2.5)\n",
    "#     ax2.legend(loc=2, fontsize=15)\n",
    "#     ax2.set_xlabel(\"Token Number\", fontsize=3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
