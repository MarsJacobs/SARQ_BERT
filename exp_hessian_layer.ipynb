{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b447e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pprint\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # Set GPU Index to use\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformer import BertForSequenceClassification,WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformer.modeling_quant import BertForSequenceClassification as QuantBertForSequenceClassification\n",
    "from transformer import BertTokenizer\n",
    "from transformer import BertAdam\n",
    "from transformer import BertConfig\n",
    "from transformer import QuantizeLinear, QuantizeAct, BertSelfAttention, FP_BertSelfAttention, ClipLinear, BertAttention, FP_BertAttention\n",
    "from utils_glue import *\n",
    "from bertviz import model_view\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0 \n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def do_eval(model, task_name, eval_dataloader,\n",
    "            device, output_mode, eval_labels, num_labels, teacher_model=None):\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    preds = []\n",
    "\n",
    "    for batch_ in tqdm(eval_dataloader, desc=\"Inference\"):\n",
    "        batch_ = tuple(t.to(device) for t in batch_)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, segment_ids, label_ids, seq_lengths = batch_\n",
    "\n",
    "            # teacher attnmap test\n",
    "            if teacher_model is not None:\n",
    "                \n",
    "                # logits, _, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids, segment_ids, input_mask)\n",
    "                \n",
    "                # # logits, _, _, _, _ = model(input_ids, segment_ids, input_mask, teacher_probs=teacher_probs)\n",
    "                # logits, _, _, _, _ = model(input_ids, segment_ids, input_mask, teacher_probs=(teacher_probs, teacher_values, teacher_reps))\n",
    "                teacher_logits, teacher_atts, teacher_reps, teacher_probs, teacher_values = teacher_model(input_ids, segment_ids, input_mask)\n",
    "                logits, student_atts, student_reps, student_probs, student_values  = model(input_ids, segment_ids, input_mask, teacher_outputs=(teacher_probs, teacher_values, teacher_reps, teacher_logits, teacher_atts))\n",
    "            else:\n",
    "                logits, _, _, _, _ = model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "        # create eval loss and other metric required by the task\n",
    "        if output_mode == \"classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        elif output_mode == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = preds[0]\n",
    "    if output_mode == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif output_mode == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result = compute_metrics(task_name, preds, eval_labels.numpy())\n",
    "    result['eval_loss'] = eval_loss\n",
    "    return result\n",
    "\n",
    "processors = {\n",
    "    \"cola\": ColaProcessor,\n",
    "    \"mnli\": MnliProcessor,\n",
    "    \"mnli-mm\": MnliMismatchedProcessor,\n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    \"sst-2\": Sst2Processor,\n",
    "    \"sts-b\": StsbProcessor,\n",
    "    \"qqp\": QqpProcessor,\n",
    "    \"qnli\": QnliProcessor,\n",
    "    \"rte\": RteProcessor   \n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "        \"cola\": \"classification\",\n",
    "        \"mnli\": \"classification\",\n",
    "        \"mrpc\": \"classification\",\n",
    "        \"sst-2\": \"classification\",\n",
    "        \"sts-b\": \"regression\",\n",
    "        \"qqp\": \"classification\",\n",
    "        \"qnli\": \"classification\",\n",
    "        \"rte\": \"classification\"\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "        \"cola\": {\"max_seq_length\": 64,\"batch_size\":16,\"eval_step\": 400}, # No Aug : 50 Aug : 400\n",
    "        \"mnli\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":8000},\n",
    "        \"mrpc\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":20},\n",
    "        \"sst-2\": {\"max_seq_length\": 64,\"batch_size\":32,\"eval_step\":100},\n",
    "        \"sts-b\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":100},\n",
    "        \"qqp\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":1000},\n",
    "        \"qnli\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":1000},\n",
    "        \"rte\": {\"max_seq_length\": 128,\"batch_size\":32,\"eval_step\":100}\n",
    "    }\n",
    "\n",
    "def get_tensor_data(output_mode, features):\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "\n",
    "    all_seq_lengths = torch.tensor([f.seq_length for f in features], dtype=torch.long)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    tensor_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_label_ids, all_seq_lengths)\n",
    "    return tensor_data, all_label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72fbac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    \"\"\"\n",
    "    the inner product of two lists of variables xs,ys\n",
    "    :param xs:\n",
    "    :param ys:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "\n",
    "def de_variable(v):\n",
    "    '''\n",
    "    normalize the vector and detach it from variable\n",
    "    '''\n",
    "\n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item() + 1e-6\n",
    "    v = [vi / s for vi in v]\n",
    "    return v\n",
    "\n",
    "def group_add(params, update, alpha=1):\n",
    "    \"\"\"\n",
    "    params = params + update*alpha\n",
    "    :param params: list of variable\n",
    "    :param update: list of data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i, p in enumerate(params):\n",
    "        params[i].data.add_(update[i] * alpha)\n",
    "    return params\n",
    "\n",
    "\n",
    "def normalization(v):\n",
    "    \"\"\"\n",
    "    normalization of a list of vectors\n",
    "    return: normalized vectors v\n",
    "    \"\"\"\n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "    # v = [vi / s for vi in v]\n",
    "    return v\n",
    "\n",
    "\n",
    "def orthonormal(w, v_list):\n",
    "    for v in v_list:\n",
    "        w = group_add(w, v, alpha=-group_product(w, v))\n",
    "    return normalization(w)\n",
    "\n",
    "\n",
    "def total_number_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6460b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"rte\"\n",
    "bert_size = \"large\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    layer_num = 24\n",
    "    head_num = 16\n",
    "else: \n",
    "    layer_num = 12\n",
    "    head_num = 12\n",
    "    \n",
    "teacher_model = None\n",
    "# torch.cuda.empty_cache()\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084fe35",
   "metadata": {},
   "source": [
    "# DEVICE / DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5214fdf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:04:23 PM Writing example 0 of 2490\n",
      "07/14 06:04:23 PM *** Example ***\n",
      "07/14 06:04:23 PM guid: train-0\n",
      "07/14 06:04:23 PM tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]\n",
      "07/14 06:04:23 PM input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:23 PM label: not_entailment\n",
      "07/14 06:04:23 PM label_id: 1\n",
      "07/14 06:04:24 PM Writing example 0 of 277\n",
      "07/14 06:04:24 PM *** Example ***\n",
      "07/14 06:04:24 PM guid: dev-0\n",
      "07/14 06:04:24 PM tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]\n",
      "07/14 06:04:24 PM input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "07/14 06:04:24 PM label: not_entailment\n",
      "07/14 06:04:24 PM label_id: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "output_dir = \"output\"\n",
    "\n",
    "if bert_size == \"large\":\n",
    "    model_dir = os.path.join(model_dir, \"BERT_large\")\n",
    "    output_dir = os.path.join(output_dir, \"BERT_large\")\n",
    "\n",
    "teacher_model_dir = os.path.join(model_dir,task_name)\n",
    "\n",
    "# Processor & Task Info\n",
    "processor = processors[task_name]()\n",
    "output_mode = output_modes[task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "if task_name in default_params:\n",
    "    batch_size = default_params[task_name][\"batch_size\"]\n",
    "    max_seq_length = default_params[task_name][\"max_seq_length\"]\n",
    "    eval_step = default_params[task_name][\"eval_step\"]\n",
    "    \n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(teacher_model_dir, do_lower_case=True)\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "data_dir = os.path.join(\"data\",task_name)\n",
    "processed_data_dir = os.path.join(data_dir,'preprocessed')\n",
    "\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "train_features = convert_examples_to_features(train_examples, label_list,\n",
    "                                max_seq_length, tokenizer, output_mode)\n",
    "\n",
    "len_train_data = int(len(train_features) * 1)\n",
    "train_features = train_features[:len_train_data]\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)\n",
    "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, output_mode)\n",
    "# dev_file = train_file = os.path.join(processed_data_dir,'dev.pkl') \n",
    "# eval_features = pickle.load(open(dev_file,'rb'))\n",
    "\n",
    "train_data, train_labels = get_tensor_data(output_mode, train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "eval_data, eval_labels = get_tensor_data(\"classification\", eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "eval_data, eval_labels = get_tensor_data(output_mode, eval_features)\n",
    "\n",
    "eval_examples = processor.get_dev_examples(data_dir)\n",
    "\n",
    "# Sampling Sentence \n",
    "i = 0 \n",
    "# num = 3\n",
    "num = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054111a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:04:25 PM loading configuration file output/BERT_large/rte/exploration/1SB_M/config.json\n",
      "07/14 06:04:30 PM Loading model output/BERT_large/rte/exploration/1SB_M/pytorch_model.bin\n",
      "07/14 06:04:32 PM loading model...\n",
      "07/14 06:04:32 PM done!\n",
      "07/14 06:04:32 PM Weights from pretrained model not used in BertForSequenceClassification: ['bert.embeddings.word_embeddings.qweight', 'bert.encoder.layer.0.attention.self.query.qweight', 'bert.encoder.layer.0.attention.self.key.qweight', 'bert.encoder.layer.0.attention.self.value.qweight', 'bert.encoder.layer.0.attention.output.dense.qweight', 'bert.encoder.layer.0.intermediate.dense.qweight', 'bert.encoder.layer.0.output.dense.qweight', 'bert.encoder.layer.1.attention.self.query.qweight', 'bert.encoder.layer.1.attention.self.key.qweight', 'bert.encoder.layer.1.attention.self.value.qweight', 'bert.encoder.layer.1.attention.output.dense.qweight', 'bert.encoder.layer.1.intermediate.dense.qweight', 'bert.encoder.layer.1.output.dense.qweight', 'bert.encoder.layer.2.attention.self.query.qweight', 'bert.encoder.layer.2.attention.self.key.qweight', 'bert.encoder.layer.2.attention.self.value.qweight', 'bert.encoder.layer.2.attention.output.dense.qweight', 'bert.encoder.layer.2.intermediate.dense.qweight', 'bert.encoder.layer.2.output.dense.qweight', 'bert.encoder.layer.3.attention.self.query.qweight', 'bert.encoder.layer.3.attention.self.key.qweight', 'bert.encoder.layer.3.attention.self.value.qweight', 'bert.encoder.layer.3.attention.output.dense.qweight', 'bert.encoder.layer.3.intermediate.dense.qweight', 'bert.encoder.layer.3.output.dense.qweight', 'bert.encoder.layer.4.attention.self.query.qweight', 'bert.encoder.layer.4.attention.self.key.qweight', 'bert.encoder.layer.4.attention.self.value.qweight', 'bert.encoder.layer.4.attention.output.dense.qweight', 'bert.encoder.layer.4.intermediate.dense.qweight', 'bert.encoder.layer.4.output.dense.qweight', 'bert.encoder.layer.5.attention.self.query.qweight', 'bert.encoder.layer.5.attention.self.key.qweight', 'bert.encoder.layer.5.attention.self.value.qweight', 'bert.encoder.layer.5.attention.output.dense.qweight', 'bert.encoder.layer.5.intermediate.dense.qweight', 'bert.encoder.layer.5.output.dense.qweight', 'bert.encoder.layer.6.attention.self.query.qweight', 'bert.encoder.layer.6.attention.self.key.qweight', 'bert.encoder.layer.6.attention.self.value.qweight', 'bert.encoder.layer.6.attention.output.dense.qweight', 'bert.encoder.layer.6.intermediate.dense.qweight', 'bert.encoder.layer.6.output.dense.qweight', 'bert.encoder.layer.7.attention.self.query.qweight', 'bert.encoder.layer.7.attention.self.key.qweight', 'bert.encoder.layer.7.attention.self.value.qweight', 'bert.encoder.layer.7.attention.output.dense.qweight', 'bert.encoder.layer.7.intermediate.dense.qweight', 'bert.encoder.layer.7.output.dense.qweight', 'bert.encoder.layer.8.attention.self.query.qweight', 'bert.encoder.layer.8.attention.self.key.qweight', 'bert.encoder.layer.8.attention.self.value.qweight', 'bert.encoder.layer.8.attention.output.dense.qweight', 'bert.encoder.layer.8.intermediate.dense.qweight', 'bert.encoder.layer.8.output.dense.qweight', 'bert.encoder.layer.9.attention.self.query.qweight', 'bert.encoder.layer.9.attention.self.key.qweight', 'bert.encoder.layer.9.attention.self.value.qweight', 'bert.encoder.layer.9.attention.output.dense.qweight', 'bert.encoder.layer.9.intermediate.dense.qweight', 'bert.encoder.layer.9.output.dense.qweight', 'bert.encoder.layer.10.attention.self.query.qweight', 'bert.encoder.layer.10.attention.self.key.qweight', 'bert.encoder.layer.10.attention.self.value.qweight', 'bert.encoder.layer.10.attention.output.dense.qweight', 'bert.encoder.layer.10.intermediate.dense.qweight', 'bert.encoder.layer.10.output.dense.qweight', 'bert.encoder.layer.11.attention.self.query.qweight', 'bert.encoder.layer.11.attention.self.key.qweight', 'bert.encoder.layer.11.attention.self.value.qweight', 'bert.encoder.layer.11.attention.output.dense.qweight', 'bert.encoder.layer.11.intermediate.dense.qweight', 'bert.encoder.layer.11.output.dense.qweight', 'bert.encoder.layer.12.attention.self.query.qweight', 'bert.encoder.layer.12.attention.self.key.qweight', 'bert.encoder.layer.12.attention.self.value.qweight', 'bert.encoder.layer.12.attention.output.dense.qweight', 'bert.encoder.layer.12.intermediate.dense.qweight', 'bert.encoder.layer.12.output.dense.qweight', 'bert.encoder.layer.13.attention.self.query.qweight', 'bert.encoder.layer.13.attention.self.key.qweight', 'bert.encoder.layer.13.attention.self.value.qweight', 'bert.encoder.layer.13.attention.output.dense.qweight', 'bert.encoder.layer.13.intermediate.dense.qweight', 'bert.encoder.layer.13.output.dense.qweight', 'bert.encoder.layer.14.attention.self.query.qweight', 'bert.encoder.layer.14.attention.self.key.qweight', 'bert.encoder.layer.14.attention.self.value.qweight', 'bert.encoder.layer.14.attention.output.dense.qweight', 'bert.encoder.layer.14.intermediate.dense.qweight', 'bert.encoder.layer.14.output.dense.qweight', 'bert.encoder.layer.15.attention.self.query.qweight', 'bert.encoder.layer.15.attention.self.key.qweight', 'bert.encoder.layer.15.attention.self.value.qweight', 'bert.encoder.layer.15.attention.output.dense.qweight', 'bert.encoder.layer.15.intermediate.dense.qweight', 'bert.encoder.layer.15.output.dense.qweight', 'bert.encoder.layer.16.attention.self.query.qweight', 'bert.encoder.layer.16.attention.self.key.qweight', 'bert.encoder.layer.16.attention.self.value.qweight', 'bert.encoder.layer.16.attention.output.dense.qweight', 'bert.encoder.layer.16.intermediate.dense.qweight', 'bert.encoder.layer.16.output.dense.qweight', 'bert.encoder.layer.17.attention.self.query.qweight', 'bert.encoder.layer.17.attention.self.key.qweight', 'bert.encoder.layer.17.attention.self.value.qweight', 'bert.encoder.layer.17.attention.output.dense.qweight', 'bert.encoder.layer.17.intermediate.dense.qweight', 'bert.encoder.layer.17.output.dense.qweight', 'bert.encoder.layer.18.attention.self.query.qweight', 'bert.encoder.layer.18.attention.self.key.qweight', 'bert.encoder.layer.18.attention.self.value.qweight', 'bert.encoder.layer.18.attention.output.dense.qweight', 'bert.encoder.layer.18.intermediate.dense.qweight', 'bert.encoder.layer.18.output.dense.qweight', 'bert.encoder.layer.19.attention.self.query.qweight', 'bert.encoder.layer.19.attention.self.key.qweight', 'bert.encoder.layer.19.attention.self.value.qweight', 'bert.encoder.layer.19.attention.output.dense.qweight', 'bert.encoder.layer.19.intermediate.dense.qweight', 'bert.encoder.layer.19.output.dense.qweight', 'bert.encoder.layer.20.attention.self.query.qweight', 'bert.encoder.layer.20.attention.self.key.qweight', 'bert.encoder.layer.20.attention.self.value.qweight', 'bert.encoder.layer.20.attention.output.dense.qweight', 'bert.encoder.layer.20.intermediate.dense.qweight', 'bert.encoder.layer.20.output.dense.qweight', 'bert.encoder.layer.21.attention.self.query.qweight', 'bert.encoder.layer.21.attention.self.key.qweight', 'bert.encoder.layer.21.attention.self.value.qweight', 'bert.encoder.layer.21.attention.output.dense.qweight', 'bert.encoder.layer.21.intermediate.dense.qweight', 'bert.encoder.layer.21.output.dense.qweight', 'bert.encoder.layer.22.attention.self.query.qweight', 'bert.encoder.layer.22.attention.self.key.qweight', 'bert.encoder.layer.22.attention.self.value.qweight', 'bert.encoder.layer.22.attention.output.dense.qweight', 'bert.encoder.layer.22.intermediate.dense.qweight', 'bert.encoder.layer.22.output.dense.qweight', 'bert.encoder.layer.23.attention.self.query.qweight', 'bert.encoder.layer.23.attention.self.key.qweight', 'bert.encoder.layer.23.attention.self.value.qweight', 'bert.encoder.layer.23.attention.output.dense.qweight', 'bert.encoder.layer.23.intermediate.dense.qweight', 'bert.encoder.layer.23.output.dense.qweight', 'bert.pooler.dense.qweight']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_dir, num_labels=num_labels)\n",
    "# teacher_model.to(device)\n",
    "# teacher_model.eval()\n",
    "\n",
    "st_model_name = \"1SB_M\"\n",
    "student_model_dir = os.path.join(output_dir, task_name, \"exploration\", st_model_name)   \n",
    "student_config = BertConfig.from_pretrained(student_model_dir)   \n",
    "student_model = QuantBertForSequenceClassification.from_pretrained(student_model_dir, config = student_config, num_labels=num_labels)\n",
    "student_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cabce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage_index: 3.890625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_percentage = 0.05\n",
    "\n",
    "percentage_index = len(train_dataloader.dataset) * data_percentage / batch_size\n",
    "print(f'percentage_index: {percentage_index}')\n",
    "\n",
    "student_model.eval()\n",
    "\n",
    "csv_path = os.path.join(\"layer_hessian_results\", f\"{task_name}-{data_percentage}-eigens.csv\")\n",
    "csv_file = open(csv_path, 'w', newline='')\n",
    "writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "writer.writerow(['block', 'iters', 'max_eigenvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809d4716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:04:35 PM block_id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms/anaconda3/envs/deep/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/autograd/engine.cpp:985.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:04:42 PM block_0-lambda: 2.3985539663986987e-06\n",
      "07/14 06:04:47 PM block_0-lambda: 0.002090829203885243\n",
      "07/14 06:04:53 PM block_0-lambda: 0.07367013065690496\n",
      "07/14 06:04:59 PM block_0-lambda: 0.5844492471840487\n",
      "07/14 06:05:04 PM block_0-lambda: 0.5747494372019327\n",
      "07/14 06:05:10 PM block_0-lambda: 0.787686819053558\n",
      "07/14 06:05:16 PM block_0-lambda: 0.7871970670769014\n",
      "07/14 06:05:16 PM block_id: 1\n",
      "07/14 06:05:22 PM block_1-lambda: -7.455763767050752e-07\n",
      "07/14 06:05:27 PM block_1-lambda: 0.025249699512159968\n",
      "07/14 06:05:33 PM block_1-lambda: 0.7545009750917734\n",
      "07/14 06:05:38 PM block_1-lambda: 0.8833756657489332\n",
      "07/14 06:05:44 PM block_1-lambda: 0.7087361898766943\n",
      "07/14 06:05:50 PM block_1-lambda: 0.783837682272057\n",
      "07/14 06:05:55 PM block_1-lambda: 1.0360744736759537\n",
      "07/14 06:06:01 PM block_1-lambda: 0.8320200357092432\n",
      "07/14 06:06:06 PM block_1-lambda: 0.4347141679511013\n",
      "07/14 06:06:12 PM block_1-lambda: 1.1705257308530999\n",
      "07/14 06:06:18 PM block_1-lambda: 1.154293810986132\n",
      "07/14 06:06:23 PM block_1-lambda: 1.1004650927930473\n",
      "07/14 06:06:29 PM block_1-lambda: 1.1493291280355797\n",
      "07/14 06:06:34 PM block_1-lambda: 1.3457050859688755\n",
      "07/14 06:06:40 PM block_1-lambda: 0.8198345751168737\n",
      "07/14 06:06:45 PM block_1-lambda: 0.8943159111053589\n",
      "07/14 06:06:51 PM block_1-lambda: 1.3393405163623242\n",
      "07/14 06:06:56 PM block_1-lambda: 1.3735222873917545\n",
      "07/14 06:07:02 PM block_1-lambda: 1.0143927761828564\n",
      "07/14 06:07:08 PM block_1-lambda: 0.8200752840463417\n",
      "07/14 06:07:13 PM block_1-lambda: 0.7728544181609249\n",
      "07/14 06:07:19 PM block_1-lambda: 0.7801331056648468\n",
      "07/14 06:07:19 PM block_id: 2\n",
      "07/14 06:07:24 PM block_2-lambda: -1.5942435468505904e-06\n",
      "07/14 06:07:30 PM block_2-lambda: 0.001099723888688298\n",
      "07/14 06:07:35 PM block_2-lambda: 0.3184880942225935\n",
      "07/14 06:07:41 PM block_2-lambda: 0.5508402629070971\n",
      "07/14 06:07:46 PM block_2-lambda: 0.8004812830423256\n",
      "07/14 06:07:51 PM block_2-lambda: 1.1906854713776982\n",
      "07/14 06:07:57 PM block_2-lambda: 0.7705231938496172\n",
      "07/14 06:08:02 PM block_2-lambda: 0.6863859858378828\n",
      "07/14 06:08:08 PM block_2-lambda: 0.9862599813315763\n",
      "07/14 06:08:13 PM block_2-lambda: 0.7822732580713478\n",
      "07/14 06:08:18 PM block_2-lambda: 1.0262043045227787\n",
      "07/14 06:08:24 PM block_2-lambda: 0.7871017149653301\n",
      "07/14 06:08:29 PM block_2-lambda: 0.6213599515248494\n",
      "07/14 06:08:35 PM block_2-lambda: 1.0039408120764308\n",
      "07/14 06:08:40 PM block_2-lambda: 0.5551845443296624\n",
      "07/14 06:08:46 PM block_2-lambda: 0.4856654707207737\n",
      "07/14 06:08:51 PM block_2-lambda: 0.8748283539431162\n",
      "07/14 06:08:56 PM block_2-lambda: 0.887811928867815\n",
      "07/14 06:09:02 PM block_2-lambda: 1.1564816394484188\n",
      "07/14 06:09:07 PM block_2-lambda: 0.8081600694771273\n",
      "07/14 06:09:13 PM block_2-lambda: 0.7555347120905497\n",
      "07/14 06:09:18 PM block_2-lambda: 0.7056613906799071\n",
      "07/14 06:09:24 PM block_2-lambda: 0.8151002447289156\n",
      "07/14 06:09:29 PM block_2-lambda: 0.8322445053652109\n",
      "07/14 06:09:34 PM block_2-lambda: 1.0346629437672565\n",
      "07/14 06:09:40 PM block_2-lambda: 1.1849125015687751\n",
      "07/14 06:09:45 PM block_2-lambda: 0.913874614669616\n",
      "07/14 06:09:51 PM block_2-lambda: 0.8488365694222201\n",
      "07/14 06:09:56 PM block_2-lambda: 0.7750266201524849\n",
      "07/14 06:10:01 PM block_2-lambda: 0.8518665114559801\n",
      "07/14 06:10:07 PM block_2-lambda: 0.6990570313480484\n",
      "07/14 06:10:12 PM block_2-lambda: 1.1041191131714359\n",
      "07/14 06:10:18 PM block_2-lambda: 1.0420037081921436\n",
      "07/14 06:10:23 PM block_2-lambda: 0.9925983229794176\n",
      "07/14 06:10:29 PM block_2-lambda: 1.2685260083301957\n",
      "07/14 06:10:34 PM block_2-lambda: 1.3447329356488453\n",
      "07/14 06:10:40 PM block_2-lambda: 0.7156905208725527\n",
      "07/14 06:10:45 PM block_2-lambda: 0.5877083161748556\n",
      "07/14 06:10:50 PM block_2-lambda: 0.8733695769405748\n",
      "07/14 06:10:56 PM block_2-lambda: 0.7616320384075363\n",
      "07/14 06:11:01 PM block_2-lambda: 0.9647574060892006\n",
      "07/14 06:11:07 PM block_2-lambda: 0.8152945644884224\n",
      "07/14 06:11:12 PM block_2-lambda: 0.8725654571410643\n",
      "07/14 06:11:18 PM block_2-lambda: 1.044554806138617\n",
      "07/14 06:11:23 PM block_2-lambda: 1.117349647613893\n",
      "07/14 06:11:29 PM block_2-lambda: 1.5278238196928338\n",
      "07/14 06:11:34 PM block_2-lambda: 1.1167823148060994\n",
      "07/14 06:11:39 PM block_2-lambda: 0.659115588329882\n",
      "07/14 06:11:45 PM block_2-lambda: 0.5047798309938974\n",
      "07/14 06:11:50 PM block_2-lambda: 0.6887158007028112\n",
      "07/14 06:11:56 PM block_2-lambda: 0.5879060676298946\n",
      "07/14 06:12:01 PM block_2-lambda: 0.8642611829152548\n",
      "07/14 06:12:07 PM block_2-lambda: 0.5379211548341805\n",
      "07/14 06:12:12 PM block_2-lambda: -0.0020203801044019827\n",
      "07/14 06:12:17 PM block_2-lambda: -0.2868382097726845\n",
      "07/14 06:12:23 PM block_2-lambda: -0.6276061889158195\n",
      "07/14 06:12:28 PM block_2-lambda: 0.3470711765519108\n",
      "07/14 06:12:34 PM block_2-lambda: 0.5990484153410517\n",
      "07/14 06:12:39 PM block_2-lambda: 0.8868788753647402\n",
      "07/14 06:12:44 PM block_2-lambda: 0.8226390057299511\n",
      "07/14 06:12:50 PM block_2-lambda: 0.906586551283258\n",
      "07/14 06:12:55 PM block_2-lambda: 0.8497540577348456\n",
      "07/14 06:13:01 PM block_2-lambda: 0.982372605657003\n",
      "07/14 06:13:06 PM block_2-lambda: 1.0231405357759162\n",
      "07/14 06:13:12 PM block_2-lambda: 0.8535740863846009\n",
      "07/14 06:13:17 PM block_2-lambda: 0.9706332654838102\n",
      "07/14 06:13:22 PM block_2-lambda: 0.7077413658540411\n",
      "07/14 06:13:28 PM block_2-lambda: 0.9300819611453627\n",
      "07/14 06:13:33 PM block_2-lambda: 1.1934145272496235\n",
      "07/14 06:13:39 PM block_2-lambda: 1.1118523164925327\n",
      "07/14 06:13:44 PM block_2-lambda: 0.6653241996305534\n",
      "07/14 06:13:50 PM block_2-lambda: 0.8006117487528238\n",
      "07/14 06:13:55 PM block_2-lambda: 0.8408766290748934\n",
      "07/14 06:14:01 PM block_2-lambda: 1.0127972828815262\n",
      "07/14 06:14:06 PM block_2-lambda: 1.0950982886624623\n",
      "07/14 06:14:12 PM block_2-lambda: 0.6512054075677711\n",
      "07/14 06:14:17 PM block_2-lambda: 0.6345292669702246\n",
      "07/14 06:14:22 PM block_2-lambda: 1.1702960523735568\n",
      "07/14 06:14:28 PM block_2-lambda: 0.5565496850683986\n",
      "07/14 06:14:33 PM block_2-lambda: 0.37166307441680785\n",
      "07/14 06:14:39 PM block_2-lambda: 0.6329715223197477\n",
      "07/14 06:14:44 PM block_2-lambda: 0.580233102821442\n",
      "07/14 06:14:50 PM block_2-lambda: 1.0313876355029492\n",
      "07/14 06:14:55 PM block_2-lambda: 0.6735470936480297\n",
      "07/14 06:15:01 PM block_2-lambda: 0.6737624325426707\n",
      "07/14 06:15:01 PM block_id: 3\n",
      "07/14 06:15:06 PM block_3-lambda: -7.837368166036755e-07\n",
      "07/14 06:15:11 PM block_3-lambda: 0.0017345784658409026\n",
      "07/14 06:15:17 PM block_3-lambda: 0.39798163601672315\n",
      "07/14 06:15:22 PM block_3-lambda: 1.1947682085764935\n",
      "07/14 06:15:27 PM block_3-lambda: 1.0192309153606613\n",
      "07/14 06:15:32 PM block_3-lambda: 1.4167220640373996\n",
      "07/14 06:15:38 PM block_3-lambda: 1.0821247637032505\n",
      "07/14 06:15:43 PM block_3-lambda: 1.1958162238798946\n",
      "07/14 06:15:48 PM block_3-lambda: 1.5398264199375626\n",
      "07/14 06:15:54 PM block_3-lambda: 1.1517132985065262\n",
      "07/14 06:15:59 PM block_3-lambda: 1.123346412995733\n",
      "07/14 06:16:04 PM block_3-lambda: 1.074569273186496\n",
      "07/14 06:16:10 PM block_3-lambda: 0.9632630252455133\n",
      "07/14 06:16:15 PM block_3-lambda: 0.8199996641840801\n",
      "07/14 06:16:20 PM block_3-lambda: 0.8124042802067646\n",
      "07/14 06:16:20 PM block_id: 4\n",
      "07/14 06:16:25 PM block_4-lambda: 7.857830262739197e-07\n",
      "07/14 06:16:30 PM block_4-lambda: 0.002379892580959213\n",
      "07/14 06:16:36 PM block_4-lambda: 0.0319607382318581\n",
      "07/14 06:16:41 PM block_4-lambda: 1.3814749966663529\n",
      "07/14 06:16:46 PM block_4-lambda: 1.665296684786019\n",
      "07/14 06:16:51 PM block_4-lambda: 1.6991661715220256\n",
      "07/14 06:16:56 PM block_4-lambda: 1.7067896831466491\n",
      "07/14 06:16:56 PM block_id: 5\n",
      "07/14 06:17:01 PM block_5-lambda: -5.084913257644119e-06\n",
      "07/14 06:17:07 PM block_5-lambda: -0.027164405608272935\n",
      "07/14 06:17:12 PM block_5-lambda: 0.6169199503090487\n",
      "07/14 06:17:17 PM block_5-lambda: 1.5677953513271838\n",
      "07/14 06:17:22 PM block_5-lambda: 1.6397192775006275\n",
      "07/14 06:17:27 PM block_5-lambda: 1.864877723785768\n",
      "07/14 06:17:32 PM block_5-lambda: 1.5796631349617218\n",
      "07/14 06:17:37 PM block_5-lambda: 1.7607204942818147\n",
      "07/14 06:17:42 PM block_5-lambda: 1.5214635397056977\n",
      "07/14 06:17:47 PM block_5-lambda: 1.7316003516017193\n",
      "07/14 06:17:52 PM block_5-lambda: 2.1439946798914407\n",
      "07/14 06:17:57 PM block_5-lambda: 2.061499905873494\n",
      "07/14 06:18:02 PM block_5-lambda: 2.1941268491936494\n",
      "07/14 06:18:07 PM block_5-lambda: 2.275649963133785\n",
      "07/14 06:18:12 PM block_5-lambda: 1.3928133187045055\n",
      "07/14 06:18:17 PM block_5-lambda: 1.2286911317143574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:18:22 PM block_5-lambda: 1.8589051028332078\n",
      "07/14 06:18:28 PM block_5-lambda: 1.9751552597107178\n",
      "07/14 06:18:33 PM block_5-lambda: 2.140727460623745\n",
      "07/14 06:18:38 PM block_5-lambda: 1.3707215335953187\n",
      "07/14 06:18:43 PM block_5-lambda: 1.9941282463839733\n",
      "07/14 06:18:48 PM block_5-lambda: 2.3416007781124497\n",
      "07/14 06:18:53 PM block_5-lambda: 1.3609137477644955\n",
      "07/14 06:18:58 PM block_5-lambda: 1.3422852788105548\n",
      "07/14 06:19:03 PM block_5-lambda: 1.8450848070014432\n",
      "07/14 06:19:08 PM block_5-lambda: 1.7365223834792294\n",
      "07/14 06:19:13 PM block_5-lambda: 1.2735859208317646\n",
      "07/14 06:19:18 PM block_5-lambda: 1.5211440243395458\n",
      "07/14 06:19:23 PM block_5-lambda: 2.2248782728570533\n",
      "07/14 06:19:28 PM block_5-lambda: 2.2595700183546685\n",
      "07/14 06:19:33 PM block_5-lambda: 2.3595867846385543\n",
      "07/14 06:19:38 PM block_5-lambda: 1.9759598697524472\n",
      "07/14 06:19:43 PM block_5-lambda: 1.3626204034889557\n",
      "07/14 06:19:49 PM block_5-lambda: 1.3521136548145707\n",
      "07/14 06:19:49 PM block_id: 6\n",
      "07/14 06:19:54 PM block_6-lambda: 3.1052106321905273e-06\n",
      "07/14 06:19:58 PM block_6-lambda: 0.6501174375235316\n",
      "07/14 06:20:03 PM block_6-lambda: 2.9154547879016066\n",
      "07/14 06:20:08 PM block_6-lambda: 3.2182330395801957\n",
      "07/14 06:20:13 PM block_6-lambda: 2.146545900398469\n",
      "07/14 06:20:18 PM block_6-lambda: 2.3633996434958586\n",
      "07/14 06:20:23 PM block_6-lambda: 2.7179893891974145\n",
      "07/14 06:20:28 PM block_6-lambda: 2.695300243944528\n",
      "07/14 06:20:28 PM block_id: 7\n",
      "07/14 06:20:33 PM block_7-lambda: 3.650106805709591e-06\n",
      "07/14 06:20:38 PM block_7-lambda: 1.0337497450740463\n",
      "07/14 06:20:42 PM block_7-lambda: 2.836050010589232\n",
      "07/14 06:20:47 PM block_7-lambda: 3.1733748960686494\n",
      "07/14 06:20:52 PM block_7-lambda: 2.2975902633973395\n",
      "07/14 06:20:57 PM block_7-lambda: 3.6363209184393823\n",
      "07/14 06:21:02 PM block_7-lambda: 2.9590245748619477\n",
      "07/14 06:21:06 PM block_7-lambda: 2.1173281995168174\n",
      "07/14 06:21:11 PM block_7-lambda: 2.317591655685241\n",
      "07/14 06:21:16 PM block_7-lambda: 2.024607585615901\n",
      "07/14 06:21:21 PM block_7-lambda: 2.502941453313253\n",
      "07/14 06:21:26 PM block_7-lambda: 3.3723796553401106\n",
      "07/14 06:21:30 PM block_7-lambda: 4.048013342432229\n",
      "07/14 06:21:35 PM block_7-lambda: 3.3762135946128264\n",
      "07/14 06:21:40 PM block_7-lambda: 3.8394179746329065\n",
      "07/14 06:21:45 PM block_7-lambda: 2.4587922000502007\n",
      "07/14 06:21:50 PM block_7-lambda: 2.4856795345444276\n",
      "07/14 06:21:54 PM block_7-lambda: 2.7318951097358184\n",
      "07/14 06:21:59 PM block_7-lambda: 3.2839889832768576\n",
      "07/14 06:22:04 PM block_7-lambda: 2.7672928040286147\n",
      "07/14 06:22:09 PM block_7-lambda: 2.1679047733904366\n",
      "07/14 06:22:14 PM block_7-lambda: 2.104825307087726\n",
      "07/14 06:22:18 PM block_7-lambda: 2.4829881047628013\n",
      "07/14 06:22:23 PM block_7-lambda: 3.1339628533665915\n",
      "07/14 06:22:28 PM block_7-lambda: 2.7423318763334588\n",
      "07/14 06:22:33 PM block_7-lambda: 2.8928526606425704\n",
      "07/14 06:22:38 PM block_7-lambda: 2.6468648029618476\n",
      "07/14 06:22:43 PM block_7-lambda: 3.134739887283509\n",
      "07/14 06:22:47 PM block_7-lambda: 3.6464069167294175\n",
      "07/14 06:22:52 PM block_7-lambda: 1.5263333607868976\n",
      "07/14 06:22:57 PM block_7-lambda: 1.5492026700552208\n",
      "07/14 06:23:02 PM block_7-lambda: 3.3892064841396836\n",
      "07/14 06:23:07 PM block_7-lambda: 3.08536636781501\n",
      "07/14 06:23:11 PM block_7-lambda: 2.563901602503765\n",
      "07/14 06:23:16 PM block_7-lambda: 2.802303206968499\n",
      "07/14 06:23:21 PM block_7-lambda: 2.3486259491089356\n",
      "07/14 06:23:26 PM block_7-lambda: 2.8007917902077057\n",
      "07/14 06:23:31 PM block_7-lambda: 3.159366714906501\n",
      "07/14 06:23:35 PM block_7-lambda: 2.982892262409011\n",
      "07/14 06:23:40 PM block_7-lambda: 2.6667828540725402\n",
      "07/14 06:23:45 PM block_7-lambda: 2.9110075556130774\n",
      "07/14 06:23:50 PM block_7-lambda: 2.269938396162776\n",
      "07/14 06:23:55 PM block_7-lambda: 1.9980560670416039\n",
      "07/14 06:24:00 PM block_7-lambda: 2.5219265734814256\n",
      "07/14 06:24:04 PM block_7-lambda: 3.044502963023971\n",
      "07/14 06:24:09 PM block_7-lambda: 3.229157352064508\n",
      "07/14 06:24:14 PM block_7-lambda: 2.5957550906752007\n",
      "07/14 06:24:19 PM block_7-lambda: 2.6522584478539155\n",
      "07/14 06:24:24 PM block_7-lambda: 3.2369352899880774\n",
      "07/14 06:24:28 PM block_7-lambda: 1.9287387587459213\n",
      "07/14 06:24:33 PM block_7-lambda: 2.211541723534764\n",
      "07/14 06:24:38 PM block_7-lambda: 2.9362476762518823\n",
      "07/14 06:24:43 PM block_7-lambda: 2.7609316661175956\n",
      "07/14 06:24:48 PM block_7-lambda: 2.442976986069277\n",
      "07/14 06:24:52 PM block_7-lambda: 3.2885455395801957\n",
      "07/14 06:24:57 PM block_7-lambda: 3.8808799651731927\n",
      "07/14 06:25:02 PM block_7-lambda: 2.525812233308233\n",
      "07/14 06:25:07 PM block_7-lambda: 2.7832751906061746\n",
      "07/14 06:25:12 PM block_7-lambda: 2.9954431985755523\n",
      "07/14 06:25:16 PM block_7-lambda: 2.3137920333678466\n",
      "07/14 06:25:21 PM block_7-lambda: 1.9034221603209713\n",
      "07/14 06:25:26 PM block_7-lambda: 2.989956162540788\n",
      "07/14 06:25:31 PM block_7-lambda: 3.613046669098268\n",
      "07/14 06:25:36 PM block_7-lambda: 3.2618462129769075\n",
      "07/14 06:25:40 PM block_7-lambda: 3.1649508189006026\n",
      "07/14 06:25:45 PM block_7-lambda: 3.3429928208929467\n",
      "07/14 06:25:50 PM block_7-lambda: 3.0898618889620986\n",
      "07/14 06:25:55 PM block_7-lambda: 3.3657613853852912\n",
      "07/14 06:26:00 PM block_7-lambda: 2.269747937060743\n",
      "07/14 06:26:04 PM block_7-lambda: 2.928669512032505\n",
      "07/14 06:26:09 PM block_7-lambda: 3.600560199783509\n",
      "07/14 06:26:14 PM block_7-lambda: 3.676775706340989\n",
      "07/14 06:26:19 PM block_7-lambda: 2.3643472817049447\n",
      "07/14 06:26:24 PM block_7-lambda: 2.662228013616968\n",
      "07/14 06:26:28 PM block_7-lambda: 3.4910920537619226\n",
      "07/14 06:26:33 PM block_7-lambda: 2.8242246329066263\n",
      "07/14 06:26:38 PM block_7-lambda: 3.3276734869164155\n",
      "07/14 06:26:43 PM block_7-lambda: 3.3612197814696283\n",
      "07/14 06:26:43 PM block_id: 8\n",
      "07/14 06:26:48 PM block_8-lambda: 8.027891892131911e-07\n",
      "07/14 06:26:52 PM block_8-lambda: 0.021087520093802947\n",
      "07/14 06:26:57 PM block_8-lambda: 2.8459756996736947\n",
      "07/14 06:27:02 PM block_8-lambda: 3.6998070406626504\n",
      "07/14 06:27:06 PM block_8-lambda: 4.531985363328313\n",
      "07/14 06:27:11 PM block_8-lambda: 2.6468084251066766\n",
      "07/14 06:27:16 PM block_8-lambda: 2.6811457254800453\n",
      "07/14 06:27:20 PM block_8-lambda: 3.3605528069308486\n",
      "07/14 06:27:25 PM block_8-lambda: 2.753995228962726\n",
      "07/14 06:27:30 PM block_8-lambda: 3.3395393095820785\n",
      "07/14 06:27:35 PM block_8-lambda: 3.666874039125251\n",
      "07/14 06:27:39 PM block_8-lambda: 4.723240128482681\n",
      "07/14 06:27:44 PM block_8-lambda: 2.6503876835466866\n",
      "07/14 06:27:49 PM block_8-lambda: 3.446300828313253\n",
      "07/14 06:27:53 PM block_8-lambda: 4.096211751302083\n",
      "07/14 06:27:58 PM block_8-lambda: 4.931504827905371\n",
      "07/14 06:28:03 PM block_8-lambda: 3.1212322434268325\n",
      "07/14 06:28:07 PM block_8-lambda: 4.747922843718625\n",
      "07/14 06:28:12 PM block_8-lambda: 3.8404254223926957\n",
      "07/14 06:28:17 PM block_8-lambda: 1.8184791166619603\n",
      "07/14 06:28:21 PM block_8-lambda: 2.4772745768229165\n",
      "07/14 06:28:26 PM block_8-lambda: 2.7358579827120986\n",
      "07/14 06:28:31 PM block_8-lambda: 3.191804326681727\n",
      "07/14 06:28:35 PM block_8-lambda: 3.3093107292451056\n",
      "07/14 06:28:40 PM block_8-lambda: 3.5368179260008783\n",
      "07/14 06:28:45 PM block_8-lambda: 2.9488758255678964\n",
      "07/14 06:28:49 PM block_8-lambda: 3.5886453528959588\n",
      "07/14 06:28:54 PM block_8-lambda: 3.2801253843498994\n",
      "07/14 06:28:59 PM block_8-lambda: 3.878690053181476\n",
      "07/14 06:29:03 PM block_8-lambda: 4.452507785046436\n",
      "07/14 06:29:08 PM block_8-lambda: 3.3746244744603415\n",
      "07/14 06:29:13 PM block_8-lambda: 3.9474737328219125\n",
      "07/14 06:29:18 PM block_8-lambda: 3.131499386216742\n",
      "07/14 06:29:22 PM block_8-lambda: 4.153134020456827\n",
      "07/14 06:29:27 PM block_8-lambda: 3.2630980464828063\n",
      "07/14 06:29:32 PM block_8-lambda: 3.5152266781971635\n",
      "07/14 06:29:36 PM block_8-lambda: 4.301398955195783\n",
      "07/14 06:29:41 PM block_8-lambda: 4.307213718153865\n",
      "07/14 06:29:41 PM block_id: 9\n",
      "07/14 06:29:46 PM block_9-lambda: -8.217048673258429e-07\n",
      "07/14 06:29:50 PM block_9-lambda: 0.3454588311743066\n",
      "07/14 06:29:55 PM block_9-lambda: 7.699924208552962\n",
      "07/14 06:29:59 PM block_9-lambda: 7.613121921278865\n",
      "07/14 06:30:04 PM block_9-lambda: 8.684549722326807\n",
      "07/14 06:30:08 PM block_9-lambda: 9.578209321661646\n",
      "07/14 06:30:13 PM block_9-lambda: 8.44962369007279\n",
      "07/14 06:30:18 PM block_9-lambda: 7.799374254831827\n",
      "07/14 06:30:22 PM block_9-lambda: 8.422041682354418\n",
      "07/14 06:30:27 PM block_9-lambda: 8.959467753827811\n",
      "07/14 06:30:31 PM block_9-lambda: 7.508851813504016\n",
      "07/14 06:30:36 PM block_9-lambda: 7.667766770205823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:30:40 PM block_9-lambda: 7.166424487010542\n",
      "07/14 06:30:45 PM block_9-lambda: 8.325072751945282\n",
      "07/14 06:30:49 PM block_9-lambda: 5.396272100119227\n",
      "07/14 06:30:54 PM block_9-lambda: 5.445457121454568\n",
      "07/14 06:30:54 PM block_id: 10\n",
      "07/14 06:30:58 PM block_10-lambda: -6.256748484559806e-06\n",
      "07/14 06:31:03 PM block_10-lambda: -0.05772609787293706\n",
      "07/14 06:31:07 PM block_10-lambda: 0.4737388273798318\n",
      "07/14 06:31:12 PM block_10-lambda: 12.289848848519076\n",
      "07/14 06:31:16 PM block_10-lambda: 10.691254274912149\n",
      "07/14 06:31:20 PM block_10-lambda: 11.076077748493976\n",
      "07/14 06:31:25 PM block_10-lambda: 8.378625831450803\n",
      "07/14 06:31:29 PM block_10-lambda: 11.694433005459338\n",
      "07/14 06:31:34 PM block_10-lambda: 10.024838612261545\n",
      "07/14 06:31:38 PM block_10-lambda: 10.255416196034137\n",
      "07/14 06:31:43 PM block_10-lambda: 12.171640664219378\n",
      "07/14 06:31:47 PM block_10-lambda: 12.773654187060743\n",
      "07/14 06:31:51 PM block_10-lambda: 5.282108414125251\n",
      "07/14 06:31:56 PM block_10-lambda: 6.903242363987199\n",
      "07/14 06:32:00 PM block_10-lambda: 10.507444818335843\n",
      "07/14 06:32:05 PM block_10-lambda: 12.121108457266565\n",
      "07/14 06:32:09 PM block_10-lambda: 8.729460811997992\n",
      "07/14 06:32:13 PM block_10-lambda: 5.347372399755271\n",
      "07/14 06:32:18 PM block_10-lambda: 6.001341302710843\n",
      "07/14 06:32:22 PM block_10-lambda: 12.515012197226406\n",
      "07/14 06:32:27 PM block_10-lambda: 7.915216530183233\n",
      "07/14 06:32:31 PM block_10-lambda: 11.12606382561496\n",
      "07/14 06:32:36 PM block_10-lambda: 9.393478209713855\n",
      "07/14 06:32:40 PM block_10-lambda: 7.844105425608685\n",
      "07/14 06:32:44 PM block_10-lambda: 8.476389934738956\n",
      "07/14 06:32:49 PM block_10-lambda: 11.50143444873243\n",
      "07/14 06:32:53 PM block_10-lambda: 10.163111430095382\n",
      "07/14 06:32:58 PM block_10-lambda: 10.627883604731426\n",
      "07/14 06:33:02 PM block_10-lambda: 7.198262777673193\n",
      "07/14 06:33:07 PM block_10-lambda: 6.860500596134538\n",
      "07/14 06:33:11 PM block_10-lambda: 9.694225142758533\n",
      "07/14 06:33:15 PM block_10-lambda: 10.768736077120984\n",
      "07/14 06:33:20 PM block_10-lambda: 11.03231382561496\n",
      "07/14 06:33:24 PM block_10-lambda: 7.746189347232681\n",
      "07/14 06:33:29 PM block_10-lambda: 10.803799181099398\n",
      "07/14 06:33:33 PM block_10-lambda: 11.957295000313755\n",
      "07/14 06:33:38 PM block_10-lambda: 10.167735394703815\n",
      "07/14 06:33:42 PM block_10-lambda: 11.850482986634036\n",
      "07/14 06:33:46 PM block_10-lambda: 10.067337710215863\n",
      "07/14 06:33:51 PM block_10-lambda: 6.402504059205572\n",
      "07/14 06:33:55 PM block_10-lambda: 10.951637605107932\n",
      "07/14 06:34:00 PM block_10-lambda: 8.951064021711847\n",
      "07/14 06:34:04 PM block_10-lambda: 6.893729703972139\n",
      "07/14 06:34:09 PM block_10-lambda: 8.194152096667922\n",
      "07/14 06:34:13 PM block_10-lambda: 9.057995654492972\n",
      "07/14 06:34:18 PM block_10-lambda: 8.5352523374749\n",
      "07/14 06:34:22 PM block_10-lambda: 6.405187645111697\n",
      "07/14 06:34:26 PM block_10-lambda: 11.500703007341867\n",
      "07/14 06:34:31 PM block_10-lambda: 8.592492038466366\n",
      "07/14 06:34:35 PM block_10-lambda: 10.883009577371988\n",
      "07/14 06:34:40 PM block_10-lambda: 11.225127070783133\n",
      "07/14 06:34:44 PM block_10-lambda: 11.606360010353916\n",
      "07/14 06:34:49 PM block_10-lambda: 8.5372054624749\n",
      "07/14 06:34:53 PM block_10-lambda: 7.353265601468373\n",
      "07/14 06:34:58 PM block_10-lambda: 9.346198269641064\n",
      "07/14 06:35:02 PM block_10-lambda: 10.199000101970382\n",
      "07/14 06:35:06 PM block_10-lambda: 10.878072838227911\n",
      "07/14 06:35:11 PM block_10-lambda: 9.94408787493725\n",
      "07/14 06:35:15 PM block_10-lambda: 11.22086000251004\n",
      "07/14 06:35:20 PM block_10-lambda: 12.966096809111447\n",
      "07/14 06:35:24 PM block_10-lambda: 9.977962631777109\n",
      "07/14 06:35:29 PM block_10-lambda: 7.969992764024849\n",
      "07/14 06:35:33 PM block_10-lambda: 9.211499317582831\n",
      "07/14 06:35:37 PM block_10-lambda: 10.178831537085843\n",
      "07/14 06:35:42 PM block_10-lambda: 11.009581293925702\n",
      "07/14 06:35:46 PM block_10-lambda: 11.12307727001757\n",
      "07/14 06:35:51 PM block_10-lambda: 11.539021319653614\n",
      "07/14 06:35:55 PM block_10-lambda: 8.809842887173694\n",
      "07/14 06:35:59 PM block_10-lambda: 12.00031375502008\n",
      "07/14 06:36:04 PM block_10-lambda: 11.103800945971386\n",
      "07/14 06:36:08 PM block_10-lambda: 9.778837027798694\n",
      "07/14 06:36:13 PM block_10-lambda: 10.732393440951306\n",
      "07/14 06:36:17 PM block_10-lambda: 12.708784356174698\n",
      "07/14 06:36:22 PM block_10-lambda: 13.071805777798694\n",
      "07/14 06:36:26 PM block_10-lambda: 9.489950034513052\n",
      "07/14 06:36:30 PM block_10-lambda: 8.561881314319779\n",
      "07/14 06:36:35 PM block_10-lambda: 11.950757130082831\n",
      "07/14 06:36:39 PM block_10-lambda: 12.531870646649097\n",
      "07/14 06:36:44 PM block_10-lambda: 9.154425318461346\n",
      "07/14 06:36:48 PM block_10-lambda: 10.648192378890561\n",
      "07/14 06:36:53 PM block_10-lambda: 13.23683405496988\n",
      "07/14 06:36:57 PM block_10-lambda: 11.225026080886044\n",
      "07/14 06:37:01 PM block_10-lambda: 10.331591992971887\n",
      "07/14 06:37:06 PM block_10-lambda: 8.595993348393574\n",
      "07/14 06:37:10 PM block_10-lambda: 10.380569151606426\n",
      "07/14 06:37:15 PM block_10-lambda: 11.928512879643574\n",
      "07/14 06:37:19 PM block_10-lambda: 12.037521178463855\n",
      "07/14 06:37:19 PM block_id: 11\n",
      "07/14 06:37:24 PM block_11-lambda: 4.056070643256467e-06\n",
      "07/14 06:37:28 PM block_11-lambda: 1.283589803550138\n",
      "07/14 06:37:32 PM block_11-lambda: 7.5149215024159135\n",
      "07/14 06:37:37 PM block_11-lambda: 10.197362692959338\n",
      "07/14 06:37:41 PM block_11-lambda: 12.061873470444278\n",
      "07/14 06:37:45 PM block_11-lambda: 10.312941217996988\n",
      "07/14 06:37:49 PM block_11-lambda: 8.360392742846386\n",
      "07/14 06:37:54 PM block_11-lambda: 12.08397162870231\n",
      "07/14 06:37:58 PM block_11-lambda: 9.499054813002008\n",
      "07/14 06:38:02 PM block_11-lambda: 9.686783265876004\n",
      "07/14 06:38:07 PM block_11-lambda: 12.64253890562249\n",
      "07/14 06:38:11 PM block_11-lambda: 9.392581066453314\n",
      "07/14 06:38:15 PM block_11-lambda: 8.672800577309237\n",
      "07/14 06:38:20 PM block_11-lambda: 8.69048655559739\n",
      "07/14 06:38:20 PM block_id: 12\n",
      "07/14 06:38:24 PM block_12-lambda: 3.1469674990418926e-06\n",
      "07/14 06:38:28 PM block_12-lambda: 0.13646963035246454\n",
      "07/14 06:38:32 PM block_12-lambda: 7.6898247286019075\n",
      "07/14 06:38:36 PM block_12-lambda: 9.481193327999499\n",
      "07/14 06:38:40 PM block_12-lambda: 12.668655089106426\n",
      "07/14 06:38:45 PM block_12-lambda: 10.83528743881777\n",
      "07/14 06:38:49 PM block_12-lambda: 13.385265672063253\n",
      "07/14 06:38:53 PM block_12-lambda: 14.078131863391064\n",
      "07/14 06:38:57 PM block_12-lambda: 12.760247042858936\n",
      "07/14 06:39:01 PM block_12-lambda: 12.93523704191767\n",
      "07/14 06:39:06 PM block_12-lambda: 10.528112449799197\n",
      "07/14 06:39:10 PM block_12-lambda: 10.823932448544177\n",
      "07/14 06:39:14 PM block_12-lambda: 10.539448810868475\n",
      "07/14 06:39:18 PM block_12-lambda: 12.539095836470883\n",
      "07/14 06:39:22 PM block_12-lambda: 13.452346495356426\n",
      "07/14 06:39:27 PM block_12-lambda: 15.11265766189759\n",
      "07/14 06:39:31 PM block_12-lambda: 10.06111849742721\n",
      "07/14 06:39:35 PM block_12-lambda: 9.478239128388553\n",
      "07/14 06:39:39 PM block_12-lambda: 13.398596338478916\n",
      "07/14 06:39:43 PM block_12-lambda: 10.843486249686245\n",
      "07/14 06:39:48 PM block_12-lambda: 11.289977291980422\n",
      "07/14 06:39:52 PM block_12-lambda: 8.94188178495231\n",
      "07/14 06:39:56 PM block_12-lambda: 11.394924424259537\n",
      "07/14 06:40:00 PM block_12-lambda: 9.979690245356426\n",
      "07/14 06:40:04 PM block_12-lambda: 10.910911223017068\n",
      "07/14 06:40:09 PM block_12-lambda: 13.212263114959839\n",
      "07/14 06:40:13 PM block_12-lambda: 11.745109343624499\n",
      "07/14 06:40:17 PM block_12-lambda: 11.52236092808735\n",
      "07/14 06:40:21 PM block_12-lambda: 12.438600103539157\n",
      "07/14 06:40:25 PM block_12-lambda: 7.762399206199799\n",
      "07/14 06:40:29 PM block_12-lambda: 10.675024316014056\n",
      "07/14 06:40:34 PM block_12-lambda: 10.50827038623243\n",
      "07/14 06:40:38 PM block_12-lambda: 12.50636040254769\n",
      "07/14 06:40:42 PM block_12-lambda: 10.456246862449799\n",
      "07/14 06:40:46 PM block_12-lambda: 11.32458937311747\n",
      "07/14 06:40:50 PM block_12-lambda: 12.861172227974398\n",
      "07/14 06:40:55 PM block_12-lambda: 13.905176369540662\n",
      "07/14 06:40:59 PM block_12-lambda: 12.652332964671185\n",
      "07/14 06:41:03 PM block_12-lambda: 12.93926585247239\n",
      "07/14 06:41:07 PM block_12-lambda: 13.748863618536646\n",
      "07/14 06:41:11 PM block_12-lambda: 9.80495321128263\n",
      "07/14 06:41:15 PM block_12-lambda: 11.286430879769076\n",
      "07/14 06:41:20 PM block_12-lambda: 12.186770519578314\n",
      "07/14 06:41:24 PM block_12-lambda: 13.394840102597891\n",
      "07/14 06:41:28 PM block_12-lambda: 13.157585419804217\n",
      "07/14 06:41:32 PM block_12-lambda: 12.456114497050702\n",
      "07/14 06:41:36 PM block_12-lambda: 12.859749545055221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:41:40 PM block_12-lambda: 12.976513475778113\n",
      "07/14 06:41:40 PM block_id: 13\n",
      "07/14 06:41:45 PM block_13-lambda: 6.401476573692747e-06\n",
      "07/14 06:41:49 PM block_13-lambda: 5.047662819245733\n",
      "07/14 06:41:53 PM block_13-lambda: 8.585212922000501\n",
      "07/14 06:41:57 PM block_13-lambda: 12.37183891817269\n",
      "07/14 06:42:01 PM block_13-lambda: 11.573087270958835\n",
      "07/14 06:42:05 PM block_13-lambda: 11.988152806538654\n",
      "07/14 06:42:09 PM block_13-lambda: 10.941034646398093\n",
      "07/14 06:42:13 PM block_13-lambda: 12.708130373054718\n",
      "07/14 06:42:17 PM block_13-lambda: 10.657008894954819\n",
      "07/14 06:42:21 PM block_13-lambda: 15.299167372615463\n",
      "07/14 06:42:25 PM block_13-lambda: 14.961269884224398\n",
      "07/14 06:42:29 PM block_13-lambda: 14.037855523657129\n",
      "07/14 06:42:33 PM block_13-lambda: 13.241739418611948\n",
      "07/14 06:42:37 PM block_13-lambda: 8.738068484877008\n",
      "07/14 06:42:41 PM block_13-lambda: 14.585494321034137\n",
      "07/14 06:42:45 PM block_13-lambda: 12.88169180628765\n",
      "07/14 06:42:49 PM block_13-lambda: 10.489403904681225\n",
      "07/14 06:42:53 PM block_13-lambda: 10.868379769076306\n",
      "07/14 06:42:58 PM block_13-lambda: 9.676739183295682\n",
      "07/14 06:43:02 PM block_13-lambda: 9.766276041666666\n",
      "07/14 06:43:02 PM block_id: 14\n",
      "07/14 06:43:06 PM block_14-lambda: 1.2289825265845621e-06\n",
      "07/14 06:43:09 PM block_14-lambda: 1.715983808279995\n",
      "07/14 06:43:13 PM block_14-lambda: 14.4542496156501\n",
      "07/14 06:43:17 PM block_14-lambda: 11.432543651167169\n",
      "07/14 06:43:21 PM block_14-lambda: 15.145818626066767\n",
      "07/14 06:43:25 PM block_14-lambda: 14.426861743850402\n",
      "07/14 06:43:29 PM block_14-lambda: 14.842497921372992\n",
      "07/14 06:43:33 PM block_14-lambda: 13.408109978978414\n",
      "07/14 06:43:37 PM block_14-lambda: 14.862930236571286\n",
      "07/14 06:43:41 PM block_14-lambda: 12.39723444559488\n",
      "07/14 06:43:45 PM block_14-lambda: 13.659193414282129\n",
      "07/14 06:43:49 PM block_14-lambda: 18.52639660203313\n",
      "07/14 06:43:53 PM block_14-lambda: 13.568364277422189\n",
      "07/14 06:43:57 PM block_14-lambda: 11.692170047377008\n",
      "07/14 06:44:01 PM block_14-lambda: 13.818606457078314\n",
      "07/14 06:44:05 PM block_14-lambda: 13.667206913591867\n",
      "07/14 06:44:09 PM block_14-lambda: 14.107889566076807\n",
      "07/14 06:44:13 PM block_14-lambda: 11.273024716051706\n",
      "07/14 06:44:17 PM block_14-lambda: 14.759605806036646\n",
      "07/14 06:44:21 PM block_14-lambda: 13.81453548569277\n",
      "07/14 06:44:25 PM block_14-lambda: 14.435783171749499\n",
      "07/14 06:44:28 PM block_14-lambda: 16.162775123933233\n",
      "07/14 06:44:32 PM block_14-lambda: 13.189207023406125\n",
      "07/14 06:44:36 PM block_14-lambda: 17.24437594126506\n",
      "07/14 06:44:40 PM block_14-lambda: 14.945025218059738\n",
      "07/14 06:44:44 PM block_14-lambda: 15.442924039909638\n",
      "07/14 06:44:48 PM block_14-lambda: 11.985248611634036\n",
      "07/14 06:44:52 PM block_14-lambda: 12.542303001066767\n",
      "07/14 06:44:56 PM block_14-lambda: 15.308209400100402\n",
      "07/14 06:45:00 PM block_14-lambda: 11.76959204003514\n",
      "07/14 06:45:04 PM block_14-lambda: 9.211487551769578\n",
      "07/14 06:45:08 PM block_14-lambda: 11.757383047816266\n",
      "07/14 06:45:12 PM block_14-lambda: 13.151378953313253\n",
      "07/14 06:45:16 PM block_14-lambda: 13.570062476468374\n",
      "07/14 06:45:20 PM block_14-lambda: 16.240820704693775\n",
      "07/14 06:45:24 PM block_14-lambda: 18.616897276606426\n",
      "07/14 06:45:27 PM block_14-lambda: 15.108867109061245\n",
      "07/14 06:45:31 PM block_14-lambda: 12.533846322791165\n",
      "07/14 06:45:35 PM block_14-lambda: 13.80524539564508\n",
      "07/14 06:45:39 PM block_14-lambda: 11.483454325112952\n",
      "07/14 06:45:43 PM block_14-lambda: 11.260299989018574\n",
      "07/14 06:45:47 PM block_14-lambda: 15.479942229856928\n",
      "07/14 06:45:51 PM block_14-lambda: 10.695256612387048\n",
      "07/14 06:45:55 PM block_14-lambda: 12.430000274535642\n",
      "07/14 06:45:59 PM block_14-lambda: 9.788502643386044\n",
      "07/14 06:46:03 PM block_14-lambda: 14.10880337757279\n",
      "07/14 06:46:07 PM block_14-lambda: 13.362788066327811\n",
      "07/14 06:46:11 PM block_14-lambda: 15.337201344440262\n",
      "07/14 06:46:15 PM block_14-lambda: 13.79122054624749\n",
      "07/14 06:46:18 PM block_14-lambda: 12.728972530747992\n",
      "07/14 06:46:22 PM block_14-lambda: 14.414795902359439\n",
      "07/14 06:46:26 PM block_14-lambda: 14.698540254769076\n",
      "07/14 06:46:30 PM block_14-lambda: 12.063182417168674\n",
      "07/14 06:46:34 PM block_14-lambda: 15.70462023876757\n",
      "07/14 06:46:38 PM block_14-lambda: 15.861339890813253\n",
      "07/14 06:46:38 PM block_id: 15\n",
      "07/14 06:46:42 PM block_15-lambda: 8.350922669990954e-06\n",
      "07/14 06:46:46 PM block_15-lambda: 0.46091111884059677\n",
      "07/14 06:46:50 PM block_15-lambda: 10.790076320908634\n",
      "07/14 06:46:53 PM block_15-lambda: 12.953898602221386\n",
      "07/14 06:46:57 PM block_15-lambda: 10.196890099460342\n",
      "07/14 06:47:01 PM block_15-lambda: 13.793455070281125\n",
      "07/14 06:47:05 PM block_15-lambda: 14.67840110441767\n",
      "07/14 06:47:09 PM block_15-lambda: 11.210769837161145\n",
      "07/14 06:47:12 PM block_15-lambda: 10.751969793235443\n",
      "07/14 06:47:16 PM block_15-lambda: 11.997483096448294\n",
      "07/14 06:47:20 PM block_15-lambda: 10.99256694747741\n",
      "07/14 06:47:24 PM block_15-lambda: 12.294797353476406\n",
      "07/14 06:47:28 PM block_15-lambda: 10.287300569465362\n",
      "07/14 06:47:31 PM block_15-lambda: 12.624900971071787\n",
      "07/14 06:47:35 PM block_15-lambda: 14.146507906626505\n",
      "07/14 06:47:39 PM block_15-lambda: 13.025259240085342\n",
      "07/14 06:47:43 PM block_15-lambda: 11.486076140499499\n",
      "07/14 06:47:47 PM block_15-lambda: 12.66472040505773\n",
      "07/14 06:47:50 PM block_15-lambda: 11.455080085968875\n",
      "07/14 06:47:54 PM block_15-lambda: 11.127777712412149\n",
      "07/14 06:47:58 PM block_15-lambda: 11.361010448042169\n",
      "07/14 06:48:02 PM block_15-lambda: 9.573330431099398\n",
      "07/14 06:48:06 PM block_15-lambda: 14.519712639620984\n",
      "07/14 06:48:09 PM block_15-lambda: 13.118431734751505\n",
      "07/14 06:48:13 PM block_15-lambda: 11.165539109563253\n",
      "07/14 06:48:17 PM block_15-lambda: 13.191024841553714\n",
      "07/14 06:48:21 PM block_15-lambda: 13.44344173569277\n",
      "07/14 06:48:25 PM block_15-lambda: 12.18188868756275\n",
      "07/14 06:48:28 PM block_15-lambda: 12.223115116716867\n",
      "07/14 06:48:28 PM block_id: 16\n",
      "07/14 06:48:32 PM block_16-lambda: 6.674759905321531e-06\n",
      "07/14 06:48:36 PM block_16-lambda: 0.6258479351978226\n",
      "07/14 06:48:40 PM block_16-lambda: 7.931956340988956\n",
      "07/14 06:48:43 PM block_16-lambda: 9.688080446787149\n",
      "07/14 06:48:47 PM block_16-lambda: 9.136951124811747\n",
      "07/14 06:48:51 PM block_16-lambda: 10.01452195500753\n",
      "07/14 06:48:54 PM block_16-lambda: 10.396204936935241\n",
      "07/14 06:48:58 PM block_16-lambda: 10.235156446096887\n",
      "07/14 06:49:02 PM block_16-lambda: 8.318531940261044\n",
      "07/14 06:49:05 PM block_16-lambda: 8.482150280810743\n",
      "07/14 06:49:09 PM block_16-lambda: 9.189337427836346\n",
      "07/14 06:49:13 PM block_16-lambda: 10.83864461753263\n",
      "07/14 06:49:16 PM block_16-lambda: 9.120511342243976\n",
      "07/14 06:49:20 PM block_16-lambda: 9.605001058923193\n",
      "07/14 06:49:24 PM block_16-lambda: 9.608488642068274\n",
      "07/14 06:49:24 PM block_id: 17\n",
      "07/14 06:49:27 PM block_17-lambda: 1.0856452498988933e-05\n",
      "07/14 06:49:31 PM block_17-lambda: 4.622688017695783\n",
      "07/14 06:49:35 PM block_17-lambda: 8.56787991810994\n",
      "07/14 06:49:38 PM block_17-lambda: 9.632386969754016\n",
      "07/14 06:49:42 PM block_17-lambda: 10.40829431005271\n",
      "07/14 06:49:45 PM block_17-lambda: 10.475366308985944\n",
      "07/14 06:49:45 PM block_id: 18\n",
      "07/14 06:49:49 PM block_18-lambda: 8.682175221512595e-06\n",
      "07/14 06:49:52 PM block_18-lambda: 1.1220346473785767\n",
      "07/14 06:49:56 PM block_18-lambda: 11.061055746423193\n",
      "07/14 06:49:59 PM block_18-lambda: 11.896594189257028\n",
      "07/14 06:50:03 PM block_18-lambda: 12.168306036646586\n",
      "07/14 06:50:06 PM block_18-lambda: 12.485465298694779\n",
      "07/14 06:50:09 PM block_18-lambda: 10.552546121987952\n",
      "07/14 06:50:13 PM block_18-lambda: 12.818074054028614\n",
      "07/14 06:50:16 PM block_18-lambda: 11.655794074736447\n",
      "07/14 06:50:20 PM block_18-lambda: 11.63436852880271\n",
      "07/14 06:50:20 PM block_id: 19\n",
      "07/14 06:50:23 PM block_19-lambda: 9.529833049778957e-06\n",
      "07/14 06:50:26 PM block_19-lambda: 1.8232979525523971\n",
      "07/14 06:50:30 PM block_19-lambda: 6.921525457297942\n",
      "07/14 06:50:33 PM block_19-lambda: 5.856802208835341\n",
      "07/14 06:50:36 PM block_19-lambda: 6.672092177302962\n",
      "07/14 06:50:40 PM block_19-lambda: 7.543223675953815\n",
      "07/14 06:50:43 PM block_19-lambda: 6.324555056162149\n",
      "07/14 06:50:46 PM block_19-lambda: 7.202983319998745\n",
      "07/14 06:50:50 PM block_19-lambda: 7.036566186621486\n",
      "07/14 06:50:53 PM block_19-lambda: 5.979629945563504\n",
      "07/14 06:50:56 PM block_19-lambda: 7.30467867564006\n",
      "07/14 06:51:00 PM block_19-lambda: 7.476908120764307\n",
      "07/14 06:51:03 PM block_19-lambda: 6.583470110912399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:51:06 PM block_19-lambda: 7.783722291509789\n",
      "07/14 06:51:09 PM block_19-lambda: 7.617292902077058\n",
      "07/14 06:51:13 PM block_19-lambda: 6.032031446096887\n",
      "07/14 06:51:16 PM block_19-lambda: 6.420045416039157\n",
      "07/14 06:51:19 PM block_19-lambda: 7.616696277296687\n",
      "07/14 06:51:23 PM block_19-lambda: 8.277602597891565\n",
      "07/14 06:51:26 PM block_19-lambda: 5.749285717087099\n",
      "07/14 06:51:29 PM block_19-lambda: 7.004094012769829\n",
      "07/14 06:51:33 PM block_19-lambda: 6.461101240901105\n",
      "07/14 06:51:36 PM block_19-lambda: 5.430116461941516\n",
      "07/14 06:51:39 PM block_19-lambda: 6.824895284262048\n",
      "07/14 06:51:42 PM block_19-lambda: 7.588298996768323\n",
      "07/14 06:51:46 PM block_19-lambda: 5.7515893652735945\n",
      "07/14 06:51:49 PM block_19-lambda: 8.520639197414658\n",
      "07/14 06:51:52 PM block_19-lambda: 9.591476256588855\n",
      "07/14 06:51:56 PM block_19-lambda: 6.8037398617909135\n",
      "07/14 06:51:59 PM block_19-lambda: 7.596023743411145\n",
      "07/14 06:52:02 PM block_19-lambda: 7.695510557856426\n",
      "07/14 06:52:06 PM block_19-lambda: 6.246705082046938\n",
      "07/14 06:52:09 PM block_19-lambda: 7.137290862669428\n",
      "07/14 06:52:12 PM block_19-lambda: 5.593434284011044\n",
      "07/14 06:52:15 PM block_19-lambda: 7.369428887424699\n",
      "07/14 06:52:19 PM block_19-lambda: 6.877166380365211\n",
      "07/14 06:52:22 PM block_19-lambda: 7.618443990806978\n",
      "07/14 06:52:25 PM block_19-lambda: 6.556162148594377\n",
      "07/14 06:52:29 PM block_19-lambda: 6.800934695814508\n",
      "07/14 06:52:32 PM block_19-lambda: 7.496297690763052\n",
      "07/14 06:52:35 PM block_19-lambda: 6.413521272590361\n",
      "07/14 06:52:39 PM block_19-lambda: 6.333911328909387\n",
      "07/14 06:52:42 PM block_19-lambda: 6.242441445469377\n",
      "07/14 06:52:45 PM block_19-lambda: 7.209646692237701\n",
      "07/14 06:52:48 PM block_19-lambda: 5.507656112732179\n",
      "07/14 06:52:52 PM block_19-lambda: 7.877952238642068\n",
      "07/14 06:52:55 PM block_19-lambda: 7.373971471824799\n",
      "07/14 06:52:58 PM block_19-lambda: 6.893385063692269\n",
      "07/14 06:53:02 PM block_19-lambda: 7.178855068712349\n",
      "07/14 06:53:05 PM block_19-lambda: 5.952191578815261\n",
      "07/14 06:53:08 PM block_19-lambda: 6.5105348150414155\n",
      "07/14 06:53:12 PM block_19-lambda: 4.912490783446285\n",
      "07/14 06:53:15 PM block_19-lambda: 6.901543184456576\n",
      "07/14 06:53:18 PM block_19-lambda: 5.888499799981175\n",
      "07/14 06:53:21 PM block_19-lambda: 5.465356543360944\n",
      "07/14 06:53:25 PM block_19-lambda: 7.576682217149849\n",
      "07/14 06:53:28 PM block_19-lambda: 7.689192316139558\n",
      "07/14 06:53:31 PM block_19-lambda: 5.5717170439570785\n",
      "07/14 06:53:35 PM block_19-lambda: 6.950290419490462\n",
      "07/14 06:53:38 PM block_19-lambda: 7.707803381494729\n",
      "07/14 06:53:41 PM block_19-lambda: 5.579181471981677\n",
      "07/14 06:53:45 PM block_19-lambda: 6.978688680503263\n",
      "07/14 06:53:48 PM block_19-lambda: 6.22603058719252\n",
      "07/14 06:53:51 PM block_19-lambda: 6.132349221103163\n",
      "07/14 06:53:55 PM block_19-lambda: 5.745465749717621\n",
      "07/14 06:53:58 PM block_19-lambda: 6.6626829583960845\n",
      "07/14 06:54:01 PM block_19-lambda: 8.30075085498243\n",
      "07/14 06:54:04 PM block_19-lambda: 7.813483916133283\n",
      "07/14 06:54:08 PM block_19-lambda: 8.2548828125\n",
      "07/14 06:54:11 PM block_19-lambda: 6.906460804154117\n",
      "07/14 06:54:14 PM block_19-lambda: 6.190820410548444\n",
      "07/14 06:54:18 PM block_19-lambda: 5.58886277532003\n",
      "07/14 06:54:21 PM block_19-lambda: 8.181486688943274\n",
      "07/14 06:54:24 PM block_19-lambda: 8.068988355766818\n",
      "07/14 06:54:28 PM block_19-lambda: 6.963008773374749\n",
      "07/14 06:54:31 PM block_19-lambda: 7.727415521460843\n",
      "07/14 06:54:34 PM block_19-lambda: 7.3140501458960845\n",
      "07/14 06:54:37 PM block_19-lambda: 6.027634953878012\n",
      "07/14 06:54:41 PM block_19-lambda: 8.12557946630271\n",
      "07/14 06:54:44 PM block_19-lambda: 8.352320414470382\n",
      "07/14 06:54:47 PM block_19-lambda: 6.712558731017821\n",
      "07/14 06:54:51 PM block_19-lambda: 6.001579560429217\n",
      "07/14 06:54:54 PM block_19-lambda: 6.542392225150603\n",
      "07/14 06:54:57 PM block_19-lambda: 6.191315064947289\n",
      "07/14 06:55:01 PM block_19-lambda: 7.17263781689257\n",
      "07/14 06:55:04 PM block_19-lambda: 6.864555879769076\n",
      "07/14 06:55:07 PM block_19-lambda: 6.266620191704317\n",
      "07/14 06:55:11 PM block_19-lambda: 7.492008071347891\n",
      "07/14 06:55:14 PM block_19-lambda: 7.0823278465424195\n",
      "07/14 06:55:17 PM block_19-lambda: 6.604090188880522\n",
      "07/14 06:55:20 PM block_19-lambda: 6.466588031814759\n",
      "07/14 06:55:24 PM block_19-lambda: 7.411825034513052\n",
      "07/14 06:55:27 PM block_19-lambda: 6.632989477441014\n",
      "07/14 06:55:30 PM block_19-lambda: 7.178612398814006\n",
      "07/14 06:55:34 PM block_19-lambda: 8.053403065386545\n",
      "07/14 06:55:37 PM block_19-lambda: 6.626987441955321\n",
      "07/14 06:55:40 PM block_19-lambda: 6.310172329944779\n",
      "07/14 06:55:44 PM block_19-lambda: 6.30042386342244\n",
      "07/14 06:55:44 PM block_id: 20\n",
      "07/14 06:55:47 PM block_20-lambda: 6.004130976446661e-06\n",
      "07/14 06:55:50 PM block_20-lambda: 0.3714157472173852\n",
      "07/14 06:55:53 PM block_20-lambda: 5.916316633722389\n",
      "07/14 06:55:56 PM block_20-lambda: 6.682921137675703\n",
      "07/14 06:56:00 PM block_20-lambda: 5.519073854009789\n",
      "07/14 06:56:03 PM block_20-lambda: 5.60319745779995\n",
      "07/14 06:56:06 PM block_20-lambda: 5.501640350464357\n",
      "07/14 06:56:09 PM block_20-lambda: 6.019961682668173\n",
      "07/14 06:56:12 PM block_20-lambda: 5.926115104951054\n",
      "07/14 06:56:16 PM block_20-lambda: 5.3654182158320785\n",
      "07/14 06:56:19 PM block_20-lambda: 5.263377729668675\n",
      "07/14 06:56:22 PM block_20-lambda: 5.184182530904869\n",
      "07/14 06:56:25 PM block_20-lambda: 6.702326885667671\n",
      "07/14 06:56:28 PM block_20-lambda: 5.167432915254769\n",
      "07/14 06:56:31 PM block_20-lambda: 5.343047482900351\n",
      "07/14 06:56:35 PM block_20-lambda: 6.650448473581827\n",
      "07/14 06:56:38 PM block_20-lambda: 5.770179840455572\n",
      "07/14 06:56:41 PM block_20-lambda: 5.534941523908133\n",
      "07/14 06:56:44 PM block_20-lambda: 6.3095678612889055\n",
      "07/14 06:56:47 PM block_20-lambda: 6.616029547879016\n",
      "07/14 06:56:51 PM block_20-lambda: 6.399714580980171\n",
      "07/14 06:56:54 PM block_20-lambda: 5.783942410266064\n",
      "07/14 06:56:57 PM block_20-lambda: 5.251662411364207\n",
      "07/14 06:57:00 PM block_20-lambda: 5.25068879031752\n",
      "07/14 06:57:00 PM block_id: 21\n",
      "07/14 06:57:03 PM block_21-lambda: 5.166116451104003e-06\n",
      "07/14 06:57:06 PM block_21-lambda: 2.4089654516503516\n",
      "07/14 06:57:09 PM block_21-lambda: 4.437694626160893\n",
      "07/14 06:57:12 PM block_21-lambda: 3.647337151339734\n",
      "07/14 06:57:15 PM block_21-lambda: 4.522812931413153\n",
      "07/14 06:57:19 PM block_21-lambda: 3.9297453485818274\n",
      "07/14 06:57:22 PM block_21-lambda: 3.7200621332988204\n",
      "07/14 06:57:25 PM block_21-lambda: 4.581924377196285\n",
      "07/14 06:57:28 PM block_21-lambda: 3.9570162976123244\n",
      "07/14 06:57:31 PM block_21-lambda: 3.660815870905497\n",
      "07/14 06:57:34 PM block_21-lambda: 4.869943641754518\n",
      "07/14 06:57:37 PM block_21-lambda: 4.333308330980171\n",
      "07/14 06:57:40 PM block_21-lambda: 4.754902912430974\n",
      "07/14 06:57:43 PM block_21-lambda: 5.032416776480924\n",
      "07/14 06:57:46 PM block_21-lambda: 3.6686629329819276\n",
      "07/14 06:57:49 PM block_21-lambda: 3.476025439649222\n",
      "07/14 06:57:52 PM block_21-lambda: 3.953321587129769\n",
      "07/14 06:57:55 PM block_21-lambda: 5.1284954270205825\n",
      "07/14 06:57:58 PM block_21-lambda: 4.808881031940261\n",
      "07/14 06:58:01 PM block_21-lambda: 4.543925212569026\n",
      "07/14 06:58:04 PM block_21-lambda: 4.168711466961597\n",
      "07/14 06:58:08 PM block_21-lambda: 4.536015644609689\n",
      "07/14 06:58:11 PM block_21-lambda: 4.579290305754267\n",
      "07/14 06:58:11 PM block_id: 22\n",
      "07/14 06:58:14 PM block_22-lambda: 4.053704934306891e-06\n",
      "07/14 06:58:17 PM block_22-lambda: 1.1794914031124497\n",
      "07/14 06:58:19 PM block_22-lambda: 4.5781946143950805\n",
      "07/14 06:58:22 PM block_22-lambda: 5.282372164439006\n",
      "07/14 06:58:25 PM block_22-lambda: 6.084964859437751\n",
      "07/14 06:58:28 PM block_22-lambda: 5.403678679561998\n",
      "07/14 06:58:31 PM block_22-lambda: 3.9559367842463606\n",
      "07/14 06:58:34 PM block_22-lambda: 4.705509538152611\n",
      "07/14 06:58:37 PM block_22-lambda: 4.757521296121988\n",
      "07/14 06:58:40 PM block_22-lambda: 3.4073628498368476\n",
      "07/14 06:58:43 PM block_22-lambda: 4.417740297126004\n",
      "07/14 06:58:46 PM block_22-lambda: 5.4810173310429215\n",
      "07/14 06:58:49 PM block_22-lambda: 4.034841024253263\n",
      "07/14 06:58:52 PM block_22-lambda: 4.827799479166667\n",
      "07/14 06:58:55 PM block_22-lambda: 5.371077572006777\n",
      "07/14 06:58:58 PM block_22-lambda: 4.352319924228163\n",
      "07/14 06:59:01 PM block_22-lambda: 4.806486688943273\n",
      "07/14 06:59:04 PM block_22-lambda: 4.785457748964609\n",
      "07/14 06:59:04 PM block_id: 23\n",
      "07/14 06:59:06 PM block_23-lambda: 5.59045964979144e-06\n",
      "07/14 06:59:09 PM block_23-lambda: 0.05909682660696497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14 06:59:12 PM block_23-lambda: 2.502325463965236\n",
      "07/14 06:59:15 PM block_23-lambda: 4.352340024159137\n",
      "07/14 06:59:18 PM block_23-lambda: 3.8900766640781876\n",
      "07/14 06:59:21 PM block_23-lambda: 3.817349230908007\n",
      "07/14 06:59:23 PM block_23-lambda: 2.9209151057354417\n",
      "07/14 06:59:26 PM block_23-lambda: 3.3446802346103164\n",
      "07/14 06:59:29 PM block_23-lambda: 4.210061437154869\n",
      "07/14 06:59:32 PM block_23-lambda: 4.557550514558233\n",
      "07/14 06:59:35 PM block_23-lambda: 3.5562499019515563\n",
      "07/14 06:59:37 PM block_23-lambda: 2.4880540227315513\n",
      "07/14 06:59:40 PM block_23-lambda: 3.9166384777390815\n",
      "07/14 06:59:43 PM block_23-lambda: 4.068452275900477\n",
      "07/14 06:59:46 PM block_23-lambda: 4.257618854323544\n",
      "07/14 06:59:49 PM block_23-lambda: 4.538678150100401\n",
      "07/14 06:59:52 PM block_23-lambda: 3.552490234375\n",
      "07/14 06:59:54 PM block_23-lambda: 3.2225981562970634\n",
      "07/14 06:59:57 PM block_23-lambda: 3.9367879231770835\n",
      "07/14 07:00:00 PM block_23-lambda: 4.412604029398845\n",
      "07/14 07:00:03 PM block_23-lambda: 3.2166179365901106\n",
      "07/14 07:00:06 PM block_23-lambda: 3.3572534767978164\n",
      "07/14 07:00:09 PM block_23-lambda: 3.3001510436276353\n",
      "07/14 07:00:11 PM block_23-lambda: 3.8845609488736197\n",
      "07/14 07:00:14 PM block_23-lambda: 3.7298542313786394\n",
      "07/14 07:00:17 PM block_23-lambda: 3.685434854652987\n",
      "07/14 07:00:20 PM block_23-lambda: 3.79731964969252\n",
      "07/14 07:00:23 PM block_23-lambda: 3.4624976958615714\n",
      "07/14 07:00:26 PM block_23-lambda: 4.266849625062751\n",
      "07/14 07:00:28 PM block_23-lambda: 4.093998307683861\n",
      "07/14 07:00:31 PM block_23-lambda: 4.397830089890813\n",
      "07/14 07:00:34 PM block_23-lambda: 4.560152230013805\n",
      "07/14 07:00:37 PM block_23-lambda: 3.60722136593248\n",
      "07/14 07:00:40 PM block_23-lambda: 3.8241270747050704\n",
      "07/14 07:00:42 PM block_23-lambda: 3.655093763726782\n",
      "07/14 07:00:45 PM block_23-lambda: 4.561411172031877\n",
      "07/14 07:00:48 PM block_23-lambda: 3.6015105343247993\n",
      "07/14 07:00:51 PM block_23-lambda: 3.9585269790097892\n",
      "07/14 07:00:54 PM block_23-lambda: 3.5655426883314507\n",
      "07/14 07:00:57 PM block_23-lambda: 4.324978625439257\n",
      "07/14 07:00:59 PM block_23-lambda: 4.052485577073921\n",
      "07/14 07:01:02 PM block_23-lambda: 3.6274105209902108\n",
      "07/14 07:01:05 PM block_23-lambda: 3.8313390279869477\n",
      "07/14 07:01:08 PM block_23-lambda: 3.283492613030246\n",
      "07/14 07:01:11 PM block_23-lambda: 3.8627785066045432\n",
      "07/14 07:01:14 PM block_23-lambda: 4.3961627761044175\n",
      "07/14 07:01:16 PM block_23-lambda: 4.002209031438253\n",
      "07/14 07:01:19 PM block_23-lambda: 3.3789844436339105\n",
      "07/14 07:01:22 PM block_23-lambda: 3.9861879157254014\n",
      "07/14 07:01:25 PM block_23-lambda: 3.398985100558484\n",
      "07/14 07:01:28 PM block_23-lambda: 3.7053073132373244\n",
      "07/14 07:01:30 PM block_23-lambda: 4.730634942112199\n",
      "07/14 07:01:33 PM block_23-lambda: 4.398487504706325\n",
      "07/14 07:01:36 PM block_23-lambda: 3.476490189272716\n",
      "07/14 07:01:39 PM block_23-lambda: 3.302845169741466\n",
      "07/14 07:01:42 PM block_23-lambda: 4.238596475746737\n",
      "07/14 07:01:44 PM block_23-lambda: 3.5607738571473395\n",
      "07/14 07:01:47 PM block_23-lambda: 3.6519098856362953\n",
      "07/14 07:01:50 PM block_23-lambda: 4.16358304310994\n",
      "07/14 07:01:53 PM block_23-lambda: 4.184972801361697\n"
     ]
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "for module in student_model.modules():\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "block_id = 0\n",
    "\n",
    "for block_id in range(layer_num):\n",
    "    logger.info(f'block_id: {block_id}')\n",
    "    model_block = student_model.bert.encoder.layer[block_id]\n",
    "    \n",
    "    v = [\n",
    "            torch.randn(p.size()).to(device) for p in model_block.parameters()\n",
    "        ]\n",
    "    v = de_variable(v)\n",
    "\n",
    "    lambda_old, lambdas = 0., 1.\n",
    "    i = 0\n",
    "    while (abs((lambdas - lambda_old) / lambdas) >= 0.005):\n",
    "\n",
    "        lambda_old = lambdas\n",
    "\n",
    "        acc_Hv = [\n",
    "            torch.zeros(p.size()).cuda() for p in model_block.parameters()\n",
    "        ]\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step < percentage_index:\n",
    "                \n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids, _ = batch\n",
    "                \n",
    "                logits, _, _, _, _ = student_model(input_ids, segment_ids, input_mask, teacher_outputs=None)\n",
    "\n",
    "                if output_mode == \"classification\":\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(\n",
    "                        logits.view(-1, num_labels), label_ids.view(-1))\n",
    "                elif output_mode == \"regression\":\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "                    \n",
    "                loss.backward(create_graph=True)\n",
    "                grads = [param.grad for param in model_block.parameters()]\n",
    "                params = model_block.parameters()\n",
    "\n",
    "                Hv = torch.autograd.grad(\n",
    "                    grads,\n",
    "                    params,\n",
    "                    grad_outputs=v,\n",
    "                    only_inputs=True,\n",
    "                    retain_graph=True)\n",
    "                acc_Hv = [\n",
    "                    acc_Hv_p + Hv_p for acc_Hv_p, Hv_p in zip(acc_Hv, Hv)\n",
    "                ]\n",
    "                student_model.zero_grad()\n",
    "        # calculate raylay quotients\n",
    "        lambdas = group_product(acc_Hv, v).item() / percentage_index\n",
    "\n",
    "        v = de_variable(acc_Hv)\n",
    "        logger.info(f'block_{block_id}-lambda: {lambdas}')\n",
    "        if abs((lambdas - lambda_old) / lambdas) < 0.005:\n",
    "            writer.writerow([f'{block_id}', f'{i}', f'{lambdas}'])\n",
    "            csv_file.flush()\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc50e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
